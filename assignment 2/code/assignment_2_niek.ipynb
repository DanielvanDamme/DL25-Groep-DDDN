{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c9752d",
   "metadata": {},
   "source": [
    "The data extraction was based on the provided code, with slight adjustments for where we decided to store the downloaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0f61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/train/\"))\n",
    "INTRA_VAL_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/val/\"))\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/test/\"))\n",
    "\n",
    "def get_dataset_name(filename_with_dir):\n",
    "    filename_without_dir = os.path.basename(filename_with_dir)\n",
    "    temp = filename_without_dir.split('.')[:-1]\n",
    "    dataset_name = ''.join(temp)\n",
    "    temp = dataset_name.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "def extract_data_from_folder_by_file(folder_path, shuffle=False):\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "    for file_name in files:\n",
    "        \n",
    "        filename_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            matrix = f.get(dataset_name)[()]\n",
    "            yield dataset_name, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d7e5",
   "metadata": {},
   "source": [
    "We first have to scale the data across different files in the same way, so we have to scan the files and find min max to perform the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bf681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_minmax_from_all_files(folder_path: str) -> tuple:\n",
    "    # Placeholders\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "\n",
    "    for (_, data) in extract_data_from_folder_by_file(folder_path):\n",
    "        data = data.T\n",
    "        if min_val is None:\n",
    "            min_val = np.min(data, axis=0)\n",
    "            max_val = np.max(data, axis=0)\n",
    "        else:\n",
    "            # Update min_val and max_val\n",
    "            min_val = np.minimum(min_val, np.min(data, axis=0))\n",
    "            max_val = np.maximum(max_val, np.max(data, axis=0))\n",
    "        \n",
    "    return min_val, max_val\n",
    "\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    # Scale the data to the range [0, 1]\n",
    "    return (data - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5558f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values: (248,), Max values: (248,)\n"
     ]
    }
   ],
   "source": [
    "min_val, max_val = learn_minmax_from_all_files(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Min values: {min_val.shape}, Max values: {max_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c45eb",
   "metadata": {},
   "source": [
    "Because of independent sampling, we can just sample each file independently and the same dropout should occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28617963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data: np.array, factor: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Downsample time series data by uniformly selecting samples at fixed intervals\n",
    "    to keep the temporal order intact.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Input time series data (1D or 2D with time dimension as first axis)\n",
    "        factor (float): Downsampling factor (e.g., 0.5 means keep half the samples)\n",
    "\n",
    "    Returns:\n",
    "        np.array: Downsampled data with timesteps reduced by the factor\n",
    "    \"\"\"\n",
    "    num_samples = int(len(data) * factor)\n",
    "    # Calculate the stride to evenly pick samples\n",
    "    stride = len(data) / num_samples\n",
    "    # Use np.floor to avoid going out of bounds and convert to int indices\n",
    "    indices = (np.floor(np.arange(num_samples) * stride)).astype(int)\n",
    "    downsampled_data = data[indices]\n",
    "    return downsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cc899",
   "metadata": {},
   "source": [
    "Here we can set the downsample factor for all sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c75352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_FACTOR = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edf396",
   "metadata": {},
   "source": [
    "Here, we define the preprocessing steps that we apply to all data after reading it from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa81762",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val), \n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f79afc",
   "metadata": {},
   "source": [
    "We should also create labels based on the file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ae138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(file_name:str) -> np.ndarray:\n",
    "    # Return a one-hot encoded label based on the file name, there are4 classes\n",
    "    # 0: rest\n",
    "    if \"rest_\" in file_name:\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    # 1: task_motor\n",
    "    elif \"task_motor_\" in file_name:\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    # 2: task_story_math\n",
    "    elif \"task_story_math_\" in file_name:\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    # 3: task_working_memory\n",
    "    elif \"task_working_memory_\" in file_name:\n",
    "        return np.array([0, 0, 0, 1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file name: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2de89b",
   "metadata": {},
   "source": [
    "To create batches by number of files, we can use a generator like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7f9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(folder, number_of_files_per_batch: int, preprocessing_pipeline: list = None, shuffle_files=True) -> Iterator[tuple]:\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for n, (name, data) in enumerate(extract_data_from_folder_by_file(folder, shuffle=shuffle_files)):\n",
    "        data = data.T\n",
    "        if preprocessing_pipeline:\n",
    "            for preprocessing_step in preprocessing_pipeline:\n",
    "                data = preprocessing_step(data)\n",
    "        \n",
    "        if data.shape[0] != 7124:\n",
    "            raise ValueError(f\"data shaped{data.shape}\")\n",
    "\n",
    "        # Add the preprocessed data to the batch\n",
    "        batch_data.append(data)\n",
    "\n",
    "        # Generate the label matrix of the length of the data for the current file\n",
    "        label_vector = generate_label(name)\n",
    "        batch_labels.append(label_vector)\n",
    "\n",
    "\n",
    "        # Check if we have reached the desired batch size\n",
    "        if (n + 1) % number_of_files_per_batch == 0:\n",
    "            # Stack along the first axis (like a batch dimension)\n",
    "            yield (batch_data, batch_labels)\n",
    "            batch_data = []\n",
    "            batch_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e79782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def keras_data_generator(folder, batch_size, preprocessing_pipeline=None, shuffle_files=True):\n",
    "    while True:  # Required for Keras fit() to work\n",
    "        gc.collect()\n",
    "        for batch_X_list, batch_y_list in create_batches(\n",
    "            folder=folder,\n",
    "            number_of_files_per_batch=batch_size,\n",
    "            preprocessing_pipeline=preprocessing_pipeline,\n",
    "            shuffle_files=shuffle_files\n",
    "        ):\n",
    "            data = np.array(batch_X_list)\n",
    "            labels = np.array(batch_y_list)\n",
    "\n",
    "            # Shuffle data and labels together\n",
    "            indices = np.arange(len(data))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            if data.shape[0] is None:\n",
    "                continue\n",
    "\n",
    "            yield data[indices], labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac02c17",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de527f",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87683b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Attention,\n",
    "    LayerNormalization,\n",
    "    Input,\n",
    "    Layer,\n",
    "    Conv1D,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    AveragePooling1D,\n",
    "    MultiHeadAttention,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "FEATURES = 248\n",
    "TIMESTEPS = 7124\n",
    "CLASSES = 4\n",
    "\n",
    "lstm_classifier = Sequential(\n",
    "    [\n",
    "        Input((TIMESTEPS, FEATURES)),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(CLASSES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lstm_classifier.compile(\n",
    "    loss=CategoricalCrossentropy(),  # works directly with one-hot encoded labels\n",
    "    optimizer=Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "bidirectional_lstm = Sequential(\n",
    "    [\n",
    "        Input((TIMESTEPS, FEATURES)),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(CLASSES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bidirectional_lstm.compile(\n",
    "    loss=CategoricalCrossentropy(), optimizer=Adam(), metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "cnn = Sequential(\n",
    "    [\n",
    "        Input((TIMESTEPS, FEATURES)),\n",
    "        Conv1D(filters=64, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.08),\n",
    "        Dense(CLASSES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = Attention()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Use same input as query and value for self-attention\n",
    "        return self.attention([inputs, inputs])\n",
    "\n",
    "\n",
    "cnn_self_attention = Sequential(\n",
    "    [\n",
    "        Input((TIMESTEPS, FEATURES)),\n",
    "        Conv1D(filters=64, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        GlobalAveragePooling1D(),\n",
    "        LayerNormalization(),  # <--- try here\n",
    "        AttentionLayer(),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.08),\n",
    "        Dense(CLASSES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_self_attention.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "class MHAttentionLayer(Layer):\n",
    "    def __init__(self, num_heads=4, key_dim=32):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()  # <-- this was missing\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_out = self.att(inputs, inputs)\n",
    "        return self.norm(attn_out + inputs)  # optional residual connection\n",
    "    \n",
    "cnn_multihead_attention = Sequential(\n",
    "    [\n",
    "        Input((TIMESTEPS, FEATURES)),\n",
    "        Conv1D(filters=64, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        \n",
    "        LayerNormalization(),  # <--- try here\n",
    "        MHAttentionLayer(num_heads=4, key_dim=32),\n",
    "\n",
    "        GlobalAveragePooling1D(),\n",
    "\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.08),\n",
    "        Dense(CLASSES, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_multihead_attention.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f53bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"LSTM\": lstm_classifier,\n",
    "    \"Bidirectional LSTM\": bidirectional_lstm,\n",
    "    \"CNN\": cnn,\n",
    "    \"CNN + self attention\": cnn_self_attention,\n",
    "    \"CNN + multihead attention\": cnn_multihead_attention\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0816f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8ac250",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1baf957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # or 'val_accuracy', depending on what you want to track\n",
    "    patience=10,           # Wait 5 epochs without improvement before stopping\n",
    "    restore_best_weights=True,  # Roll back to best weights (optional, but recommended)\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74fe0c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 11s 1s/step - loss: 1.2631 - accuracy: 0.3214 - val_loss: 0.9399 - val_accuracy: 1.0000\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.9212 - accuracy: 0.6786 - val_loss: 0.6816 - val_accuracy: 1.0000\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.5982 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 1.0000\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.4075 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 1.0000\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.2242 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 1.0000\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.1392 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 1.0000\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0513 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 7s 997ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 7s 998ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 7s 998ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 6s 976ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 6s 954ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 6s 967ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 6s 950ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 6s 935ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 6s 889ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 6s 952ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 7s 983ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 6s 982ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 6s 933ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 6s 921ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 6s 990ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 7s 987ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 7s 994ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 6s 931ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 6s 936ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 9.4342e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 6s 934ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 6s 888ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 9.8419e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 6s 966ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 6.8065e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 7s 997ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.7081e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.3749e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 6s 956ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 6.3883e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 7s 998ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 6.6886e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 6s 958ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 8.3194e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 7s 990ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 7.2310e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.5054e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 6s 883ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 7.5953e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 6s 892ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 7.2705e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 6s 866ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 6.2140e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 6s 961ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.0121e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 6s 978ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 5.8635e-04 - val_accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 6s 900ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.8279e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 6s 936ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 5.4427e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 6s 891ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.4779e-04 - val_accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 6s 867ms/step - loss: 8.9564e-04 - accuracy: 1.0000 - val_loss: 3.4855e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 6s 884ms/step - loss: 7.5399e-04 - accuracy: 1.0000 - val_loss: 6.5952e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 6s 908ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.9111e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 6s 933ms/step - loss: 9.3996e-04 - accuracy: 1.0000 - val_loss: 4.7821e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 7s 997ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.1534e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - 6s 873ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.2823e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "7/7 [==============================] - 6s 935ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3765e-04 - val_accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 9.7477e-04 - accuracy: 1.0000 - val_loss: 3.8439e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "7/7 [==============================] - 7s 981ms/step - loss: 9.9875e-04 - accuracy: 1.0000 - val_loss: 4.1325e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 9.9789e-04 - accuracy: 1.0000 - val_loss: 3.3967e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "7/7 [==============================] - 10s 1s/step - loss: 7.6973e-04 - accuracy: 1.0000 - val_loss: 3.9190e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "7/7 [==============================] - 14s 2s/step - loss: 8.3771e-04 - accuracy: 1.0000 - val_loss: 3.6597e-04 - val_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "7/7 [==============================] - 12s 2s/step - loss: 9.5387e-04 - accuracy: 1.0000 - val_loss: 4.8970e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.5007e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 6.1446e-04 - accuracy: 1.0000 - val_loss: 3.5761e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 4.9237e-04 - accuracy: 1.0000 - val_loss: 4.4510e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "7/7 [==============================] - 10s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5224e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 7.4054e-04 - accuracy: 1.0000 - val_loss: 3.3476e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 5.6414e-04 - accuracy: 1.0000 - val_loss: 3.2779e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 9.2393e-04 - accuracy: 1.0000 - val_loss: 3.2126e-04 - val_accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.1462e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.0691e-04 - val_accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.9996e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "7/7 [==============================] - 10s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.5425e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 5.3412e-04 - accuracy: 1.0000 - val_loss: 3.7653e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 6.2335e-04 - accuracy: 1.0000 - val_loss: 2.8128e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000  Restoring model weights from the end of the best epoch: 83.\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.7518e-04 - val_accuracy: 1.0000\n",
      "Epoch 93: early stopping\n",
      "Epoch 1/1000\n",
      "7/7 [==============================] - 19s 2s/step - loss: 1.2367 - accuracy: 0.4643 - val_loss: 0.8902 - val_accuracy: 1.0000\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.8652 - accuracy: 0.8929 - val_loss: 0.6555 - val_accuracy: 1.0000\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.6182 - accuracy: 0.9643 - val_loss: 0.3004 - val_accuracy: 1.0000\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.4258 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 1.0000\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.2421 - accuracy: 1.0000 - val_loss: 0.1300 - val_accuracy: 1.0000\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.1751 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 1.0000\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.1125 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 8.1840e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 6.1535e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 9.2928e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 8.8004e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 6.6743e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 7.8099e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 7.1914e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 6.8764e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 6.4426e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 7.2811e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 5.7148e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.3563e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 5.0358e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 7.8391e-04 - accuracy: 1.0000 - val_loss: 4.8128e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6251e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.2038e-04 - accuracy: 1.0000 - val_loss: 4.5212e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 8.0526e-04 - accuracy: 1.0000 - val_loss: 3.5869e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.9018e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.0577e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.5444e-04 - accuracy: 1.0000 - val_loss: 4.7675e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.6415e-04 - accuracy: 1.0000 - val_loss: 4.3406e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 8.1083e-04 - accuracy: 1.0000 - val_loss: 2.9592e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 8.4738e-04 - accuracy: 1.0000 - val_loss: 3.5352e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9450e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.2457e-04 - accuracy: 1.0000 - val_loss: 3.4882e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.9681e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.0514e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.4612e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 8.7330e-04 - accuracy: 1.0000 - val_loss: 3.2151e-04 - val_accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3184e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5760e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 9.8125e-04 - accuracy: 1.0000 - val_loss: 2.0563e-04 - val_accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3463e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.7400e-04 - accuracy: 1.0000 - val_loss: 2.5883e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 13s 2s/step - loss: 5.4612e-04 - accuracy: 1.0000 - val_loss: 1.9911e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 12s 2s/step - loss: 4.1747e-04 - accuracy: 1.0000 - val_loss: 2.2302e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 16s 3s/step - loss: 3.3265e-04 - accuracy: 1.0000 - val_loss: 1.4545e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 9.1574e-04 - accuracy: 1.0000 - val_loss: 1.6592e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.3351e-04 - accuracy: 1.0000 - val_loss: 1.6276e-04 - val_accuracy: 1.0000\n",
      "Epoch 74/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 3.7534e-04 - accuracy: 1.0000 - val_loss: 1.9051e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 3.4858e-04 - accuracy: 1.0000 - val_loss: 1.4033e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.5886e-04 - accuracy: 1.0000 - val_loss: 1.3365e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 4.0750e-04 - accuracy: 1.0000 - val_loss: 1.3043e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 3.9506e-04 - accuracy: 1.0000 - val_loss: 1.5451e-04 - val_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.3432e-04 - accuracy: 1.0000 - val_loss: 1.4143e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 2.2526e-04 - accuracy: 1.0000 - val_loss: 1.1390e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 1.7551e-04 - accuracy: 1.0000 - val_loss: 1.1229e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 2.6723e-04 - accuracy: 1.0000 - val_loss: 9.4558e-05 - val_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 4.0581e-04 - accuracy: 1.0000 - val_loss: 1.3407e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.1041e-04 - accuracy: 1.0000 - val_loss: 1.3449e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.2888e-04 - accuracy: 1.0000 - val_loss: 1.0150e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.5826e-04 - accuracy: 1.0000 - val_loss: 6.9138e-05 - val_accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.9395e-04 - accuracy: 1.0000 - val_loss: 1.1399e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1500e-04 - val_accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.8464e-04 - accuracy: 1.0000 - val_loss: 8.7942e-05 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.0170e-04 - accuracy: 1.0000 - val_loss: 6.0407e-05 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.0176e-04 - accuracy: 1.0000 - val_loss: 9.6882e-05 - val_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.7711e-04 - accuracy: 1.0000 - val_loss: 8.5379e-05 - val_accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 3.9448e-04 - accuracy: 1.0000 - val_loss: 7.8078e-05 - val_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.3508e-04 - accuracy: 1.0000 - val_loss: 6.0973e-05 - val_accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 3.5051e-04 - accuracy: 1.0000 - val_loss: 7.1940e-05 - val_accuracy: 1.0000\n",
      "Epoch 96/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 1.3218e-04 - accuracy: 1.0000 - val_loss: 9.0803e-05 - val_accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.2129e-04 - accuracy: 1.0000 - val_loss: 6.7946e-05 - val_accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.5668e-04 - accuracy: 1.0000 - val_loss: 6.6427e-05 - val_accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.4111e-04 - accuracy: 1.0000 - val_loss: 7.9032e-05 - val_accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.2710e-04 - accuracy: 1.0000 - val_loss: 4.4792e-05 - val_accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.7911e-04 - accuracy: 1.0000 - val_loss: 6.2821e-05 - val_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.1662e-04 - accuracy: 1.0000 - val_loss: 6.1062e-05 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.5592e-04 - accuracy: 1.0000 - val_loss: 5.9543e-05 - val_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 9.5540e-05 - accuracy: 1.0000 - val_loss: 5.8500e-05 - val_accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.2335e-04 - accuracy: 1.0000 - val_loss: 5.7546e-05 - val_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.0170e-04 - accuracy: 1.0000 - val_loss: 5.6563e-05 - val_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.3642e-04 - accuracy: 1.0000 - val_loss: 4.4076e-05 - val_accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 4.6587e-04 - accuracy: 1.0000 - val_loss: 3.6179e-05 - val_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 3.1323e-04 - accuracy: 1.0000 - val_loss: 5.2480e-05 - val_accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 5.5897e-04 - accuracy: 1.0000 - val_loss: 6.2732e-05 - val_accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.7340e-04 - accuracy: 1.0000 - val_loss: 5.4089e-05 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.2965e-04 - accuracy: 1.0000 - val_loss: 3.7460e-05 - val_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.1566e-04 - accuracy: 1.0000 - val_loss: 4.7712e-05 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.1575e-04 - accuracy: 1.0000 - val_loss: 6.2821e-05 - val_accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.3865e-04 - accuracy: 1.0000 - val_loss: 3.5643e-05 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.6916e-04 - accuracy: 1.0000 - val_loss: 5.5878e-05 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.6111e-04 - accuracy: 1.0000 - val_loss: 4.3540e-05 - val_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 5.8571e-04 - accuracy: 1.0000 - val_loss: 5.0662e-05 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.6524e-04 - accuracy: 1.0000 - val_loss: 3.2096e-05 - val_accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.5587e-04 - accuracy: 1.0000 - val_loss: 3.0457e-05 - val_accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.9196e-05 - accuracy: 1.0000 - val_loss: 3.8623e-05 - val_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.3555e-05 - accuracy: 1.0000 - val_loss: 3.3289e-05 - val_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.3529e-04 - accuracy: 1.0000 - val_loss: 2.9772e-05 - val_accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 5.8932e-04 - accuracy: 1.0000 - val_loss: 3.6745e-05 - val_accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.5288e-04 - accuracy: 1.0000 - val_loss: 3.5762e-05 - val_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.0293e-04 - accuracy: 1.0000 - val_loss: 2.6702e-05 - val_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 6.0829e-05 - accuracy: 1.0000 - val_loss: 3.5285e-05 - val_accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 8.6479e-05 - accuracy: 1.0000 - val_loss: 2.3126e-05 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.4381e-04 - accuracy: 1.0000 - val_loss: 3.4004e-05 - val_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 6.0988e-05 - accuracy: 1.0000 - val_loss: 3.0696e-05 - val_accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.9926e-04 - accuracy: 1.0000 - val_loss: 3.2275e-05 - val_accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 7.7676e-05 - accuracy: 1.0000 - val_loss: 4.3153e-05 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.4359e-04 - accuracy: 1.0000 - val_loss: 3.2067e-05 - val_accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 6.3172e-05 - accuracy: 1.0000 - val_loss: 2.1457e-05 - val_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.2266e-04 - accuracy: 1.0000 - val_loss: 3.9457e-05 - val_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.4871e-04 - accuracy: 1.0000 - val_loss: 4.0471e-05 - val_accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 1.6787e-04 - accuracy: 1.0000 - val_loss: 3.0577e-05 - val_accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.0707e-04 - accuracy: 1.0000 - val_loss: 2.9027e-05 - val_accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.4638e-04 - accuracy: 1.0000 - val_loss: 3.9428e-05 - val_accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 4.8194e-04 - accuracy: 1.0000 - val_loss: 2.7388e-05 - val_accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.6472e-05 - accuracy: 1.0000 - val_loss: 2.7805e-05 - val_accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.5629e-04 - accuracy: 1.0000 - val_loss: 3.5136e-05 - val_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.9776e-04 - accuracy: 1.0000 - val_loss: 1.8805e-05 - val_accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.7010e-04 - accuracy: 1.0000 - val_loss: 2.6255e-05 - val_accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 4.5645e-05 - accuracy: 1.0000 - val_loss: 2.5987e-05 - val_accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "7/7 [==============================] - 17s 2s/step - loss: 6.4519e-05 - accuracy: 1.0000 - val_loss: 3.3587e-05 - val_accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.0006e-04 - accuracy: 1.0000 - val_loss: 2.4736e-05 - val_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.8889e-04 - accuracy: 1.0000 - val_loss: 1.6540e-05 - val_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.7042e-04 - accuracy: 1.0000 - val_loss: 2.4765e-05 - val_accuracy: 1.0000\n",
      "Epoch 150/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.5984e-04 - accuracy: 1.0000 - val_loss: 2.4527e-05 - val_accuracy: 1.0000\n",
      "Epoch 151/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 8.0887e-05 - accuracy: 1.0000 - val_loss: 1.7613e-05 - val_accuracy: 1.0000\n",
      "Epoch 152/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.0805e-04 - accuracy: 1.0000 - val_loss: 3.0368e-05 - val_accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.3903e-04 - accuracy: 1.0000 - val_loss: 2.3663e-05 - val_accuracy: 1.0000\n",
      "Epoch 154/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 4.4004e-05 - accuracy: 1.0000 - val_loss: 2.4437e-05 - val_accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.5931e-04 - accuracy: 1.0000 - val_loss: 2.2947e-05 - val_accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.1571e-05 - accuracy: 1.0000 - val_loss: 2.8938e-05 - val_accuracy: 1.0000\n",
      "Epoch 157/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.7920e-04 - accuracy: 1.0000 - val_loss: 2.2232e-05 - val_accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 5.3693e-05 - accuracy: 1.0000 - val_loss: 1.5199e-05 - val_accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.3596e-04 - accuracy: 1.0000 - val_loss: 2.1636e-05 - val_accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.6129e-04 - accuracy: 1.0000 - val_loss: 2.8401e-05 - val_accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.0019e-04 - accuracy: 1.0000 - val_loss: 2.1010e-05 - val_accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 7.6648e-05 - accuracy: 1.0000 - val_loss: 2.7209e-05 - val_accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.5608e-05 - accuracy: 1.0000 - val_loss: 1.4007e-05 - val_accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.2369e-04 - accuracy: 1.0000 - val_loss: 2.0086e-05 - val_accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 4.8608e-05 - accuracy: 1.0000 - val_loss: 1.9818e-05 - val_accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 9.0511e-05 - accuracy: 1.0000 - val_loss: 1.9908e-05 - val_accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 9.3448e-05 - accuracy: 1.0000 - val_loss: 1.3709e-05 - val_accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 8.5161e-05 - accuracy: 1.0000 - val_loss: 2.4944e-05 - val_accuracy: 1.0000\n",
      "Epoch 169/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 3.9214e-04 - accuracy: 1.0000 - val_loss: 1.8745e-05 - val_accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.9287e-04 - accuracy: 1.0000 - val_loss: 2.4110e-05 - val_accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 7.9748e-04 - accuracy: 1.0000 - val_loss: 1.2189e-05 - val_accuracy: 1.0000\n",
      "Epoch 172/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 6.2218e-05 - accuracy: 1.0000 - val_loss: 2.1308e-05 - val_accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.3226e-05 - accuracy: 1.0000 - val_loss: 1.6689e-05 - val_accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 4.1686e-04 - accuracy: 1.0000 - val_loss: 2.0444e-05 - val_accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.0554e-05 - accuracy: 1.0000 - val_loss: 1.8060e-05 - val_accuracy: 1.0000\n",
      "Epoch 176/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.1519e-04 - accuracy: 1.0000 - val_loss: 1.9967e-05 - val_accuracy: 1.0000\n",
      "Epoch 177/1000\n",
      "7/7 [==============================] - 17s 3s/step - loss: 4.7272e-05 - accuracy: 1.0000 - val_loss: 1.5944e-05 - val_accuracy: 1.0000\n",
      "Epoch 178/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 4.9352e-05 - accuracy: 1.0000 - val_loss: 1.1027e-05 - val_accuracy: 1.0000\n",
      "Epoch 179/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 7.2744e-05 - accuracy: 1.0000 - val_loss: 1.4812e-05 - val_accuracy: 1.0000\n",
      "Epoch 180/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 3.7923e-04 - accuracy: 1.0000 - val_loss: 1.6391e-05 - val_accuracy: 1.0000\n",
      "Epoch 181/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.2961e-04 - accuracy: 1.0000 - val_loss: 1.4812e-05 - val_accuracy: 1.0000\n",
      "Epoch 182/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 4.7722e-05 - accuracy: 1.0000 - val_loss: 9.8645e-06 - val_accuracy: 1.0000\n",
      "Epoch 183/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.6767e-04 - accuracy: 1.0000 - val_loss: 1.5855e-05 - val_accuracy: 1.0000\n",
      "Epoch 184/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 2.9653e-04 - accuracy: 1.0000 - val_loss: 1.7494e-05 - val_accuracy: 1.0000\n",
      "Epoch 185/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.9668e-05 - accuracy: 1.0000 - val_loss: 1.3500e-05 - val_accuracy: 1.0000\n",
      "Epoch 186/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 5.2503e-05 - accuracy: 1.0000 - val_loss: 1.8328e-05 - val_accuracy: 1.0000\n",
      "Epoch 187/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 6.3474e-05 - accuracy: 1.0000 - val_loss: 8.0764e-06 - val_accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 8.0604e-05 - accuracy: 1.0000 - val_loss: 1.7196e-05 - val_accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.2912e-05 - accuracy: 1.0000 - val_loss: 1.2815e-05 - val_accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.5504e-04 - accuracy: 1.0000 - val_loss: 1.2666e-05 - val_accuracy: 1.0000\n",
      "Epoch 191/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 9.3967e-05 - accuracy: 1.0000 - val_loss: 1.0729e-05 - val_accuracy: 1.0000\n",
      "Epoch 192/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 8.8623e-05 - accuracy: 1.0000 - val_loss: 1.7226e-05 - val_accuracy: 1.0000\n",
      "Epoch 193/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 1.0338e-04 - accuracy: 1.0000 - val_loss: 1.2249e-05 - val_accuracy: 1.0000\n",
      "Epoch 194/1000\n",
      "7/7 [==============================] - 16s 2s/step - loss: 2.8226e-05 - accuracy: 1.0000 - val_loss: 1.2129e-05 - val_accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 1.0174e-04 - accuracy: 1.0000 - val_loss: 1.6570e-05 - val_accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "7/7 [==============================] - 15s 2s/step - loss: 3.7795e-05 - accuracy: 1.0000 - val_loss: 1.1712e-05 - val_accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.2318e-05 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 187.\n",
      "7/7 [==============================] - 15s 2s/step - loss: 7.2318e-05 - accuracy: 1.0000 - val_loss: 1.1593e-05 - val_accuracy: 1.0000\n",
      "Epoch 197: early stopping\n",
      "Epoch 1/1000\n",
      "7/7 [==============================] - 14s 1s/step - loss: 1.3320 - accuracy: 0.4286 - val_loss: 1.1133 - val_accuracy: 0.7500\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.1473 - accuracy: 0.5357 - val_loss: 1.1212 - val_accuracy: 0.7500\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.9247 - accuracy: 0.7500 - val_loss: 0.4925 - val_accuracy: 0.7500\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.7707 - accuracy: 0.7143 - val_loss: 0.7921 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.5378 - accuracy: 0.7857 - val_loss: 0.3830 - val_accuracy: 0.7500\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.4024 - accuracy: 0.8929 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2627 - accuracy: 0.9643 - val_loss: 0.2709 - val_accuracy: 1.0000\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2480 - accuracy: 0.8929 - val_loss: 0.1696 - val_accuracy: 1.0000\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1166 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 9s 2s/step - loss: 0.0453 - accuracy: 0.9643 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0648 - accuracy: 0.9643 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0746 - accuracy: 0.9643 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.1038 - accuracy: 0.9643 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0654 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 8.7061e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 6.2463e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 7.5267e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 6.6554e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 9.2359e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 8.0771e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.3458e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 5.1476e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 9.2162e-04 - accuracy: 1.0000 - val_loss: 2.2950e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 6.6894e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.9318e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.2908e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 7.9666e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.4987e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 5.3395e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 6.3249e-04 - accuracy: 1.0000 - val_loss: 4.3099e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 6.9063e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 4.2647e-04 - accuracy: 1.0000 - val_loss: 1.9523e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 7.1042e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 8.9936e-04 - accuracy: 1.0000 - val_loss: 4.1102e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.1551e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 6.5294e-04 - accuracy: 1.0000 - val_loss: 5.2156e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 4.2349e-04 - accuracy: 1.0000 - val_loss: 6.3563e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000  Restoring model weights from the end of the best epoch: 39.\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.5851e-04 - val_accuracy: 1.0000\n",
      "Epoch 49: early stopping\n",
      "Epoch 1/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 1.5510 - accuracy: 0.2500 - val_loss: 1.2191 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.2384 - accuracy: 0.5357 - val_loss: 0.9647 - val_accuracy: 1.0000\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.0920 - accuracy: 0.5714 - val_loss: 0.6425 - val_accuracy: 1.0000\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 9s 2s/step - loss: 0.8774 - accuracy: 0.7500 - val_loss: 0.8626 - val_accuracy: 0.7500\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 10s 2s/step - loss: 0.7432 - accuracy: 0.7143 - val_loss: 0.6240 - val_accuracy: 0.7500\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.6228 - accuracy: 0.7143 - val_loss: 0.5356 - val_accuracy: 0.7500\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.5660 - accuracy: 0.7500 - val_loss: 0.4920 - val_accuracy: 0.7500\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.5028 - accuracy: 0.7857 - val_loss: 0.5449 - val_accuracy: 0.7500\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.4231 - accuracy: 0.8214 - val_loss: 0.3748 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.4100 - accuracy: 0.9286 - val_loss: 0.3799 - val_accuracy: 0.7500\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.2885 - accuracy: 0.9643 - val_loss: 0.1498 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.2003 - accuracy: 1.0000 - val_loss: 0.1953 - val_accuracy: 1.0000\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 0.1343 - accuracy: 0.9643 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 6s 990ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 6s 966ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 6s 933ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 6s 944ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 6s 960ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 6s 974ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 6s 963ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 6s 907ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 5s 882ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 5s 881ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 5s 885ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 6s 905ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 5s 899ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 6s 946ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 5s 873ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 5s 895ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 6s 912ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 5s 889ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 5s 865ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 5s 880ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 5s 888ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 6s 948ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 5s 896ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 6s 903ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 5s 895ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 5s 883ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 5s 894ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 6s 953ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 6s 924ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 9.5717e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 6s 916ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 5s 884ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 5s 843ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 5s 886ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 6s 924ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 6s 929ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 6s 904ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 5s 891ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 6s 939ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 48.\n",
      "7/7 [==============================] - 6s 938ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 58: early stopping\n",
      "Epoch 1/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 1.4809 - accuracy: 0.2143 - val_loss: 1.2217 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 6s 948ms/step - loss: 1.1517 - accuracy: 0.5000 - val_loss: 1.0310 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 1.0746 - accuracy: 0.4643 - val_loss: 1.0377 - val_accuracy: 0.2500\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 6s 958ms/step - loss: 0.9440 - accuracy: 0.5714 - val_loss: 1.0706 - val_accuracy: 0.2500\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 6s 1s/step - loss: 0.9348 - accuracy: 0.5000 - val_loss: 0.9126 - val_accuracy: 0.7500\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 6s 974ms/step - loss: 0.8603 - accuracy: 0.6071 - val_loss: 0.7109 - val_accuracy: 0.7500\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 6s 868ms/step - loss: 0.7028 - accuracy: 0.5714 - val_loss: 0.5659 - val_accuracy: 1.0000\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 6s 914ms/step - loss: 0.6475 - accuracy: 0.7143 - val_loss: 0.3350 - val_accuracy: 0.7500\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 6s 975ms/step - loss: 0.4713 - accuracy: 0.7857 - val_loss: 0.3147 - val_accuracy: 1.0000\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.3610 - accuracy: 0.8929 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 6s 999ms/step - loss: 0.2455 - accuracy: 0.9286 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 6s 948ms/step - loss: 0.1754 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 1.0000\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 6s 948ms/step - loss: 0.1326 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 6s 941ms/step - loss: 0.1145 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 1.0000\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 6s 917ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 6s 917ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 6s 933ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 6s 997ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 6s 917ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 6s 904ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 6s 915ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 6s 928ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 9s 2s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 6s 873ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 6s 926ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 10s 2s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.7735e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 9.5565e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 7.7363e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 5.4055e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 9.0728e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 9.5006e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 8.5488e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.4659e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 8s 1s/step - loss: 9.4513e-04 - accuracy: 1.0000 - val_loss: 8.6194e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.1750e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 6s 915ms/step - loss: 8.1728e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 6s 933ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 7.2546e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 6s 968ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 7.6681e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 6s 925ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 6s 888ms/step - loss: 9.7741e-04 - accuracy: 1.0000 - val_loss: 5.1979e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 6s 866ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 6.6623e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 6s 920ms/step - loss: 8.7503e-04 - accuracy: 1.0000 - val_loss: 6.4670e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 6s 882ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.3050e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 6s 902ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.8390e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 62.\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 6.2334e-04 - val_accuracy: 1.0000\n",
      "Epoch 72: early stopping\n"
     ]
    }
   ],
   "source": [
    "intra_training_progress = dict()\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    intra_training_progress[name] = model.fit(\n",
    "        keras_data_generator(INTRA_TRAIN_FOLDER, 4, intra_preprocessing_pipeline),\n",
    "        steps_per_epoch=7,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=keras_data_generator(INTRA_VAL_FOLDER, 1, intra_preprocessing_pipeline),\n",
    "        validation_steps=4,\n",
    "        callbacks=[early_stopping],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e1679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "LSTM Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.2630544900894165,
          0.9211754202842712,
          0.5982221961021423,
          0.4075344204902649,
          0.22418205440044403,
          0.13924908638000488,
          0.085673026740551,
          0.06268109381198883,
          0.06360185891389847,
          0.051285333931446075,
          0.029494566842913628,
          0.019326869398355484,
          0.033614274114370346,
          0.02209964580833912,
          0.015392781235277653,
          0.010976316407322884,
          0.009357503615319729,
          0.009380832314491272,
          0.009683649055659771,
          0.015776371583342552,
          0.008545923046767712,
          0.00897859875112772,
          0.00841523427516222,
          0.007084954064339399,
          0.006465264596045017,
          0.008318507112562656,
          0.006391818635165691,
          0.004886370152235031,
          0.004576980601996183,
          0.0046919179148972034,
          0.00333803822286427,
          0.0035879197530448437,
          0.004963429179042578,
          0.004751001484692097,
          0.0029077401850372553,
          0.004597132559865713,
          0.00334186851978302,
          0.003379715606570244,
          0.004334493074566126,
          0.004420392215251923,
          0.0035059000365436077,
          0.003871456952765584,
          0.004021564964205027,
          0.0018896693363785744,
          0.0010206684237346053,
          0.004279495682567358,
          0.005507190246134996,
          0.0028640918899327517,
          0.0029386214446276426,
          0.0023665849585086107,
          0.002422855468466878,
          0.001960292225703597,
          0.0020743918139487505,
          0.0032695927657186985,
          0.002760844072327018,
          0.0012565319193527102,
          0.0017613882664591074,
          0.0021718121133744717,
          0.0036166405770927668,
          0.0018373666098341346,
          0.0016824129270389676,
          0.0011709848186001182,
          0.0010382502805441618,
          0.0022208804730325937,
          0.002629517111927271,
          0.0017033397452905774,
          0.0008956425590440631,
          0.00075399229535833,
          0.003380885347723961,
          0.0009399596601724625,
          0.0015140889445319772,
          0.0017830833094194531,
          0.0011932461056858301,
          0.0009747726144269109,
          0.0009987486992031336,
          0.0009978949092328548,
          0.0007697267574258149,
          0.000837709114421159,
          0.0009538658778183162,
          0.001307945465669036,
          0.0006144580547697842,
          0.0004923717933706939,
          0.0011788801057264209,
          0.0007405448122881353,
          0.0005641383468173444,
          0.0009239260689355433,
          0.0014130722265690565,
          0.0013505229726433754,
          0.0010348232463002205,
          0.0011148771736770868,
          0.0005341245559975505,
          0.000623350846581161,
          0.0013362501049414277
         ]
        },
        {
         "mode": "lines",
         "name": "Bidirectional LSTM Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.2366946935653687,
          0.8651623129844666,
          0.6182160377502441,
          0.4258226752281189,
          0.2420949935913086,
          0.17511792480945587,
          0.11250413209199905,
          0.058806050568819046,
          0.04926959425210953,
          0.02382032386958599,
          0.023088768124580383,
          0.020978765562176704,
          0.017511367797851562,
          0.014906889759004116,
          0.010330435819923878,
          0.02924300916492939,
          0.009722496382892132,
          0.005973730236291885,
          0.005692760460078716,
          0.007220453582704067,
          0.0061488887295126915,
          0.00436810776591301,
          0.004673541523516178,
          0.0037987916730344296,
          0.004638420883566141,
          0.0029574385844171047,
          0.002325089881196618,
          0.003340161871165037,
          0.001971409423276782,
          0.003441011533141136,
          0.0028515213634818792,
          0.001890593208372593,
          0.002229623729363084,
          0.001973626436665654,
          0.003333505941554904,
          0.0034952503629028797,
          0.002887209178879857,
          0.004110424313694239,
          0.0015932100359350443,
          0.001567973755300045,
          0.001095445011742413,
          0.0014243535697460175,
          0.005995337851345539,
          0.0024246443063020706,
          0.0011880230158567429,
          0.0018843368161469698,
          0.001516007469035685,
          0.0007839050958864391,
          0.0010800630552694201,
          0.0006203847588039935,
          0.00080525764496997,
          0.0010462647769600153,
          0.0017307826783508062,
          0.0009544365457259119,
          0.000764150929171592,
          0.0008108341135084629,
          0.0008473755442537367,
          0.0011735608568415046,
          0.00092457135906443,
          0.001162625034339726,
          0.001638690009713173,
          0.0013064233353361487,
          0.0008733021677471697,
          0.0017872543539851904,
          0.0012256635818630457,
          0.000981251592747867,
          0.0016634167404845357,
          0.0003740016254596412,
          0.0005461201653815806,
          0.0004174663336016238,
          0.00033265078673139215,
          0.0009157445165328681,
          0.0009335108916275203,
          0.00037533906288444996,
          0.0003485751512926072,
          0.00035886300611309707,
          0.00040750138578005135,
          0.00039505932363681495,
          0.00023432410671375692,
          0.00022525805979967117,
          0.00017550600750837475,
          0.0002672297414392233,
          0.00040581077337265015,
          0.00021040512365289032,
          0.00032888021087273955,
          0.0006582639762200415,
          0.00029395450837910175,
          0.0012947677168995142,
          0.00038463613600470126,
          0.0003016982809640467,
          0.0003017554699908942,
          0.00017711054533720016,
          0.00039448303868994117,
          0.000635076139587909,
          0.0003505106142256409,
          0.00013218207459431142,
          0.0003212853625882417,
          0.00015667905972804874,
          0.0007411086116917431,
          0.0006270976155065,
          0.00027910704375244677,
          0.00021661743812728673,
          0.00015592013369314373,
          0.00009554003190714866,
          0.0002233471896033734,
          0.00010170343011850491,
          0.00013641869009006768,
          0.0004658685647882521,
          0.0003132251149509102,
          0.0005589748034253716,
          0.00037340246490202844,
          0.0002296502498211339,
          0.000115664501208812,
          0.00011574999371077865,
          0.00023864868853706867,
          0.0001691573124844581,
          0.0006611123098991811,
          0.0005857101059518754,
          0.00016524137754458934,
          0.00025587392156012356,
          0.00009919578587869182,
          0.00006355477671604604,
          0.00023529065947514027,
          0.0005893200868740678,
          0.00025288405595347285,
          0.00010292822116753086,
          0.00006082875916035846,
          0.000086478547018487,
          0.00014380949141923338,
          0.00006098840822232887,
          0.00019926225650124252,
          0.00007767575880279765,
          0.00014359068882185966,
          0.00006317169027170166,
          0.00012266340490896255,
          0.00014871385064907372,
          0.00016787309141363949,
          0.00010706579632824287,
          0.00024637722526676953,
          0.00048194456030614674,
          0.00006647171539952978,
          0.0003562946221791208,
          0.00019776080443989486,
          0.0001700967113720253,
          0.000045644930651178584,
          0.00006451914669014513,
          0.00010005739750340581,
          0.0001888937986223027,
          0.00027041841531172395,
          0.0001598389499122277,
          0.00008088714821496978,
          0.000708053819835186,
          0.00013902669888921082,
          0.00004400371472002007,
          0.0002593141107354313,
          0.00006157091411296278,
          0.00017919579113367945,
          0.000053693376685259864,
          0.00013595861673820764,
          0.00016128772404044867,
          0.00010018593457061797,
          0.00007664804434170946,
          0.00009560842590872198,
          0.0001236864918610081,
          0.00004860811532125808,
          0.00009051140659721568,
          0.0000934480267460458,
          0.00008516143134329468,
          0.00039213712443597615,
          0.00019287217583041638,
          0.0007974825566634536,
          0.0000622180086793378,
          0.00007322604506043717,
          0.0004168576269876212,
          0.00007055389869492501,
          0.00011518855899339542,
          0.00004727181658381596,
          0.00004935178003506735,
          0.00007274440577020869,
          0.0003792321658693254,
          0.0001296060800086707,
          0.00004772192914970219,
          0.00026767331291921437,
          0.0002965275489259511,
          0.0000696678544045426,
          0.000052503255574265495,
          0.00006347446469590068,
          0.00008060387335717678,
          0.000022912310669198632,
          0.00015504278417211026,
          0.000093966766144149,
          0.00008862333197612315,
          0.00010337509593227878,
          0.00002822574788297061,
          0.00010173947521252558,
          0.00003779471808229573,
          0.00007231840572785586
         ]
        },
        {
         "mode": "lines",
         "name": "CNN Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.3319960832595825,
          1.147304892539978,
          0.9246909022331238,
          0.7706685662269592,
          0.5377654433250427,
          0.4023682475090027,
          0.2626672089099884,
          0.24803443253040314,
          0.11656550318002701,
          0.11981774866580963,
          0.056544121354818344,
          0.03467366099357605,
          0.04530753567814827,
          0.04478805139660835,
          0.038433365523815155,
          0.02999875321984291,
          0.01319078542292118,
          0.06478836387395859,
          0.07463397085666656,
          0.1038060262799263,
          0.06544304639101028,
          0.013543887063860893,
          0.01844150200486183,
          0.006132776383310556,
          0.008594295009970665,
          0.00243379850871861,
          0.0016793422400951385,
          0.012481275014579296,
          0.004444464109838009,
          0.002568031894043088,
          0.001515594427473843,
          0.0013280122075229883,
          0.0017576878890395164,
          0.0009216175531037152,
          0.0018100840970873833,
          0.0018061771988868713,
          0.0019224350107833743,
          0.0026442506350576878,
          0.001722427667118609,
          0.0037469076924026012,
          0.000632492417935282,
          0.0024531628005206585,
          0.0004264657909516245,
          0.0016989102587103844,
          0.0008993562660180032,
          0.0012162895873188972,
          0.0006529386155307293,
          0.0004234866064507514,
          0.001300737843848765
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + self attention Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.5509897470474243,
          1.2384049892425537,
          1.0920363664627075,
          0.8774401545524597,
          0.7432451248168945,
          0.6228328943252563,
          0.565987229347229,
          0.5027664303779602,
          0.4230509102344513,
          0.4099789559841156,
          0.2885051667690277,
          0.20027275383472443,
          0.12510864436626434,
          0.13429339230060577,
          0.075721375644207,
          0.0623716302216053,
          0.06626537442207336,
          0.03386026993393898,
          0.03012043423950672,
          0.047330342233181,
          0.026164794340729713,
          0.019962364807724953,
          0.01732471026480198,
          0.016579139977693558,
          0.01413075439631939,
          0.009802011772990227,
          0.014985167421400547,
          0.006761257536709309,
          0.006381705403327942,
          0.009712384082376957,
          0.009158780798316002,
          0.010510780848562717,
          0.006616857834160328,
          0.006708218716084957,
          0.005954784341156483,
          0.010213027708232403,
          0.005896850489079952,
          0.0049995011650025845,
          0.0026365630328655243,
          0.004466672893613577,
          0.0032429664861410856,
          0.0033152219839394093,
          0.0032592874485999346,
          0.0035485534463077784,
          0.0037424121983349323,
          0.0017803673399612308,
          0.002553520957008004,
          0.0026776862796396017,
          0.002048013499006629,
          0.002532311249524355,
          0.003251005196943879,
          0.001646946300752461,
          0.0049768309108912945,
          0.0017845307011157274,
          0.0044354586862027645,
          0.0023498195223510265,
          0.0021583628840744495,
          0.0016621036920696497
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + multihead attention Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.4809151887893677,
          1.1516693830490112,
          1.0745794773101807,
          0.944028913974762,
          0.9347926378250122,
          0.8602938652038574,
          0.7028185129165649,
          0.6474791765213013,
          0.4712543785572052,
          0.36103320121765137,
          0.24551378190517426,
          0.1754123419523239,
          0.1325760930776596,
          0.11448293179273605,
          0.06616272777318954,
          0.04258314147591591,
          0.04524262994527817,
          0.032782040536403656,
          0.021204626187682152,
          0.02568979747593403,
          0.011860032565891743,
          0.014734407886862755,
          0.013575619086623192,
          0.018793683499097824,
          0.015753168612718582,
          0.011623806320130825,
          0.005746112670749426,
          0.01457586232572794,
          0.007446471601724625,
          0.005379394628107548,
          0.003909624181687832,
          0.006957561708986759,
          0.005224655847996473,
          0.005794145632535219,
          0.00422996049746871,
          0.0033594483975321054,
          0.005611725151538849,
          0.00814969651401043,
          0.005193620454519987,
          0.006066040601581335,
          0.002880657324567437,
          0.006508621387183666,
          0.004734579939395189,
          0.002053422387689352,
          0.004002024419605732,
          0.003822634695097804,
          0.0019533715676516294,
          0.0027809764724224806,
          0.0026713632978498936,
          0.0025176305789500475,
          0.0022474383004009724,
          0.0016931224381551147,
          0.0020395475439727306,
          0.0026415397878736258,
          0.001400856883265078,
          0.0009072785614989698,
          0.001476070610806346,
          0.0021960423327982426,
          0.003255906980484724,
          0.0014911983162164688,
          0.000945130770560354,
          0.0023961844854056835,
          0.0008172803209163249,
          0.002324212808161974,
          0.0012758321827277541,
          0.0018786529544740915,
          0.000977405346930027,
          0.0019981712102890015,
          0.0008750266279093921,
          0.0020364171359688044,
          0.0013258723774924874,
          0.003809490706771612
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Intra Training Loss over Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "for name, training in intra_training_progress.items():\n",
    "    fig.add_trace(go.Scatter(x=np.arange(EPOCHS), y=training.history[\"loss\"], mode='lines', name=f\"{name} Training Loss\"))\n",
    "\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Intra Training Loss over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    yaxis_type='log',         # <-- this makes the y-axis logarithmic\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0934f51",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe81d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing intra and cross results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af777f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 450ms/step - loss: 3.4880e-04 - accuracy: 1.0000\n",
      "Model: LSTM, Loss: 0.0003487993963062763, Accuracy: 1.0\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 1.1951e-05 - accuracy: 1.0000\n",
      "Model: Bidirectional LSTM, Loss: 1.1950623957091011e-05, Accuracy: 1.0\n",
      "4/4 [==============================] - 1s 362ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Model: CNN, Loss: 0.0014628693461418152, Accuracy: 1.0\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Model: CNN + self attention, Loss: 0.001731989672407508, Accuracy: 1.0\n",
      "4/4 [==============================] - 2s 451ms/step - loss: 8.3668e-04 - accuracy: 1.0000\n",
      "Model: CNN + multihead attention, Loss: 0.0008366784313693643, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "    loss, accuracy = model.evaluate(\n",
    "        keras_data_generator(\n",
    "            INTRA_TEST_FOLDER,\n",
    "            batch_size=2,\n",
    "            preprocessing_pipeline=intra_preprocessing_pipeline,\n",
    "        ),\n",
    "        steps=4,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    results.append({\"Model\": name, \"Task\": \"Intra\", \"Loss\": loss, \"Accuracy\": accuracy})\n",
    "\n",
    "    print(f\"Model: {name}, Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcfe22",
   "metadata": {},
   "source": [
    "# Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "237da414",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROSS_TRAIN_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Cross/train/\"))\n",
    "CROSS_VAL_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Cross/val/\"))\n",
    "CROSS_TEST_1_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Cross/test1/\"))\n",
    "CROSS_TEST_2_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Cross/test2/\"))\n",
    "CROSS_TEST_3_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Cross/test3/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f818a3b",
   "metadata": {},
   "source": [
    "We should also scale the features, now based on data from multiple subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values: (248,), Max values: (248,)\n"
     ]
    }
   ],
   "source": [
    "min_val, max_val = learn_minmax_from_all_files(CROSS_TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19865a88",
   "metadata": {},
   "source": [
    "And define a preprocessing pipeline, it is the same one as before. The order only matters for computation cost, but that is slightly irrelevant for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db5b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val), \n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99be4f7",
   "metadata": {},
   "source": [
    "## Cross - training loop\n",
    "\n",
    "Using the same loop as before, we train the models. We have a slightly lower initial learning rate (0.00005) compared to intra, because we noticed that this helps with convergence, probably due to a harder landscape created by more variety in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ff5642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 3.6604 - accuracy: 0.2500 - val_loss: 3.0079 - val_accuracy: 0.2500\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 3.1581 - accuracy: 0.3750 - val_loss: 2.6318 - val_accuracy: 0.3750\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 2.7859 - accuracy: 0.3571 - val_loss: 2.5225 - val_accuracy: 0.3750\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 2.3741 - accuracy: 0.4464 - val_loss: 2.1095 - val_accuracy: 0.3750\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 2.2309 - accuracy: 0.5000 - val_loss: 1.9215 - val_accuracy: 0.3750\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 2.1307 - accuracy: 0.5357 - val_loss: 1.7852 - val_accuracy: 0.3750\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 2.0996 - accuracy: 0.4643 - val_loss: 1.6894 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.8432 - accuracy: 0.5357 - val_loss: 1.4108 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.5841 - accuracy: 0.5179 - val_loss: 1.3490 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.3661 - accuracy: 0.5179 - val_loss: 1.3178 - val_accuracy: 0.6250\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1410 - accuracy: 0.5536 - val_loss: 1.1554 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0480 - accuracy: 0.6071 - val_loss: 1.0279 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.9405 - accuracy: 0.5536 - val_loss: 0.8765 - val_accuracy: 0.6250\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.8461 - accuracy: 0.6607 - val_loss: 0.7575 - val_accuracy: 0.6250\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.8463 - accuracy: 0.6429 - val_loss: 0.6892 - val_accuracy: 0.6250\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9174 - accuracy: 0.5357 - val_loss: 0.6285 - val_accuracy: 0.6250\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8620 - accuracy: 0.5714 - val_loss: 0.5698 - val_accuracy: 0.8750\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8429 - accuracy: 0.6607 - val_loss: 0.5398 - val_accuracy: 0.8750\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.8769 - accuracy: 0.6607 - val_loss: 0.5139 - val_accuracy: 0.8750\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7617 - accuracy: 0.6786 - val_loss: 0.4915 - val_accuracy: 0.8750\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7350 - accuracy: 0.6964 - val_loss: 0.4708 - val_accuracy: 0.8750\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7180 - accuracy: 0.7500 - val_loss: 0.4831 - val_accuracy: 0.8750\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6973 - accuracy: 0.7143 - val_loss: 0.4791 - val_accuracy: 0.8750\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7254 - accuracy: 0.7679 - val_loss: 0.4658 - val_accuracy: 0.8750\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7199 - accuracy: 0.7143 - val_loss: 0.4557 - val_accuracy: 0.8750\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6128 - accuracy: 0.8036 - val_loss: 0.4491 - val_accuracy: 0.8750\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6406 - accuracy: 0.7500 - val_loss: 0.4112 - val_accuracy: 0.8750\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5578 - accuracy: 0.7857 - val_loss: 0.4202 - val_accuracy: 0.8750\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5265 - accuracy: 0.8214 - val_loss: 0.3815 - val_accuracy: 1.0000\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6091 - accuracy: 0.7857 - val_loss: 0.4908 - val_accuracy: 0.8750\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7252 - accuracy: 0.7321 - val_loss: 0.4948 - val_accuracy: 0.8750\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7948 - accuracy: 0.6607 - val_loss: 0.5499 - val_accuracy: 0.8750\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7556 - accuracy: 0.6964 - val_loss: 0.5552 - val_accuracy: 0.8750\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7550 - accuracy: 0.7500 - val_loss: 0.5213 - val_accuracy: 0.8750\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6540 - accuracy: 0.7679 - val_loss: 0.4914 - val_accuracy: 0.8750\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6387 - accuracy: 0.7857 - val_loss: 0.4716 - val_accuracy: 0.8750\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5385 - accuracy: 0.8214 - val_loss: 0.4544 - val_accuracy: 0.8750\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6063 - accuracy: 0.7500 - val_loss: 0.3903 - val_accuracy: 0.8750\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.5116 - accuracy: 0.8571Restoring model weights from the end of the best epoch: 29.\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5116 - accuracy: 0.8571 - val_loss: 0.3957 - val_accuracy: 1.0000\n",
      "Epoch 39: early stopping\n",
      "Epoch 1/1000\n",
      "8/8 [==============================] - 19s 2s/step - loss: 3.5462 - accuracy: 0.5000 - val_loss: 2.9420 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 2.7764 - accuracy: 0.5000 - val_loss: 2.1993 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 2.1917 - accuracy: 0.5000 - val_loss: 2.2298 - val_accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.8711 - accuracy: 0.5179 - val_loss: 1.7362 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.5503 - accuracy: 0.5536 - val_loss: 1.4854 - val_accuracy: 0.5000\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3286 - accuracy: 0.5714 - val_loss: 1.2046 - val_accuracy: 0.6250\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2104 - accuracy: 0.5893 - val_loss: 0.9876 - val_accuracy: 0.6250\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0514 - accuracy: 0.6071 - val_loss: 1.0663 - val_accuracy: 0.6250\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9129 - accuracy: 0.6429 - val_loss: 0.8154 - val_accuracy: 0.7500\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7574 - accuracy: 0.6786 - val_loss: 0.8282 - val_accuracy: 0.8750\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7684 - accuracy: 0.7500 - val_loss: 0.8258 - val_accuracy: 0.8750\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7234 - accuracy: 0.7143 - val_loss: 0.7995 - val_accuracy: 0.8750\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.8222 - accuracy: 0.7143 - val_loss: 0.7838 - val_accuracy: 0.8750\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9031 - accuracy: 0.6786 - val_loss: 1.2201 - val_accuracy: 0.7500\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.7049 - accuracy: 0.7857 - val_loss: 0.7633 - val_accuracy: 0.8750\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.8992 - accuracy: 0.6607 - val_loss: 0.7713 - val_accuracy: 0.8750\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.6955 - accuracy: 0.7679 - val_loss: 0.7600 - val_accuracy: 0.8750\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.5085 - accuracy: 0.8571 - val_loss: 0.7846 - val_accuracy: 0.8750\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4216 - accuracy: 0.8571 - val_loss: 0.7841 - val_accuracy: 0.8750\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3682 - accuracy: 0.8929 - val_loss: 0.7599 - val_accuracy: 0.8750\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2895 - accuracy: 0.9107 - val_loss: 0.7385 - val_accuracy: 0.8750\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2701 - accuracy: 0.9464 - val_loss: 0.7334 - val_accuracy: 0.8750\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.2484 - accuracy: 0.9643 - val_loss: 0.7215 - val_accuracy: 0.8750\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.2864 - accuracy: 0.9643 - val_loss: 0.7101 - val_accuracy: 0.8750\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.3766 - accuracy: 0.8750 - val_loss: 0.7008 - val_accuracy: 0.8750\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2914 - accuracy: 0.9464 - val_loss: 0.6773 - val_accuracy: 0.8750\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3987 - accuracy: 0.9286 - val_loss: 0.7108 - val_accuracy: 0.8750\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.3048 - accuracy: 0.8929 - val_loss: 0.6973 - val_accuracy: 0.8750\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.3995 - accuracy: 0.8929 - val_loss: 0.9071 - val_accuracy: 0.7500\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2611 - accuracy: 0.9286 - val_loss: 0.9143 - val_accuracy: 0.7500\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2178 - accuracy: 0.9643 - val_loss: 0.9153 - val_accuracy: 0.7500\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2006 - accuracy: 0.9643 - val_loss: 0.6063 - val_accuracy: 0.8750\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1779 - accuracy: 0.9643 - val_loss: 0.6026 - val_accuracy: 0.8750\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1755 - accuracy: 0.9643 - val_loss: 0.5943 - val_accuracy: 0.8750\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1353 - accuracy: 0.9821 - val_loss: 0.5907 - val_accuracy: 0.8750\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0903 - accuracy: 0.9821 - val_loss: 0.5907 - val_accuracy: 0.8750\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1214 - accuracy: 0.9821 - val_loss: 0.5922 - val_accuracy: 0.8750\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1562 - accuracy: 0.9643 - val_loss: 0.5917 - val_accuracy: 0.8750\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1695 - accuracy: 0.9464 - val_loss: 0.6089 - val_accuracy: 0.8750\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1214 - accuracy: 0.9821 - val_loss: 0.5805 - val_accuracy: 0.8750\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1064 - accuracy: 0.9643 - val_loss: 0.9169 - val_accuracy: 0.7500\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2065 - accuracy: 0.9643 - val_loss: 0.5968 - val_accuracy: 0.8750\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4320 - accuracy: 0.8929 - val_loss: 1.6203 - val_accuracy: 0.6250\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9587 - accuracy: 0.6964 - val_loss: 1.1077 - val_accuracy: 0.6250\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7783 - accuracy: 0.7321 - val_loss: 0.7428 - val_accuracy: 0.6250\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.5728 - accuracy: 0.7143 - val_loss: 0.6029 - val_accuracy: 0.6250\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.5134 - accuracy: 0.7500 - val_loss: 0.5749 - val_accuracy: 0.5000\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.4376 - accuracy: 0.8750 - val_loss: 0.5616 - val_accuracy: 0.6250\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.4017 - accuracy: 0.8571 - val_loss: 0.5459 - val_accuracy: 0.6250\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.4870 - accuracy: 0.7679 - val_loss: 0.5291 - val_accuracy: 0.6250\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.4156 - accuracy: 0.8393 - val_loss: 0.5138 - val_accuracy: 0.7500\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.4409 - accuracy: 0.8036 - val_loss: 0.4870 - val_accuracy: 0.7500\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.5012 - accuracy: 0.8036 - val_loss: 0.4799 - val_accuracy: 0.7500\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.4602 - accuracy: 0.8393 - val_loss: 0.4568 - val_accuracy: 0.7500\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2905 - accuracy: 0.8750 - val_loss: 0.3885 - val_accuracy: 0.8750\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3305 - accuracy: 0.8571 - val_loss: 0.3458 - val_accuracy: 0.8750\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2496 - accuracy: 0.9107 - val_loss: 0.3131 - val_accuracy: 0.8750\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2897 - accuracy: 0.8929 - val_loss: 0.4680 - val_accuracy: 0.7500\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2020 - accuracy: 0.9286 - val_loss: 0.2577 - val_accuracy: 0.8750\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3621 - accuracy: 0.8929 - val_loss: 0.7395 - val_accuracy: 0.6250\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5533 - accuracy: 0.7679 - val_loss: 0.6737 - val_accuracy: 0.6250\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.4962 - accuracy: 0.7857 - val_loss: 0.4260 - val_accuracy: 0.7500\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2765 - accuracy: 0.9107 - val_loss: 0.3087 - val_accuracy: 0.8750\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3626 - accuracy: 0.8571 - val_loss: 0.2998 - val_accuracy: 0.8750\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2109 - accuracy: 0.9643 - val_loss: 0.2863 - val_accuracy: 0.8750\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2572 - accuracy: 0.9286 - val_loss: 0.3892 - val_accuracy: 0.7500\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2234 - accuracy: 0.8750 - val_loss: 0.2510 - val_accuracy: 0.8750\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1628 - accuracy: 0.9643 - val_loss: 0.2404 - val_accuracy: 0.8750\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1749 - accuracy: 0.9464 - val_loss: 0.2321 - val_accuracy: 0.8750\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2327 - accuracy: 0.9464 - val_loss: 0.2270 - val_accuracy: 0.8750\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1748 - accuracy: 0.9464 - val_loss: 0.2226 - val_accuracy: 0.8750\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2117 - accuracy: 0.9643 - val_loss: 0.2192 - val_accuracy: 0.8750\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1826 - accuracy: 0.8929 - val_loss: 0.2178 - val_accuracy: 0.8750\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1570 - accuracy: 0.9643 - val_loss: 0.2177 - val_accuracy: 0.8750\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1554 - accuracy: 0.9464 - val_loss: 0.2119 - val_accuracy: 0.8750\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1592 - accuracy: 0.9643 - val_loss: 0.2109 - val_accuracy: 0.8750\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1286 - accuracy: 0.9643 - val_loss: 0.2068 - val_accuracy: 0.8750\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1270 - accuracy: 0.9643 - val_loss: 0.2023 - val_accuracy: 0.8750\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1324 - accuracy: 0.9643 - val_loss: 0.2089 - val_accuracy: 0.8750\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1560 - accuracy: 0.9464 - val_loss: 0.2216 - val_accuracy: 0.8750\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2993 - accuracy: 0.8929 - val_loss: 0.6325 - val_accuracy: 0.6250\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.4420 - accuracy: 0.8393 - val_loss: 0.5679 - val_accuracy: 0.6250\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1785 - accuracy: 0.9643 - val_loss: 0.2235 - val_accuracy: 0.8750\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1299 - accuracy: 0.9821 - val_loss: 0.2267 - val_accuracy: 0.8750\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1112 - accuracy: 0.9821 - val_loss: 0.2192 - val_accuracy: 0.8750\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0944 - accuracy: 0.9821 - val_loss: 0.2126 - val_accuracy: 0.8750\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1089 - accuracy: 0.9821 - val_loss: 0.2090 - val_accuracy: 0.8750\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1192 - accuracy: 0.9643 - val_loss: 0.2019 - val_accuracy: 0.8750\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1736 - accuracy: 0.9464 - val_loss: 0.1982 - val_accuracy: 0.8750\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2664 - accuracy: 0.9286 - val_loss: 0.1930 - val_accuracy: 0.8750\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1682 - accuracy: 0.9821 - val_loss: 0.1799 - val_accuracy: 0.8750\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.1755 - val_accuracy: 0.8750\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.8750\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0961 - accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.8750\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 0.8750\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0741 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.8750\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1016 - accuracy: 0.9821 - val_loss: 0.1713 - val_accuracy: 0.8750\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0824 - accuracy: 0.9821 - val_loss: 0.1713 - val_accuracy: 0.8750\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0764 - accuracy: 0.9821 - val_loss: 0.1465 - val_accuracy: 0.8750\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0864 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0853 - accuracy: 0.9643 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1109 - accuracy: 0.9464 - val_loss: 0.1038 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0644 - accuracy: 0.9821 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 19s 2s/step - loss: 0.0839 - accuracy: 1.0000 - val_loss: 0.0981 - val_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1038 - accuracy: 0.9464 - val_loss: 0.2018 - val_accuracy: 0.8750\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2757 - accuracy: 0.9107 - val_loss: 0.2290 - val_accuracy: 0.8750\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3238 - accuracy: 0.8929 - val_loss: 0.2317 - val_accuracy: 0.8750\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5084 - accuracy: 0.8214 - val_loss: 0.2072 - val_accuracy: 0.8750\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.5868 - accuracy: 0.7857 - val_loss: 0.2233 - val_accuracy: 0.8750\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.4885 - accuracy: 0.8036 - val_loss: 0.1750 - val_accuracy: 0.8750\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3681 - accuracy: 0.8214 - val_loss: 0.1680 - val_accuracy: 0.8750\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2841 - accuracy: 0.8750 - val_loss: 0.1673 - val_accuracy: 0.8750\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.3195 - accuracy: 0.9107 - val_loss: 0.1675 - val_accuracy: 0.8750\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8571Restoring model weights from the end of the best epoch: 105.\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.4298 - accuracy: 0.8571 - val_loss: 0.4281 - val_accuracy: 0.7500\n",
      "Epoch 115: early stopping\n",
      "Epoch 1/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 6.1002 - accuracy: 0.2500 - val_loss: 5.7462 - val_accuracy: 0.2500\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 4.8798 - accuracy: 0.3929 - val_loss: 4.7564 - val_accuracy: 0.3750\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.8674 - accuracy: 0.4107 - val_loss: 3.6476 - val_accuracy: 0.3750\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.5279 - accuracy: 0.4643 - val_loss: 2.4678 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.0136 - accuracy: 0.5893 - val_loss: 1.9639 - val_accuracy: 0.6250\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.6430 - accuracy: 0.6071 - val_loss: 1.6739 - val_accuracy: 0.6250\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.4403 - accuracy: 0.6071 - val_loss: 1.4322 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.2574 - accuracy: 0.5714 - val_loss: 1.3686 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9552 - accuracy: 0.6429 - val_loss: 1.3598 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8875 - accuracy: 0.6429 - val_loss: 1.3236 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9792 - accuracy: 0.6071 - val_loss: 1.2920 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9565 - accuracy: 0.6250 - val_loss: 1.2774 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8323 - accuracy: 0.6071 - val_loss: 1.2679 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9351 - accuracy: 0.5714 - val_loss: 1.2610 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9205 - accuracy: 0.6071 - val_loss: 1.2419 - val_accuracy: 0.3750\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9005 - accuracy: 0.6429 - val_loss: 1.2067 - val_accuracy: 0.5000\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7305 - accuracy: 0.7321 - val_loss: 1.1788 - val_accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9378 - accuracy: 0.5893 - val_loss: 1.1820 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8026 - accuracy: 0.6250 - val_loss: 1.1493 - val_accuracy: 0.5000\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7887 - accuracy: 0.6964 - val_loss: 1.1194 - val_accuracy: 0.5000\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8069 - accuracy: 0.6429 - val_loss: 1.1113 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7531 - accuracy: 0.6964 - val_loss: 1.1024 - val_accuracy: 0.5000\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8319 - accuracy: 0.6250 - val_loss: 1.0812 - val_accuracy: 0.5000\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7481 - accuracy: 0.6786 - val_loss: 1.0704 - val_accuracy: 0.3750\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7648 - accuracy: 0.6607 - val_loss: 1.0565 - val_accuracy: 0.5000\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8483 - accuracy: 0.6250 - val_loss: 1.0329 - val_accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7059 - accuracy: 0.7143 - val_loss: 1.0248 - val_accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7721 - accuracy: 0.6250 - val_loss: 1.0132 - val_accuracy: 0.5000\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7245 - accuracy: 0.6250 - val_loss: 0.9945 - val_accuracy: 0.5000\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6608 - accuracy: 0.6964 - val_loss: 0.9809 - val_accuracy: 0.5000\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7548 - accuracy: 0.5893 - val_loss: 0.9585 - val_accuracy: 0.5000\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6709 - accuracy: 0.6250 - val_loss: 0.9711 - val_accuracy: 0.5000\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6011 - accuracy: 0.7500 - val_loss: 0.9444 - val_accuracy: 0.5000\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6845 - accuracy: 0.6607 - val_loss: 0.9195 - val_accuracy: 0.5000\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6317 - accuracy: 0.7679 - val_loss: 0.9140 - val_accuracy: 0.5000\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.7364 - accuracy: 0.6071 - val_loss: 0.9028 - val_accuracy: 0.5000\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6026 - accuracy: 0.6786 - val_loss: 0.8803 - val_accuracy: 0.5000\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6321 - accuracy: 0.6964 - val_loss: 0.8647 - val_accuracy: 0.5000\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6010 - accuracy: 0.7679 - val_loss: 0.8640 - val_accuracy: 0.5000\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5730 - accuracy: 0.7321 - val_loss: 0.8608 - val_accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6441 - accuracy: 0.6071 - val_loss: 0.8451 - val_accuracy: 0.5000\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6329 - accuracy: 0.7143 - val_loss: 0.8434 - val_accuracy: 0.5000\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5321 - accuracy: 0.8214 - val_loss: 0.8414 - val_accuracy: 0.6250\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6069 - accuracy: 0.7321 - val_loss: 0.8165 - val_accuracy: 0.5000\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5425 - accuracy: 0.7321 - val_loss: 0.8056 - val_accuracy: 0.5000\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5191 - accuracy: 0.7500 - val_loss: 0.7861 - val_accuracy: 0.5000\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6158 - accuracy: 0.7143 - val_loss: 0.7823 - val_accuracy: 0.6250\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5933 - accuracy: 0.7143 - val_loss: 0.7737 - val_accuracy: 0.5000\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5788 - accuracy: 0.6964 - val_loss: 0.7729 - val_accuracy: 0.5000\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.5734 - accuracy: 0.7321 - val_loss: 0.7447 - val_accuracy: 0.5000\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6097 - accuracy: 0.7321 - val_loss: 0.7356 - val_accuracy: 0.6250\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.5769 - accuracy: 0.7143 - val_loss: 0.7244 - val_accuracy: 0.5000\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5277 - accuracy: 0.7679 - val_loss: 0.7389 - val_accuracy: 0.5000\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5345 - accuracy: 0.7679 - val_loss: 0.7310 - val_accuracy: 0.6250\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5720 - accuracy: 0.7143 - val_loss: 0.7076 - val_accuracy: 0.5000\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4457 - accuracy: 0.8036 - val_loss: 0.7038 - val_accuracy: 0.6250\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5424 - accuracy: 0.7500 - val_loss: 0.6946 - val_accuracy: 0.6250\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5191 - accuracy: 0.7857 - val_loss: 0.6949 - val_accuracy: 0.5000\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5374 - accuracy: 0.7500 - val_loss: 0.6979 - val_accuracy: 0.6250\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5421 - accuracy: 0.7857 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5437 - accuracy: 0.6964 - val_loss: 0.6615 - val_accuracy: 0.6250\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5865 - accuracy: 0.7321 - val_loss: 0.6472 - val_accuracy: 0.6250\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4662 - accuracy: 0.8036 - val_loss: 0.6594 - val_accuracy: 0.6250\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5527 - accuracy: 0.6964 - val_loss: 0.6386 - val_accuracy: 0.6250\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4334 - accuracy: 0.8571 - val_loss: 0.6357 - val_accuracy: 0.6250\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4894 - accuracy: 0.7857 - val_loss: 0.6196 - val_accuracy: 0.5000\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4614 - accuracy: 0.8214 - val_loss: 0.6320 - val_accuracy: 0.5000\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4259 - accuracy: 0.8214 - val_loss: 0.6113 - val_accuracy: 0.5000\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3890 - accuracy: 0.8571 - val_loss: 0.5952 - val_accuracy: 0.6250\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4870 - accuracy: 0.8036 - val_loss: 0.5878 - val_accuracy: 0.6250\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5084 - accuracy: 0.8214 - val_loss: 0.5910 - val_accuracy: 0.6250\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4105 - accuracy: 0.8393 - val_loss: 0.5800 - val_accuracy: 0.5000\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4503 - accuracy: 0.8036 - val_loss: 0.5649 - val_accuracy: 0.6250\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4339 - accuracy: 0.8214 - val_loss: 0.5538 - val_accuracy: 0.6250\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4580 - accuracy: 0.8036 - val_loss: 0.5432 - val_accuracy: 0.6250\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4592 - accuracy: 0.8036 - val_loss: 0.5594 - val_accuracy: 0.6250\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4484 - accuracy: 0.8214 - val_loss: 0.5471 - val_accuracy: 0.6250\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4255 - accuracy: 0.8214 - val_loss: 0.5272 - val_accuracy: 0.6250\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4354 - accuracy: 0.8214 - val_loss: 0.5324 - val_accuracy: 0.7500\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4079 - accuracy: 0.7679 - val_loss: 0.5322 - val_accuracy: 0.6250\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4904 - accuracy: 0.7321 - val_loss: 0.5506 - val_accuracy: 0.6250\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3696 - accuracy: 0.8750 - val_loss: 0.5321 - val_accuracy: 0.7500\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4060 - accuracy: 0.8393 - val_loss: 0.5161 - val_accuracy: 0.7500\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4224 - accuracy: 0.8214 - val_loss: 0.5111 - val_accuracy: 0.6250\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3807 - accuracy: 0.8571 - val_loss: 0.5061 - val_accuracy: 0.6250\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4017 - accuracy: 0.8214 - val_loss: 0.4905 - val_accuracy: 0.6250\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4457 - accuracy: 0.8036 - val_loss: 0.4834 - val_accuracy: 0.6250\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3568 - accuracy: 0.8393 - val_loss: 0.4988 - val_accuracy: 0.6250\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3767 - accuracy: 0.8750 - val_loss: 0.4930 - val_accuracy: 0.6250\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3861 - accuracy: 0.8571 - val_loss: 0.4807 - val_accuracy: 0.7500\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4164 - accuracy: 0.8214 - val_loss: 0.4793 - val_accuracy: 0.6250\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3670 - accuracy: 0.8393 - val_loss: 0.4688 - val_accuracy: 0.6250\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3990 - accuracy: 0.9107 - val_loss: 0.4763 - val_accuracy: 0.7500\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3404 - accuracy: 0.8929 - val_loss: 0.4743 - val_accuracy: 0.7500\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3557 - accuracy: 0.8750 - val_loss: 0.4418 - val_accuracy: 0.7500\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3594 - accuracy: 0.8571 - val_loss: 0.4668 - val_accuracy: 0.7500\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4251 - accuracy: 0.8393 - val_loss: 0.4451 - val_accuracy: 0.7500\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3554 - accuracy: 0.8571 - val_loss: 0.4346 - val_accuracy: 0.7500\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3547 - accuracy: 0.8929 - val_loss: 0.4469 - val_accuracy: 0.8750\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4208 - accuracy: 0.8214 - val_loss: 0.4317 - val_accuracy: 0.7500\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.2955 - accuracy: 0.8929 - val_loss: 0.4264 - val_accuracy: 0.7500\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3011 - accuracy: 0.9107 - val_loss: 0.4154 - val_accuracy: 0.8750\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3637 - accuracy: 0.8571 - val_loss: 0.4257 - val_accuracy: 0.8750\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3477 - accuracy: 0.9107 - val_loss: 0.4152 - val_accuracy: 0.8750\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3336 - accuracy: 0.8929 - val_loss: 0.4180 - val_accuracy: 0.8750\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3498 - accuracy: 0.8929 - val_loss: 0.4054 - val_accuracy: 0.8750\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.2785 - accuracy: 0.9107 - val_loss: 0.4095 - val_accuracy: 0.7500\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3449 - accuracy: 0.8750 - val_loss: 0.4091 - val_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.2914 - accuracy: 0.9286 - val_loss: 0.3838 - val_accuracy: 0.8750\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3394 - accuracy: 0.9107 - val_loss: 0.3894 - val_accuracy: 0.7500\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.2488 - accuracy: 0.9643 - val_loss: 0.3911 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2837 - accuracy: 0.8929 - val_loss: 0.3722 - val_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.3277 - accuracy: 0.8929 - val_loss: 0.3639 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2794 - accuracy: 0.9107 - val_loss: 0.3679 - val_accuracy: 0.7500\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2513 - accuracy: 0.9643 - val_loss: 0.3818 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2760 - accuracy: 0.9107 - val_loss: 0.3664 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2821 - accuracy: 0.8929 - val_loss: 0.3604 - val_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.3012 - accuracy: 0.9107 - val_loss: 0.3628 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2984 - accuracy: 0.8571 - val_loss: 0.3533 - val_accuracy: 0.7500\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.2466 - accuracy: 0.9464 - val_loss: 0.3546 - val_accuracy: 0.8750\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.3063 - accuracy: 0.9107 - val_loss: 0.3403 - val_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2893 - accuracy: 0.9107 - val_loss: 0.3468 - val_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2557 - accuracy: 0.9107 - val_loss: 0.3368 - val_accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2850 - accuracy: 0.8929 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2414 - accuracy: 0.9286 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2285 - accuracy: 0.9464 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2584 - accuracy: 0.9464 - val_loss: 0.3177 - val_accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2880 - accuracy: 0.8929 - val_loss: 0.3228 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.3078 - accuracy: 0.9107 - val_loss: 0.3076 - val_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2479 - accuracy: 0.9464 - val_loss: 0.3354 - val_accuracy: 0.8750\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2862 - accuracy: 0.8750 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3143 - accuracy: 0.9286 - val_loss: 0.3097 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2641 - accuracy: 0.9107 - val_loss: 0.3282 - val_accuracy: 0.7500\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2387 - accuracy: 0.9464 - val_loss: 0.2950 - val_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2117 - accuracy: 0.9464 - val_loss: 0.3009 - val_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2535 - accuracy: 0.9464 - val_loss: 0.3065 - val_accuracy: 0.8750\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2705 - accuracy: 0.8571 - val_loss: 0.2945 - val_accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2172 - accuracy: 0.9464 - val_loss: 0.2795 - val_accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.2463 - accuracy: 0.9286 - val_loss: 0.2997 - val_accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1819 - accuracy: 0.9464 - val_loss: 0.2988 - val_accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2141 - accuracy: 0.9464 - val_loss: 0.2728 - val_accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2691 - accuracy: 0.9464 - val_loss: 0.2647 - val_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2496 - accuracy: 0.9107 - val_loss: 0.3015 - val_accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2274 - accuracy: 0.9464 - val_loss: 0.2896 - val_accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2261 - accuracy: 0.9464 - val_loss: 0.2643 - val_accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1883 - accuracy: 0.9643 - val_loss: 0.2547 - val_accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2358 - accuracy: 0.9286 - val_loss: 0.2661 - val_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2014 - accuracy: 0.9464 - val_loss: 0.2583 - val_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2233 - accuracy: 0.9464 - val_loss: 0.2522 - val_accuracy: 1.0000\n",
      "Epoch 150/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2197 - accuracy: 0.9643 - val_loss: 0.2466 - val_accuracy: 1.0000\n",
      "Epoch 151/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2007 - accuracy: 0.9643 - val_loss: 0.2322 - val_accuracy: 1.0000\n",
      "Epoch 152/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2436 - accuracy: 0.8750 - val_loss: 0.2432 - val_accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1951 - accuracy: 0.9464 - val_loss: 0.2372 - val_accuracy: 1.0000\n",
      "Epoch 154/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2214 - accuracy: 0.9643 - val_loss: 0.2502 - val_accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2053 - accuracy: 0.9821 - val_loss: 0.2437 - val_accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2376 - accuracy: 0.9464 - val_loss: 0.2318 - val_accuracy: 1.0000\n",
      "Epoch 157/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2250 - accuracy: 0.9643 - val_loss: 0.2212 - val_accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1745 - accuracy: 0.9643 - val_loss: 0.2301 - val_accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1867 - accuracy: 0.9643 - val_loss: 0.2242 - val_accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2002 - accuracy: 0.9286 - val_loss: 0.2288 - val_accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2309 - accuracy: 0.9107 - val_loss: 0.2377 - val_accuracy: 1.0000\n",
      "Epoch 162/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1885 - accuracy: 0.9643 - val_loss: 0.2249 - val_accuracy: 1.0000\n",
      "Epoch 163/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2128 - accuracy: 0.9286 - val_loss: 0.2240 - val_accuracy: 1.0000\n",
      "Epoch 164/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1678 - accuracy: 0.9643 - val_loss: 0.2196 - val_accuracy: 1.0000\n",
      "Epoch 165/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1883 - accuracy: 0.9821 - val_loss: 0.2119 - val_accuracy: 1.0000\n",
      "Epoch 166/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1721 - accuracy: 0.9464 - val_loss: 0.2072 - val_accuracy: 1.0000\n",
      "Epoch 167/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.1799 - accuracy: 0.9643 - val_loss: 0.1951 - val_accuracy: 1.0000\n",
      "Epoch 168/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2014 - accuracy: 0.9643 - val_loss: 0.2118 - val_accuracy: 1.0000\n",
      "Epoch 169/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.1915 - accuracy: 0.9643 - val_loss: 0.2008 - val_accuracy: 1.0000\n",
      "Epoch 170/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1748 - accuracy: 0.9643 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 171/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1560 - accuracy: 0.9821 - val_loss: 0.1922 - val_accuracy: 1.0000\n",
      "Epoch 172/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1776 - accuracy: 0.9643 - val_loss: 0.1897 - val_accuracy: 1.0000\n",
      "Epoch 173/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2080 - accuracy: 0.9464 - val_loss: 0.2054 - val_accuracy: 1.0000\n",
      "Epoch 174/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1770 - accuracy: 0.9821 - val_loss: 0.1917 - val_accuracy: 1.0000\n",
      "Epoch 175/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1200 - accuracy: 1.0000 - val_loss: 0.1867 - val_accuracy: 1.0000\n",
      "Epoch 176/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1630 - accuracy: 0.9464 - val_loss: 0.1758 - val_accuracy: 1.0000\n",
      "Epoch 177/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1616 - accuracy: 1.0000 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
      "Epoch 178/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1532 - accuracy: 0.9821 - val_loss: 0.2029 - val_accuracy: 1.0000\n",
      "Epoch 179/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1847 - accuracy: 0.9286 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 180/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1943 - accuracy: 0.9643 - val_loss: 0.1884 - val_accuracy: 1.0000\n",
      "Epoch 181/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1552 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 1.0000\n",
      "Epoch 182/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1950 - accuracy: 0.9464 - val_loss: 0.1655 - val_accuracy: 1.0000\n",
      "Epoch 183/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2349 - accuracy: 0.9107 - val_loss: 0.1719 - val_accuracy: 1.0000\n",
      "Epoch 184/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1735 - accuracy: 0.9643 - val_loss: 0.1831 - val_accuracy: 1.0000\n",
      "Epoch 185/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1180 - accuracy: 1.0000 - val_loss: 0.1768 - val_accuracy: 1.0000\n",
      "Epoch 186/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.1242 - accuracy: 0.9821 - val_loss: 0.1660 - val_accuracy: 1.0000\n",
      "Epoch 187/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1820 - accuracy: 0.9643 - val_loss: 0.1605 - val_accuracy: 1.0000\n",
      "Epoch 188/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1512 - accuracy: 0.9464 - val_loss: 0.1594 - val_accuracy: 1.0000\n",
      "Epoch 189/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1461 - accuracy: 0.9821 - val_loss: 0.1430 - val_accuracy: 1.0000\n",
      "Epoch 190/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1088 - accuracy: 0.9821 - val_loss: 0.1390 - val_accuracy: 1.0000\n",
      "Epoch 191/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1381 - accuracy: 0.9821 - val_loss: 0.1415 - val_accuracy: 1.0000\n",
      "Epoch 192/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1379 - accuracy: 0.9643 - val_loss: 0.1488 - val_accuracy: 1.0000\n",
      "Epoch 193/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1467 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 1.0000\n",
      "Epoch 194/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1485 - accuracy: 0.9643 - val_loss: 0.1506 - val_accuracy: 1.0000\n",
      "Epoch 195/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.1409 - val_accuracy: 1.0000\n",
      "Epoch 196/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1504 - accuracy: 0.9643 - val_loss: 0.1339 - val_accuracy: 1.0000\n",
      "Epoch 197/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1006 - accuracy: 1.0000 - val_loss: 0.1390 - val_accuracy: 1.0000\n",
      "Epoch 198/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.1463 - val_accuracy: 1.0000\n",
      "Epoch 199/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1112 - accuracy: 1.0000 - val_loss: 0.1389 - val_accuracy: 1.0000\n",
      "Epoch 200/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1171 - accuracy: 0.9821 - val_loss: 0.1317 - val_accuracy: 1.0000\n",
      "Epoch 201/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1248 - accuracy: 1.0000 - val_loss: 0.1225 - val_accuracy: 1.0000\n",
      "Epoch 202/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1505 - accuracy: 0.9821 - val_loss: 0.1202 - val_accuracy: 1.0000\n",
      "Epoch 203/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1173 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 1.0000\n",
      "Epoch 204/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1064 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 1.0000\n",
      "Epoch 205/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1741 - accuracy: 0.9464 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
      "Epoch 206/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1221 - accuracy: 0.9643 - val_loss: 0.1214 - val_accuracy: 1.0000\n",
      "Epoch 207/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1315 - accuracy: 0.9821 - val_loss: 0.1058 - val_accuracy: 1.0000\n",
      "Epoch 208/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 1.0000\n",
      "Epoch 209/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1438 - accuracy: 0.9643 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
      "Epoch 210/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1015 - accuracy: 0.9821 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
      "Epoch 211/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 0.1226 - val_accuracy: 1.0000\n",
      "Epoch 212/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1255 - accuracy: 0.9643 - val_loss: 0.1236 - val_accuracy: 1.0000\n",
      "Epoch 213/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0999 - accuracy: 0.9821 - val_loss: 0.1159 - val_accuracy: 1.0000\n",
      "Epoch 214/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1302 - accuracy: 0.9643 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
      "Epoch 215/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.1227 - accuracy: 0.9821 - val_loss: 0.1119 - val_accuracy: 1.0000\n",
      "Epoch 216/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1162 - accuracy: 0.9821 - val_loss: 0.1388 - val_accuracy: 1.0000\n",
      "Epoch 217/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 207.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 1.0000\n",
      "Epoch 217: early stopping\n",
      "Epoch 1/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.6504 - accuracy: 0.4107 - val_loss: 1.9278 - val_accuracy: 0.2500\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.3944 - accuracy: 0.5179 - val_loss: 1.5446 - val_accuracy: 0.3750\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.1589 - accuracy: 0.6607 - val_loss: 1.4253 - val_accuracy: 0.3750\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.1630 - accuracy: 0.5893 - val_loss: 1.3190 - val_accuracy: 0.3750\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.0666 - accuracy: 0.5536 - val_loss: 1.2278 - val_accuracy: 0.3750\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9537 - accuracy: 0.5357 - val_loss: 1.1404 - val_accuracy: 0.3750\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9477 - accuracy: 0.5179 - val_loss: 1.0795 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8515 - accuracy: 0.6250 - val_loss: 1.0161 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7874 - accuracy: 0.5893 - val_loss: 0.9749 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8777 - accuracy: 0.5714 - val_loss: 0.9568 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7975 - accuracy: 0.6071 - val_loss: 0.9261 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6780 - accuracy: 0.8036 - val_loss: 0.9319 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6603 - accuracy: 0.6964 - val_loss: 0.9000 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.6691 - accuracy: 0.6964 - val_loss: 0.8862 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.6393 - accuracy: 0.7500 - val_loss: 0.9308 - val_accuracy: 0.6250\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6042 - accuracy: 0.7143 - val_loss: 0.8514 - val_accuracy: 0.6250\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6129 - accuracy: 0.7321 - val_loss: 0.8814 - val_accuracy: 0.6250\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.5857 - accuracy: 0.7679 - val_loss: 0.8561 - val_accuracy: 0.6250\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.5844 - accuracy: 0.7143 - val_loss: 0.8094 - val_accuracy: 0.6250\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5745 - accuracy: 0.7857 - val_loss: 0.8302 - val_accuracy: 0.6250\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5470 - accuracy: 0.7857 - val_loss: 0.8314 - val_accuracy: 0.6250\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5166 - accuracy: 0.8036 - val_loss: 0.7870 - val_accuracy: 0.6250\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5569 - accuracy: 0.7679 - val_loss: 0.8038 - val_accuracy: 0.6250\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5319 - accuracy: 0.7500 - val_loss: 0.7379 - val_accuracy: 0.6250\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5579 - accuracy: 0.7143 - val_loss: 0.8005 - val_accuracy: 0.6250\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4736 - accuracy: 0.7857 - val_loss: 0.7235 - val_accuracy: 0.6250\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4692 - accuracy: 0.6964 - val_loss: 0.7380 - val_accuracy: 0.6250\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.4901 - accuracy: 0.8036 - val_loss: 0.7156 - val_accuracy: 0.6250\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4666 - accuracy: 0.8036 - val_loss: 0.6896 - val_accuracy: 0.6250\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4302 - accuracy: 0.8571 - val_loss: 0.7098 - val_accuracy: 0.6250\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4482 - accuracy: 0.7857 - val_loss: 0.6764 - val_accuracy: 0.6250\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4410 - accuracy: 0.8393 - val_loss: 0.7119 - val_accuracy: 0.6250\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4196 - accuracy: 0.8214 - val_loss: 0.6145 - val_accuracy: 0.7500\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3882 - accuracy: 0.9107 - val_loss: 0.7219 - val_accuracy: 0.6250\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3609 - accuracy: 0.8929 - val_loss: 0.6392 - val_accuracy: 0.6250\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3807 - accuracy: 0.8393 - val_loss: 0.6337 - val_accuracy: 0.6250\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3725 - accuracy: 0.8571 - val_loss: 0.6275 - val_accuracy: 0.6250\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3608 - accuracy: 0.8929 - val_loss: 0.6192 - val_accuracy: 0.6250\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3607 - accuracy: 0.8571 - val_loss: 0.5726 - val_accuracy: 0.6250\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3281 - accuracy: 0.9107 - val_loss: 0.6055 - val_accuracy: 0.6250\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.2836 - accuracy: 0.9643 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.3017 - accuracy: 0.9821 - val_loss: 0.5659 - val_accuracy: 0.6250\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.3009 - accuracy: 0.9643 - val_loss: 0.4775 - val_accuracy: 0.7500\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.3161 - accuracy: 0.8750 - val_loss: 0.5465 - val_accuracy: 0.6250\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.2791 - accuracy: 0.9107 - val_loss: 0.4662 - val_accuracy: 0.8750\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.3123 - accuracy: 0.8929 - val_loss: 0.4823 - val_accuracy: 0.7500\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2559 - accuracy: 0.9286 - val_loss: 0.4331 - val_accuracy: 0.8750\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2560 - accuracy: 0.9286 - val_loss: 0.3957 - val_accuracy: 0.8750\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2535 - accuracy: 0.9464 - val_loss: 0.4510 - val_accuracy: 0.7500\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2128 - accuracy: 0.9821 - val_loss: 0.4381 - val_accuracy: 0.7500\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2306 - accuracy: 0.9464 - val_loss: 0.3954 - val_accuracy: 0.8750\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2351 - accuracy: 0.9643 - val_loss: 0.4359 - val_accuracy: 0.7500\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.2016 - accuracy: 0.9643 - val_loss: 0.3517 - val_accuracy: 0.8750\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.1895 - accuracy: 0.9821 - val_loss: 0.3834 - val_accuracy: 0.8750\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.1767 - accuracy: 0.9821 - val_loss: 0.3691 - val_accuracy: 0.8750\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.2396 - accuracy: 0.9464 - val_loss: 0.3371 - val_accuracy: 0.8750\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1991 - accuracy: 0.9643 - val_loss: 0.3816 - val_accuracy: 0.7500\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.2101 - accuracy: 0.9464 - val_loss: 0.2505 - val_accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.2163 - accuracy: 0.9643 - val_loss: 0.3177 - val_accuracy: 0.8750\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1893 - accuracy: 0.9643 - val_loss: 0.3212 - val_accuracy: 0.8750\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1774 - accuracy: 0.9643 - val_loss: 0.3281 - val_accuracy: 0.7500\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1388 - accuracy: 0.9821 - val_loss: 0.2371 - val_accuracy: 1.0000\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1866 - accuracy: 0.9464 - val_loss: 0.2520 - val_accuracy: 1.0000\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1556 - accuracy: 1.0000 - val_loss: 0.2480 - val_accuracy: 0.8750\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1381 - accuracy: 1.0000 - val_loss: 0.2809 - val_accuracy: 0.8750\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1492 - accuracy: 0.9821 - val_loss: 0.2316 - val_accuracy: 0.8750\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1804 - accuracy: 0.9464 - val_loss: 0.2196 - val_accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1411 - accuracy: 0.9821 - val_loss: 0.2442 - val_accuracy: 0.8750\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1295 - accuracy: 1.0000 - val_loss: 0.2343 - val_accuracy: 0.8750\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1515 - accuracy: 0.9821 - val_loss: 0.2298 - val_accuracy: 0.8750\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1572 - accuracy: 0.9821 - val_loss: 0.2071 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1440 - accuracy: 0.9821 - val_loss: 0.1693 - val_accuracy: 1.0000\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.8750\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1189 - accuracy: 0.9821 - val_loss: 0.2105 - val_accuracy: 0.8750\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 1.0000\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0958 - accuracy: 1.0000 - val_loss: 0.1823 - val_accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.1779 - val_accuracy: 1.0000\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0895 - accuracy: 1.0000 - val_loss: 0.1812 - val_accuracy: 1.0000\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0921 - accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1054 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 1.0000\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.1241 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0868 - accuracy: 1.0000 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0927 - accuracy: 0.9821 - val_loss: 0.1174 - val_accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0761 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0970 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 1.0000\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0894 - accuracy: 0.9821 - val_loss: 0.0836 - val_accuracy: 1.0000\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0801 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0587 - accuracy: 0.9821 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.0529 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.0653 - val_accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0508 - accuracy: 0.9821 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 137/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 138/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 139/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 140/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
      "Epoch 141/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 142/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 143/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 144/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Epoch 145/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 146/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 147/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 148/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 149/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0370 - accuracy: 0.9821 - val_loss: 0.1372 - val_accuracy: 0.8750\n",
      "Epoch 150/1000\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 151/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 152/1000\n",
      "8/8 [==============================] - 23s 3s/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 153/1000\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
      "Epoch 154/1000\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 155/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 156/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 157/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 158/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 159/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 160/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 161/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 151.\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 161: early stopping\n",
      "Epoch 1/1000\n",
      "8/8 [==============================] - 24s 3s/step - loss: 2.0988 - accuracy: 0.4643 - val_loss: 1.5244 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.9043 - accuracy: 0.6964 - val_loss: 1.5388 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.9027 - accuracy: 0.6071 - val_loss: 1.3791 - val_accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.9686 - accuracy: 0.6071 - val_loss: 1.1560 - val_accuracy: 0.6250\n",
      "Epoch 5/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.8039 - accuracy: 0.7321 - val_loss: 1.1127 - val_accuracy: 0.5000\n",
      "Epoch 6/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.7320 - accuracy: 0.7321 - val_loss: 1.0559 - val_accuracy: 0.5000\n",
      "Epoch 7/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.7637 - accuracy: 0.6964 - val_loss: 1.0405 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.6548 - accuracy: 0.7679 - val_loss: 1.0123 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.6177 - accuracy: 0.7143 - val_loss: 0.9885 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.6541 - accuracy: 0.6786 - val_loss: 0.9533 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.6761 - accuracy: 0.7143 - val_loss: 0.8961 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.6062 - accuracy: 0.8036 - val_loss: 0.8714 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.5996 - accuracy: 0.7857 - val_loss: 0.9204 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.5657 - accuracy: 0.7500 - val_loss: 0.8604 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.5741 - accuracy: 0.7321 - val_loss: 0.8550 - val_accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.5691 - accuracy: 0.7321 - val_loss: 0.8328 - val_accuracy: 0.5000\n",
      "Epoch 17/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5622 - accuracy: 0.6786 - val_loss: 0.8118 - val_accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5433 - accuracy: 0.7500 - val_loss: 0.7578 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5601 - accuracy: 0.7679 - val_loss: 0.7482 - val_accuracy: 0.5000\n",
      "Epoch 20/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4764 - accuracy: 0.7857 - val_loss: 0.7458 - val_accuracy: 0.5000\n",
      "Epoch 21/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5426 - accuracy: 0.7143 - val_loss: 0.7893 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4816 - accuracy: 0.8214 - val_loss: 0.7362 - val_accuracy: 0.7500\n",
      "Epoch 23/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4795 - accuracy: 0.8036 - val_loss: 0.6839 - val_accuracy: 0.5000\n",
      "Epoch 24/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4450 - accuracy: 0.7857 - val_loss: 0.7451 - val_accuracy: 0.5000\n",
      "Epoch 25/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4696 - accuracy: 0.8393 - val_loss: 0.7232 - val_accuracy: 0.5000\n",
      "Epoch 26/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3780 - accuracy: 0.8571 - val_loss: 0.6746 - val_accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4573 - accuracy: 0.8036 - val_loss: 0.6560 - val_accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4215 - accuracy: 0.7500 - val_loss: 0.6738 - val_accuracy: 0.5000\n",
      "Epoch 29/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4041 - accuracy: 0.7857 - val_loss: 0.6401 - val_accuracy: 0.6250\n",
      "Epoch 30/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4466 - accuracy: 0.7679 - val_loss: 0.5723 - val_accuracy: 0.6250\n",
      "Epoch 31/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3819 - accuracy: 0.8214 - val_loss: 0.6144 - val_accuracy: 0.7500\n",
      "Epoch 32/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4101 - accuracy: 0.8214 - val_loss: 0.5765 - val_accuracy: 0.5000\n",
      "Epoch 33/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3870 - accuracy: 0.8750 - val_loss: 0.6303 - val_accuracy: 0.5000\n",
      "Epoch 34/1000\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3839 - accuracy: 0.8036 - val_loss: 0.5862 - val_accuracy: 0.6250\n",
      "Epoch 35/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3610 - accuracy: 0.8571 - val_loss: 0.5366 - val_accuracy: 0.7500\n",
      "Epoch 36/1000\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3980 - accuracy: 0.8036 - val_loss: 0.5001 - val_accuracy: 0.7500\n",
      "Epoch 37/1000\n",
      "8/8 [==============================] - 17s 2s/step - loss: 0.3985 - accuracy: 0.8393 - val_loss: 0.5813 - val_accuracy: 0.5000\n",
      "Epoch 38/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3625 - accuracy: 0.8571 - val_loss: 0.5279 - val_accuracy: 0.6250\n",
      "Epoch 39/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3505 - accuracy: 0.8393 - val_loss: 0.5249 - val_accuracy: 0.6250\n",
      "Epoch 40/1000\n",
      "8/8 [==============================] - 18s 3s/step - loss: 0.4126 - accuracy: 0.8571 - val_loss: 0.4844 - val_accuracy: 0.7500\n",
      "Epoch 41/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.4343 - accuracy: 0.7857 - val_loss: 0.4644 - val_accuracy: 0.6250\n",
      "Epoch 42/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3134 - accuracy: 0.8750 - val_loss: 0.5567 - val_accuracy: 0.6250\n",
      "Epoch 43/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3541 - accuracy: 0.8750 - val_loss: 0.4913 - val_accuracy: 0.6250\n",
      "Epoch 44/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2729 - accuracy: 0.9107 - val_loss: 0.4719 - val_accuracy: 0.7500\n",
      "Epoch 45/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2943 - accuracy: 0.8571 - val_loss: 0.4393 - val_accuracy: 0.6250\n",
      "Epoch 46/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3396 - accuracy: 0.8393 - val_loss: 0.4484 - val_accuracy: 0.8750\n",
      "Epoch 47/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2344 - accuracy: 0.9464 - val_loss: 0.4254 - val_accuracy: 0.7500\n",
      "Epoch 48/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.3073 - accuracy: 0.9107 - val_loss: 0.3876 - val_accuracy: 0.8750\n",
      "Epoch 49/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2476 - accuracy: 0.9286 - val_loss: 0.4116 - val_accuracy: 0.7500\n",
      "Epoch 50/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2288 - accuracy: 0.9286 - val_loss: 0.4367 - val_accuracy: 0.8750\n",
      "Epoch 51/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2397 - accuracy: 0.9464 - val_loss: 0.4036 - val_accuracy: 0.8750\n",
      "Epoch 52/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2553 - accuracy: 0.9464 - val_loss: 0.3747 - val_accuracy: 0.8750\n",
      "Epoch 53/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.2392 - accuracy: 0.9286 - val_loss: 0.3567 - val_accuracy: 0.8750\n",
      "Epoch 54/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2115 - accuracy: 0.9821 - val_loss: 0.3577 - val_accuracy: 0.8750\n",
      "Epoch 55/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2711 - accuracy: 0.9464 - val_loss: 0.3594 - val_accuracy: 0.8750\n",
      "Epoch 56/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2002 - accuracy: 0.9464 - val_loss: 0.2986 - val_accuracy: 1.0000\n",
      "Epoch 57/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2101 - accuracy: 0.9464 - val_loss: 0.4339 - val_accuracy: 0.8750\n",
      "Epoch 58/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2090 - accuracy: 0.9286 - val_loss: 0.2988 - val_accuracy: 1.0000\n",
      "Epoch 59/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1805 - accuracy: 0.9464 - val_loss: 0.3181 - val_accuracy: 1.0000\n",
      "Epoch 60/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1778 - accuracy: 0.9643 - val_loss: 0.2944 - val_accuracy: 0.8750\n",
      "Epoch 61/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1994 - accuracy: 0.9643 - val_loss: 0.2584 - val_accuracy: 1.0000\n",
      "Epoch 62/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2313 - accuracy: 0.9107 - val_loss: 0.2595 - val_accuracy: 0.8750\n",
      "Epoch 63/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1841 - accuracy: 0.9464 - val_loss: 0.2589 - val_accuracy: 0.8750\n",
      "Epoch 64/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1518 - accuracy: 0.9643 - val_loss: 0.2393 - val_accuracy: 0.8750\n",
      "Epoch 65/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1562 - accuracy: 0.9643 - val_loss: 0.2774 - val_accuracy: 0.8750\n",
      "Epoch 66/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1341 - accuracy: 0.9821 - val_loss: 0.2659 - val_accuracy: 0.8750\n",
      "Epoch 67/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.2055 - accuracy: 0.9464 - val_loss: 0.1822 - val_accuracy: 1.0000\n",
      "Epoch 68/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1773 - accuracy: 0.9464 - val_loss: 0.2549 - val_accuracy: 1.0000\n",
      "Epoch 69/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1567 - accuracy: 0.9821 - val_loss: 0.3995 - val_accuracy: 0.8750\n",
      "Epoch 70/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1393 - accuracy: 0.9464 - val_loss: 0.1711 - val_accuracy: 1.0000\n",
      "Epoch 71/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1476 - accuracy: 0.9464 - val_loss: 0.1739 - val_accuracy: 1.0000\n",
      "Epoch 72/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.2494 - val_accuracy: 0.8750\n",
      "Epoch 73/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1034 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.8750\n",
      "Epoch 74/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1475 - accuracy: 0.9643 - val_loss: 0.1753 - val_accuracy: 1.0000\n",
      "Epoch 75/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 0.2070 - val_accuracy: 0.8750\n",
      "Epoch 76/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1159 - accuracy: 0.9821 - val_loss: 0.1637 - val_accuracy: 1.0000\n",
      "Epoch 77/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1151 - accuracy: 0.9821 - val_loss: 0.1871 - val_accuracy: 0.8750\n",
      "Epoch 78/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.1150 - accuracy: 0.9464 - val_loss: 0.1677 - val_accuracy: 0.8750\n",
      "Epoch 79/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1078 - accuracy: 0.9821 - val_loss: 0.1134 - val_accuracy: 1.0000\n",
      "Epoch 80/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0908 - accuracy: 0.9821 - val_loss: 0.1830 - val_accuracy: 0.8750\n",
      "Epoch 81/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 1.0000\n",
      "Epoch 82/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0877 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
      "Epoch 83/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 1.0000\n",
      "Epoch 84/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0581 - accuracy: 0.9821 - val_loss: 0.1428 - val_accuracy: 1.0000\n",
      "Epoch 85/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 1.0000\n",
      "Epoch 86/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.0966 - val_accuracy: 1.0000\n",
      "Epoch 87/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0529 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
      "Epoch 88/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0803 - accuracy: 1.0000 - val_loss: 0.1020 - val_accuracy: 1.0000\n",
      "Epoch 89/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.1005 - accuracy: 0.9821 - val_loss: 0.0999 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0767 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 1.0000\n",
      "Epoch 91/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0676 - accuracy: 0.9821 - val_loss: 0.1111 - val_accuracy: 1.0000\n",
      "Epoch 92/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.2101 - val_accuracy: 0.8750\n",
      "Epoch 93/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 0.8750\n",
      "Epoch 96/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
      "Epoch 97/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
      "Epoch 98/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
      "Epoch 99/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
      "Epoch 100/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 101/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000\n",
      "Epoch 102/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
      "Epoch 103/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 104/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 105/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
      "Epoch 106/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 1.0000\n",
      "Epoch 107/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "Epoch 108/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
      "Epoch 109/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 110/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 111/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
      "Epoch 112/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 113/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 114/1000\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
      "Epoch 115/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 120/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 121/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 122/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 123/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
      "Epoch 124/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
      "Epoch 125/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 126/1000\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 127/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 128/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 130/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 131/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 132/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 135/1000\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 136/1000\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 126.\n",
      "8/8 [==============================] - 19s 3s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 136: early stopping\n"
     ]
    }
   ],
   "source": [
    "cross_training_progress = dict()\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(learning_rate=0.00005),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    cross_training_progress[name] = model.fit(\n",
    "        keras_data_generator(CROSS_TRAIN_FOLDER, 7, cross_preprocessing_pipeline),\n",
    "        steps_per_epoch=8,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=keras_data_generator(CROSS_VAL_FOLDER, 8, cross_preprocessing_pipeline),\n",
    "        validation_steps=1,\n",
    "        callbacks=[early_stopping],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14779d19",
   "metadata": {},
   "source": [
    "## Cross - Convergence plots\n",
    "\n",
    "Let us plot the convergence of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5410415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "LSTM Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          3.66044020652771,
          3.1581342220306396,
          2.785855531692505,
          2.3740713596343994,
          2.2309346199035645,
          2.1307098865509033,
          2.099604606628418,
          1.8432284593582153,
          1.584072470664978,
          1.3661143779754639,
          1.140971064567566,
          1.0479977130889893,
          0.9405378699302673,
          0.8461459279060364,
          0.846253514289856,
          0.9174076914787292,
          0.8620457053184509,
          0.8428694009780884,
          0.8768672943115234,
          0.7617413401603699,
          0.7349971532821655,
          0.7179532647132874,
          0.6973021626472473,
          0.7253689169883728,
          0.7199420928955078,
          0.6128334403038025,
          0.6406343579292297,
          0.5577991604804993,
          0.5265064835548401,
          0.6091033220291138,
          0.7252489328384399,
          0.794827401638031,
          0.7556443810462952,
          0.7549710273742676,
          0.6540163159370422,
          0.6387344002723694,
          0.538509726524353,
          0.6063122153282166,
          0.5116210579872131
         ]
        },
        {
         "mode": "lines",
         "name": "Bidirectional LSTM Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          3.5462124347686768,
          2.7764270305633545,
          2.1917474269866943,
          1.871085524559021,
          1.5502842664718628,
          1.3285655975341797,
          1.2103710174560547,
          1.0514005422592163,
          0.9129377603530884,
          0.7574242353439331,
          0.768376886844635,
          0.7234238386154175,
          0.8222414255142212,
          0.9031262993812561,
          0.7048856616020203,
          0.8992258310317993,
          0.6955481767654419,
          0.5084990859031677,
          0.42160764336586,
          0.3682270348072052,
          0.2894895076751709,
          0.2700505554676056,
          0.24835047125816345,
          0.28637412190437317,
          0.37663987278938293,
          0.29143667221069336,
          0.3987087309360504,
          0.3048497140407562,
          0.3995003402233124,
          0.2611136734485626,
          0.217768594622612,
          0.2005891650915146,
          0.1779193878173828,
          0.17545412480831146,
          0.13531021773815155,
          0.09031904488801956,
          0.1213873028755188,
          0.1561729460954666,
          0.16945160925388336,
          0.12141157686710358,
          0.10640250891447067,
          0.20652881264686584,
          0.43198469281196594,
          0.9586678743362427,
          0.7783269882202148,
          0.5727906227111816,
          0.5134495496749878,
          0.4375961720943451,
          0.4017324447631836,
          0.4870428442955017,
          0.4156149923801422,
          0.44091910123825073,
          0.5012446045875549,
          0.46016231179237366,
          0.2905293107032776,
          0.3304874002933502,
          0.2495831549167633,
          0.28972020745277405,
          0.20197327435016632,
          0.3621354103088379,
          0.5532920956611633,
          0.4962257742881775,
          0.27654126286506653,
          0.36260488629341125,
          0.21091651916503906,
          0.25718042254447937,
          0.2233715057373047,
          0.16277864575386047,
          0.1749439239501953,
          0.23270542919635773,
          0.17484460771083832,
          0.2116740643978119,
          0.18256260454654694,
          0.15695109963417053,
          0.15543651580810547,
          0.15922297537326813,
          0.1285848319530487,
          0.12696459889411926,
          0.13242264091968536,
          0.1560286432504654,
          0.29927700757980347,
          0.4420054852962494,
          0.17847704887390137,
          0.12993381917476654,
          0.11119816452264786,
          0.09440167993307114,
          0.10893022269010544,
          0.11923784762620926,
          0.17360028624534607,
          0.2663862109184265,
          0.16819289326667786,
          0.0935007855296135,
          0.09163982421159744,
          0.09609448164701462,
          0.06520050019025803,
          0.07407285273075104,
          0.10156332701444626,
          0.08237557858228683,
          0.07642694562673569,
          0.08638367801904678,
          0.0852804109454155,
          0.11088147014379501,
          0.0433460958302021,
          0.06441353261470795,
          0.08390089124441147,
          0.1037706732749939,
          0.2756601870059967,
          0.3237646520137787,
          0.5083740949630737,
          0.5867705345153809,
          0.48849377036094666,
          0.3680821359157562,
          0.284147173166275,
          0.3195362687110901,
          0.4297787845134735
         ]
        },
        {
         "mode": "lines",
         "name": "CNN Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          6.100190162658691,
          4.8798322677612305,
          3.8673527240753174,
          2.5278546810150146,
          2.0136301517486572,
          1.6430094242095947,
          1.4403066635131836,
          1.2574059963226318,
          0.9551526308059692,
          0.8874989151954651,
          0.9791750907897949,
          0.9565234780311584,
          0.8323133587837219,
          0.9350829720497131,
          0.9205368161201477,
          0.9005153775215149,
          0.7304716110229492,
          0.9377965927124023,
          0.8026085495948792,
          0.7886837124824524,
          0.8068762421607971,
          0.7531268000602722,
          0.8319383859634399,
          0.7480513453483582,
          0.7648465037345886,
          0.8482790589332581,
          0.7058795094490051,
          0.7721413373947144,
          0.7244774103164673,
          0.6608262062072754,
          0.7548128962516785,
          0.6708799600601196,
          0.6011044383049011,
          0.6845197677612305,
          0.6316894888877869,
          0.7364264130592346,
          0.6025527119636536,
          0.6320568323135376,
          0.6009930372238159,
          0.573013961315155,
          0.644144594669342,
          0.6328698992729187,
          0.5320888757705688,
          0.6068587899208069,
          0.5424674153327942,
          0.5191006064414978,
          0.6157978773117065,
          0.5933459401130676,
          0.5787929892539978,
          0.5734183192253113,
          0.6097065210342407,
          0.5768672823905945,
          0.5276640057563782,
          0.5344967842102051,
          0.5720381140708923,
          0.44568246603012085,
          0.5423569083213806,
          0.5190855264663696,
          0.5373573899269104,
          0.5421451926231384,
          0.5436757206916809,
          0.5865157246589661,
          0.466193825006485,
          0.5526813268661499,
          0.4333931505680084,
          0.4893634617328644,
          0.4614371359348297,
          0.4258551001548767,
          0.38898441195487976,
          0.48699966073036194,
          0.5084001421928406,
          0.41053295135498047,
          0.45026013255119324,
          0.43386727571487427,
          0.45802804827690125,
          0.45916029810905457,
          0.44841665029525757,
          0.42551591992378235,
          0.4354000985622406,
          0.4079417288303375,
          0.4904199540615082,
          0.3695501387119293,
          0.4060265123844147,
          0.4224061667919159,
          0.3807380497455597,
          0.4016849100589752,
          0.44565850496292114,
          0.35675281286239624,
          0.3767155706882477,
          0.38605648279190063,
          0.4164377748966217,
          0.36695823073387146,
          0.39896461367607117,
          0.3404252827167511,
          0.35566073656082153,
          0.35940784215927124,
          0.42510849237442017,
          0.3553745746612549,
          0.35466355085372925,
          0.4207746386528015,
          0.29545140266418457,
          0.3011470437049866,
          0.3637327253818512,
          0.34769806265830994,
          0.33359307050704956,
          0.3498022258281708,
          0.2785436809062958,
          0.3448806405067444,
          0.29140764474868774,
          0.33941593766212463,
          0.24882185459136963,
          0.28374627232551575,
          0.3277324140071869,
          0.27943357825279236,
          0.25130459666252136,
          0.27601829171180725,
          0.28207823634147644,
          0.3011602461338043,
          0.2983569800853729,
          0.24659200012683868,
          0.3062862753868103,
          0.28929010033607483,
          0.2556709051132202,
          0.2849884331226349,
          0.2414097934961319,
          0.22851796448230743,
          0.2584258019924164,
          0.2879641354084015,
          0.3078254163265228,
          0.24789337813854218,
          0.2861924171447754,
          0.31425046920776367,
          0.26409292221069336,
          0.2386992871761322,
          0.21170969307422638,
          0.2534744143486023,
          0.27047938108444214,
          0.21718694269657135,
          0.24630704522132874,
          0.18191932141780853,
          0.21408537030220032,
          0.2690935730934143,
          0.24963952600955963,
          0.22743462026119232,
          0.2260691225528717,
          0.18831346929073334,
          0.23577727377414703,
          0.20142889022827148,
          0.22329528629779816,
          0.21968623995780945,
          0.20074544847011566,
          0.2435745894908905,
          0.19512708485126495,
          0.22141991555690765,
          0.20531654357910156,
          0.23759429156780243,
          0.22501640021800995,
          0.17447400093078613,
          0.18671925365924835,
          0.20019663870334625,
          0.23087814450263977,
          0.18846699595451355,
          0.2127951830625534,
          0.1677878201007843,
          0.18827803432941437,
          0.17205378413200378,
          0.179927259683609,
          0.20135970413684845,
          0.19146431982517242,
          0.17480091750621796,
          0.15601904690265656,
          0.17757318913936615,
          0.20797204971313477,
          0.17699658870697021,
          0.11997097730636597,
          0.16303583979606628,
          0.16156312823295593,
          0.1531989723443985,
          0.18470795452594757,
          0.1942993402481079,
          0.1552288979291916,
          0.19499342143535614,
          0.23486867547035217,
          0.17347367107868195,
          0.11799512803554535,
          0.12416242808103561,
          0.1819612979888916,
          0.15116575360298157,
          0.14608649909496307,
          0.10881196707487106,
          0.13811138272285461,
          0.13787131011486053,
          0.14668656885623932,
          0.1484839767217636,
          0.1120910495519638,
          0.15043096244335175,
          0.10055110603570938,
          0.15892080962657928,
          0.11116824299097061,
          0.11708076298236847,
          0.12475444376468658,
          0.15047906339168549,
          0.11732473224401474,
          0.10640730708837509,
          0.17409732937812805,
          0.12209576368331909,
          0.1314636617898941,
          0.10102494060993195,
          0.143798828125,
          0.10147052258253098,
          0.10092996060848236,
          0.12552276253700256,
          0.09989478439092636,
          0.1301950067281723,
          0.12268287688493729,
          0.1162024736404419,
          0.10254384577274323
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + self attention Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          1.6503912210464478,
          1.3944414854049683,
          1.1589326858520508,
          1.1629616022109985,
          1.066555380821228,
          0.9537433981895447,
          0.9476746916770935,
          0.8515467047691345,
          0.787365734577179,
          0.8776896595954895,
          0.7975484132766724,
          0.6780217885971069,
          0.6602575182914734,
          0.6691311597824097,
          0.639321506023407,
          0.6041661500930786,
          0.6128664016723633,
          0.585730254650116,
          0.5843766331672668,
          0.5744682550430298,
          0.546972393989563,
          0.5166387557983398,
          0.5569411516189575,
          0.531917154788971,
          0.5579416155815125,
          0.47359445691108704,
          0.46919605135917664,
          0.4901406168937683,
          0.4665800929069519,
          0.4301685392856598,
          0.44819942116737366,
          0.4410129487514496,
          0.4196252226829529,
          0.3882122039794922,
          0.36087021231651306,
          0.38070225715637207,
          0.3725033700466156,
          0.3608131408691406,
          0.360704630613327,
          0.32813963294029236,
          0.2836205065250397,
          0.3017466068267822,
          0.30089643597602844,
          0.3160701394081116,
          0.2791043519973755,
          0.31226107478141785,
          0.25586462020874023,
          0.25604715943336487,
          0.2534819543361664,
          0.21283374726772308,
          0.23063881695270538,
          0.23512135446071625,
          0.2015983760356903,
          0.1894690841436386,
          0.17674119770526886,
          0.23964865505695343,
          0.19910643994808197,
          0.2101157158613205,
          0.21625475585460663,
          0.18931280076503754,
          0.1773786097764969,
          0.13882586359977722,
          0.18658867478370667,
          0.15559080243110657,
          0.13814212381839752,
          0.14922451972961426,
          0.18036405742168427,
          0.14111198484897614,
          0.12953640520572662,
          0.1514875888824463,
          0.15716345608234406,
          0.14397621154785156,
          0.10023637115955353,
          0.11886183172464371,
          0.10126055032014847,
          0.09575909376144409,
          0.09883809834718704,
          0.08947981894016266,
          0.09207393229007721,
          0.10536092519760132,
          0.12408946454524994,
          0.09444569796323776,
          0.09299930185079575,
          0.08143609017133713,
          0.08679710328578949,
          0.09273580461740494,
          0.07608027756214142,
          0.07566196471452713,
          0.0969948023557663,
          0.07425403594970703,
          0.08314483612775803,
          0.06129169091582298,
          0.0520765595138073,
          0.063393734395504,
          0.08936657011508942,
          0.08005279302597046,
          0.08143556118011475,
          0.0759691372513771,
          0.06247914955019951,
          0.04949865862727165,
          0.05816469341516495,
          0.04833192750811577,
          0.058721452951431274,
          0.04616592451930046,
          0.04325224086642265,
          0.05401155725121498,
          0.04503945633769035,
          0.05289003252983093,
          0.0581146739423275,
          0.04458560422062874,
          0.03550634905695915,
          0.03291650488972664,
          0.034223172813653946,
          0.03968309611082077,
          0.03152479603886604,
          0.026511114090681076,
          0.04529305547475815,
          0.024502860382199287,
          0.02823551371693611,
          0.03231600672006607,
          0.026997337117791176,
          0.033396050333976746,
          0.020044269040226936,
          0.027807610109448433,
          0.03095976449549198,
          0.032020408660173416,
          0.050758104771375656,
          0.03220529481768608,
          0.02200104109942913,
          0.027029920369386673,
          0.021380707621574402,
          0.02595009095966816,
          0.02286512590944767,
          0.016039064154028893,
          0.023539578542113304,
          0.021156180649995804,
          0.01680985651910305,
          0.01714189350605011,
          0.01572372019290924,
          0.015753809362649918,
          0.014686750248074532,
          0.011545014567673206,
          0.013782539404928684,
          0.013178219087421894,
          0.012620260939002037,
          0.019864294677972794,
          0.011580320075154305,
          0.019504744559526443,
          0.03697710484266281,
          0.03320982679724693,
          0.01683955267071724,
          0.02020386978983879,
          0.010471549816429615,
          0.0117641044780612,
          0.013373324647545815,
          0.009435760788619518,
          0.008768604137003422,
          0.012663126923143864,
          0.007235501892864704,
          0.01226468849927187,
          0.008065025322139263
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + multihead attention Training Loss",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          2.0987679958343506,
          0.9042749404907227,
          0.9026806950569153,
          0.9685654044151306,
          0.803925633430481,
          0.731977641582489,
          0.7637249827384949,
          0.6548420190811157,
          0.6176873445510864,
          0.6541236042976379,
          0.6760878562927246,
          0.6061542630195618,
          0.5996099710464478,
          0.5656527280807495,
          0.5740994215011597,
          0.5691254734992981,
          0.562175452709198,
          0.5432926416397095,
          0.5600985288619995,
          0.47638243436813354,
          0.5425776243209839,
          0.48156461119651794,
          0.47946545481681824,
          0.44495758414268494,
          0.4696093499660492,
          0.378042995929718,
          0.457313597202301,
          0.4214596748352051,
          0.4040716290473938,
          0.44657519459724426,
          0.38190555572509766,
          0.4100817143917084,
          0.3869723379611969,
          0.38389042019844055,
          0.3609587550163269,
          0.39801469445228577,
          0.3984890878200531,
          0.36250782012939453,
          0.3504956364631653,
          0.41260385513305664,
          0.4343169331550598,
          0.31344741582870483,
          0.354125440120697,
          0.2728763818740845,
          0.29432398080825806,
          0.3395521938800812,
          0.23437151312828064,
          0.30734339356422424,
          0.2475823611021042,
          0.22877968847751617,
          0.23970568180084229,
          0.2552901804447174,
          0.23915830254554749,
          0.21154437959194183,
          0.2710934281349182,
          0.20024517178535461,
          0.21014222502708435,
          0.20902732014656067,
          0.18050196766853333,
          0.17784760892391205,
          0.1993778944015503,
          0.23125658929347992,
          0.18410421907901764,
          0.151799276471138,
          0.1562032848596573,
          0.13414980471134186,
          0.2055463045835495,
          0.1772826611995697,
          0.15667617321014404,
          0.13930408656597137,
          0.14764399826526642,
          0.0902557298541069,
          0.10344672948122025,
          0.14752650260925293,
          0.09746986627578735,
          0.11587661504745483,
          0.11514844000339508,
          0.11499115079641342,
          0.10776110738515854,
          0.09080278128385544,
          0.06968867778778076,
          0.0877070277929306,
          0.05630052462220192,
          0.058143384754657745,
          0.05337727814912796,
          0.0768599659204483,
          0.052912138402462006,
          0.08025313168764114,
          0.10051620006561279,
          0.07666110247373581,
          0.0675692930817604,
          0.06031230092048645,
          0.06627675145864487,
          0.06411459296941757,
          0.0487058125436306,
          0.04711277037858963,
          0.05425252765417099,
          0.027929384261369705,
          0.036503542214632034,
          0.03422710299491882,
          0.030416948720812798,
          0.0447891540825367,
          0.021238407120108604,
          0.03264110907912254,
          0.027598286047577858,
          0.025872766971588135,
          0.03051372990012169,
          0.02647855505347252,
          0.028496233746409416,
          0.02295050024986267,
          0.024634527042508125,
          0.03116818331182003,
          0.015505627728998661,
          0.018505141139030457,
          0.019990921020507812,
          0.025212837383151054,
          0.01426890678703785,
          0.019590450450778008,
          0.020660100504755974,
          0.016273848712444305,
          0.013204952701926231,
          0.014248323626816273,
          0.020301954820752144,
          0.011888976208865643,
          0.023116927593946457,
          0.025975598022341728,
          0.011924900114536285,
          0.011094162240624428,
          0.016987590119242668,
          0.00854451209306717,
          0.011828341521322727,
          0.007399991154670715,
          0.009714188985526562,
          0.010103428736329079,
          0.013010510243475437,
          0.007813694886863232
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cross Training Loss over Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "for name, training in cross_training_progress.items():\n",
    "    fig.add_trace(go.Scatter(x=np.arange(EPOCHS), y=training.history[\"loss\"], mode='lines', name=f\"{name} Training Loss\"))\n",
    "\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Cross Training Loss over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    yaxis_type='log',         # <-- this makes the y-axis logarithmic\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94e92312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "LSTM Training Accuracy",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          0.25,
          0.375,
          0.3571428656578064,
          0.4464285671710968,
          0.5,
          0.5357142686843872,
          0.4642857015132904,
          0.5357142686843872,
          0.5178571343421936,
          0.5178571343421936,
          0.5535714030265808,
          0.6071428656578064,
          0.5535714030265808,
          0.6607142686843872,
          0.6428571343421936,
          0.5357142686843872,
          0.5714285969734192,
          0.6607142686843872,
          0.6607142686843872,
          0.6785714030265808,
          0.6964285969734192,
          0.75,
          0.7142857313156128,
          0.7678571343421936,
          0.7142857313156128,
          0.8035714030265808,
          0.75,
          0.7857142686843872,
          0.8214285969734192,
          0.7857142686843872,
          0.7321428656578064,
          0.6607142686843872,
          0.6964285969734192,
          0.75,
          0.7678571343421936,
          0.7857142686843872,
          0.8214285969734192,
          0.75,
          0.8571428656578064
         ]
        },
        {
         "mode": "lines",
         "name": "Bidirectional LSTM Training Accuracy",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          0.5,
          0.5,
          0.5,
          0.5178571343421936,
          0.5535714030265808,
          0.5714285969734192,
          0.5892857313156128,
          0.6071428656578064,
          0.6428571343421936,
          0.6785714030265808,
          0.75,
          0.7142857313156128,
          0.7142857313156128,
          0.6785714030265808,
          0.7857142686843872,
          0.6607142686843872,
          0.7678571343421936,
          0.8571428656578064,
          0.8571428656578064,
          0.8928571343421936,
          0.9107142686843872,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.875,
          0.9464285969734192,
          0.9285714030265808,
          0.8928571343421936,
          0.8928571343421936,
          0.9285714030265808,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          0.9642857313156128,
          0.9464285969734192,
          0.9821428656578064,
          0.9642857313156128,
          0.9642857313156128,
          0.8928571343421936,
          0.6964285969734192,
          0.7321428656578064,
          0.7142857313156128,
          0.75,
          0.875,
          0.8571428656578064,
          0.7678571343421936,
          0.8392857313156128,
          0.8035714030265808,
          0.8035714030265808,
          0.8392857313156128,
          0.875,
          0.8571428656578064,
          0.9107142686843872,
          0.8928571343421936,
          0.9285714030265808,
          0.8928571343421936,
          0.7678571343421936,
          0.7857142686843872,
          0.9107142686843872,
          0.8571428656578064,
          0.9642857313156128,
          0.9285714030265808,
          0.875,
          0.9642857313156128,
          0.9464285969734192,
          0.9464285969734192,
          0.9464285969734192,
          0.9642857313156128,
          0.8928571343421936,
          0.9642857313156128,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9464285969734192,
          0.8928571343421936,
          0.8392857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          0.9642857313156128,
          0.9464285969734192,
          0.9285714030265808,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          1,
          0.9642857313156128,
          0.9464285969734192,
          1,
          0.9821428656578064,
          1,
          0.9464285969734192,
          0.9107142686843872,
          0.8928571343421936,
          0.8214285969734192,
          0.7857142686843872,
          0.8035714030265808,
          0.8214285969734192,
          0.875,
          0.9107142686843872,
          0.8571428656578064
         ]
        },
        {
         "mode": "lines",
         "name": "CNN Training Accuracy",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          0.25,
          0.3928571343421936,
          0.4107142984867096,
          0.4642857015132904,
          0.5892857313156128,
          0.6071428656578064,
          0.6071428656578064,
          0.5714285969734192,
          0.6428571343421936,
          0.6428571343421936,
          0.6071428656578064,
          0.625,
          0.6071428656578064,
          0.5714285969734192,
          0.6071428656578064,
          0.6428571343421936,
          0.7321428656578064,
          0.5892857313156128,
          0.625,
          0.6964285969734192,
          0.6428571343421936,
          0.6964285969734192,
          0.625,
          0.6785714030265808,
          0.6607142686843872,
          0.625,
          0.7142857313156128,
          0.625,
          0.625,
          0.6964285969734192,
          0.5892857313156128,
          0.625,
          0.75,
          0.6607142686843872,
          0.7678571343421936,
          0.6071428656578064,
          0.6785714030265808,
          0.6964285969734192,
          0.7678571343421936,
          0.7321428656578064,
          0.6071428656578064,
          0.7142857313156128,
          0.8214285969734192,
          0.7321428656578064,
          0.7321428656578064,
          0.75,
          0.7142857313156128,
          0.7142857313156128,
          0.6964285969734192,
          0.7321428656578064,
          0.7321428656578064,
          0.7142857313156128,
          0.7678571343421936,
          0.7678571343421936,
          0.7142857313156128,
          0.8035714030265808,
          0.75,
          0.7857142686843872,
          0.75,
          0.7857142686843872,
          0.6964285969734192,
          0.7321428656578064,
          0.8035714030265808,
          0.6964285969734192,
          0.8571428656578064,
          0.7857142686843872,
          0.8214285969734192,
          0.8214285969734192,
          0.8571428656578064,
          0.8035714030265808,
          0.8214285969734192,
          0.8392857313156128,
          0.8035714030265808,
          0.8214285969734192,
          0.8035714030265808,
          0.8035714030265808,
          0.8214285969734192,
          0.8214285969734192,
          0.8214285969734192,
          0.7678571343421936,
          0.7321428656578064,
          0.875,
          0.8392857313156128,
          0.8214285969734192,
          0.8571428656578064,
          0.8214285969734192,
          0.8035714030265808,
          0.8392857313156128,
          0.875,
          0.8571428656578064,
          0.8214285969734192,
          0.8392857313156128,
          0.9107142686843872,
          0.8928571343421936,
          0.875,
          0.8571428656578064,
          0.8392857313156128,
          0.8571428656578064,
          0.8928571343421936,
          0.8214285969734192,
          0.8928571343421936,
          0.9107142686843872,
          0.8571428656578064,
          0.9107142686843872,
          0.8928571343421936,
          0.8928571343421936,
          0.9107142686843872,
          0.875,
          0.9285714030265808,
          0.9107142686843872,
          0.9642857313156128,
          0.8928571343421936,
          0.8928571343421936,
          0.9107142686843872,
          0.9642857313156128,
          0.9107142686843872,
          0.8928571343421936,
          0.9107142686843872,
          0.8571428656578064,
          0.9464285969734192,
          0.9107142686843872,
          0.9107142686843872,
          0.9107142686843872,
          0.8928571343421936,
          0.9285714030265808,
          0.9464285969734192,
          0.9464285969734192,
          0.8928571343421936,
          0.9107142686843872,
          0.9464285969734192,
          0.875,
          0.9285714030265808,
          0.9107142686843872,
          0.9464285969734192,
          0.9464285969734192,
          0.9464285969734192,
          0.8571428656578064,
          0.9464285969734192,
          0.9285714030265808,
          0.9464285969734192,
          0.9464285969734192,
          0.9464285969734192,
          0.9107142686843872,
          0.9464285969734192,
          0.9464285969734192,
          0.9642857313156128,
          0.9285714030265808,
          0.9464285969734192,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.875,
          0.9464285969734192,
          0.9642857313156128,
          0.9821428656578064,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9285714030265808,
          0.9107142686843872,
          0.9642857313156128,
          0.9285714030265808,
          0.9642857313156128,
          0.9821428656578064,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9642857313156128,
          0.9464285969734192,
          0.9821428656578064,
          1,
          0.9464285969734192,
          1,
          0.9821428656578064,
          0.9285714030265808,
          0.9642857313156128,
          1,
          0.9464285969734192,
          0.9107142686843872,
          0.9642857313156128,
          1,
          0.9821428656578064,
          0.9642857313156128,
          0.9464285969734192,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          0.9642857313156128,
          1,
          0.9642857313156128,
          1,
          0.9642857313156128,
          1,
          0.9464285969734192,
          1,
          0.9821428656578064,
          1,
          0.9821428656578064,
          1,
          1,
          0.9464285969734192,
          0.9642857313156128,
          0.9821428656578064,
          1,
          0.9642857313156128,
          0.9821428656578064,
          1,
          0.9642857313156128,
          0.9821428656578064,
          0.9642857313156128,
          0.9821428656578064,
          0.9821428656578064,
          1
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + self attention Training Accuracy",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          0.4107142984867096,
          0.5178571343421936,
          0.6607142686843872,
          0.5892857313156128,
          0.5535714030265808,
          0.5357142686843872,
          0.5178571343421936,
          0.625,
          0.5892857313156128,
          0.5714285969734192,
          0.6071428656578064,
          0.8035714030265808,
          0.6964285969734192,
          0.6964285969734192,
          0.75,
          0.7142857313156128,
          0.7321428656578064,
          0.7678571343421936,
          0.7142857313156128,
          0.7857142686843872,
          0.7857142686843872,
          0.8035714030265808,
          0.7678571343421936,
          0.75,
          0.7142857313156128,
          0.7857142686843872,
          0.6964285969734192,
          0.8035714030265808,
          0.8035714030265808,
          0.8571428656578064,
          0.7857142686843872,
          0.8392857313156128,
          0.8214285969734192,
          0.9107142686843872,
          0.8928571343421936,
          0.8392857313156128,
          0.8571428656578064,
          0.8928571343421936,
          0.8571428656578064,
          0.9107142686843872,
          0.9642857313156128,
          0.9821428656578064,
          0.9642857313156128,
          0.875,
          0.9107142686843872,
          0.8928571343421936,
          0.9285714030265808,
          0.9285714030265808,
          0.9464285969734192,
          0.9821428656578064,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9821428656578064,
          0.9464285969734192,
          0.9642857313156128,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9464285969734192,
          1,
          1,
          0.9821428656578064,
          0.9464285969734192,
          0.9821428656578064,
          1,
          0.9821428656578064,
          0.9821428656578064,
          0.9821428656578064,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        },
        {
         "mode": "lines",
         "name": "CNN + multihead attention Training Accuracy",
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "i4"
         },
         "y": [
          0.4642857015132904,
          0.6964285969734192,
          0.6071428656578064,
          0.6071428656578064,
          0.7321428656578064,
          0.7321428656578064,
          0.6964285969734192,
          0.7678571343421936,
          0.7142857313156128,
          0.6785714030265808,
          0.7142857313156128,
          0.8035714030265808,
          0.7857142686843872,
          0.75,
          0.7321428656578064,
          0.7321428656578064,
          0.6785714030265808,
          0.75,
          0.7678571343421936,
          0.7857142686843872,
          0.7142857313156128,
          0.8214285969734192,
          0.8035714030265808,
          0.7857142686843872,
          0.8392857313156128,
          0.8571428656578064,
          0.8035714030265808,
          0.75,
          0.7857142686843872,
          0.7678571343421936,
          0.8214285969734192,
          0.8214285969734192,
          0.875,
          0.8035714030265808,
          0.8571428656578064,
          0.8035714030265808,
          0.8392857313156128,
          0.8571428656578064,
          0.8392857313156128,
          0.8571428656578064,
          0.7857142686843872,
          0.875,
          0.875,
          0.9107142686843872,
          0.8571428656578064,
          0.8392857313156128,
          0.9464285969734192,
          0.9107142686843872,
          0.9285714030265808,
          0.9285714030265808,
          0.9464285969734192,
          0.9464285969734192,
          0.9285714030265808,
          0.9821428656578064,
          0.9464285969734192,
          0.9464285969734192,
          0.9464285969734192,
          0.9285714030265808,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9107142686843872,
          0.9464285969734192,
          0.9642857313156128,
          0.9642857313156128,
          0.9821428656578064,
          0.9464285969734192,
          0.9464285969734192,
          0.9821428656578064,
          0.9464285969734192,
          0.9464285969734192,
          1,
          1,
          0.9642857313156128,
          1,
          0.9821428656578064,
          0.9821428656578064,
          0.9464285969734192,
          0.9821428656578064,
          0.9821428656578064,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          0.9821428656578064,
          1,
          0.9821428656578064,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cross Training Accuracy over Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "for name, training in cross_training_progress.items():\n",
    "    fig.add_trace(go.Scatter(x=np.arange(EPOCHS), y=training.history[\"accuracy\"], mode='lines', name=f\"{name} Training Accuracy\"))\n",
    "\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Cross Training Accuracy over Epochs',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    yaxis_type='log',         # <-- this makes the y-axis logarithmic\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94314a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 573ms/step - loss: 1.1934 - accuracy: 0.3125\n",
      "8/8 [==============================] - 5s 648ms/step - loss: 2.9333 - accuracy: 0.2500\n",
      "8/8 [==============================] - 5s 651ms/step - loss: 1.2379 - accuracy: 0.4375\n",
      "8/8 [==============================] - 5s 666ms/step - loss: 0.7184 - accuracy: 0.8125\n",
      "8/8 [==============================] - 7s 930ms/step - loss: 2.9953 - accuracy: 0.2500\n",
      "8/8 [==============================] - 8s 959ms/step - loss: 1.6524 - accuracy: 0.7500\n",
      "8/8 [==============================] - 5s 674ms/step - loss: 0.3894 - accuracy: 0.8750\n",
      "8/8 [==============================] - 5s 653ms/step - loss: 3.1403 - accuracy: 0.5000\n",
      "8/8 [==============================] - 5s 655ms/step - loss: 0.8045 - accuracy: 0.6250\n",
      "8/8 [==============================] - 4s 620ms/step - loss: 1.4273 - accuracy: 0.6875\n",
      "8/8 [==============================] - 5s 646ms/step - loss: 4.3169 - accuracy: 0.2500\n",
      "8/8 [==============================] - 4s 600ms/step - loss: 2.1184 - accuracy: 0.3750\n",
      "8/8 [==============================] - 4s 584ms/step - loss: 1.9636 - accuracy: 0.5000\n",
      "8/8 [==============================] - 4s 596ms/step - loss: 4.5229 - accuracy: 0.2500\n",
      "8/8 [==============================] - 5s 682ms/step - loss: 3.1635 - accuracy: 0.5000\n",
      "[{'Model': 'LSTM', 'Task': 'Intra', 'Loss': 0.0003487993963062763, 'Accuracy': 1.0}, {'Model': 'Bidirectional LSTM', 'Task': 'Intra', 'Loss': 1.1950623957091011e-05, 'Accuracy': 1.0}, {'Model': 'CNN', 'Task': 'Intra', 'Loss': 0.0014628693461418152, 'Accuracy': 1.0}, {'Model': 'CNN + self attention', 'Task': 'Intra', 'Loss': 0.001731989672407508, 'Accuracy': 1.0}, {'Model': 'CNN + multihead attention', 'Task': 'Intra', 'Loss': 0.0008366784313693643, 'Accuracy': 1.0}, {'Model': 'LSTM', 'Task': 'Cross_1', 'Loss': 1.193352222442627, 'Accuracy': 0.3125}, {'Model': 'LSTM', 'Task': 'Cross_2', 'Loss': 2.9333462715148926, 'Accuracy': 0.25}, {'Model': 'LSTM', 'Task': 'Cross_3', 'Loss': 1.2379070520401, 'Accuracy': 0.4375}, {'Model': 'Bidirectional LSTM', 'Task': 'Cross_1', 'Loss': 0.7183987498283386, 'Accuracy': 0.8125}, {'Model': 'Bidirectional LSTM', 'Task': 'Cross_2', 'Loss': 2.9952516555786133, 'Accuracy': 0.25}, {'Model': 'Bidirectional LSTM', 'Task': 'Cross_3', 'Loss': 1.6523751020431519, 'Accuracy': 0.75}, {'Model': 'CNN', 'Task': 'Cross_1', 'Loss': 0.3894091844558716, 'Accuracy': 0.875}, {'Model': 'CNN', 'Task': 'Cross_2', 'Loss': 3.1402618885040283, 'Accuracy': 0.5}, {'Model': 'CNN', 'Task': 'Cross_3', 'Loss': 0.8045410513877869, 'Accuracy': 0.625}, {'Model': 'CNN + self attention', 'Task': 'Cross_1', 'Loss': 1.4273329973220825, 'Accuracy': 0.6875}, {'Model': 'CNN + self attention', 'Task': 'Cross_2', 'Loss': 4.316936016082764, 'Accuracy': 0.25}, {'Model': 'CNN + self attention', 'Task': 'Cross_3', 'Loss': 2.118443012237549, 'Accuracy': 0.375}, {'Model': 'CNN + multihead attention', 'Task': 'Cross_1', 'Loss': 1.9636197090148926, 'Accuracy': 0.5}, {'Model': 'CNN + multihead attention', 'Task': 'Cross_2', 'Loss': 4.522858619689941, 'Accuracy': 0.25}, {'Model': 'CNN + multihead attention', 'Task': 'Cross_3', 'Loss': 3.163492202758789, 'Accuracy': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "# Collect the test results for each model\n",
    "for name, model in MODELS.items():\n",
    "\n",
    "    # Evaluate on the first test folder\n",
    "    loss_1, accuracy_1 = model.evaluate(\n",
    "        keras_data_generator(\n",
    "            CROSS_TEST_1_FOLDER,\n",
    "            batch_size=2,\n",
    "            preprocessing_pipeline=cross_preprocessing_pipeline,\n",
    "        ),\n",
    "        steps=8,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    results.append(\n",
    "        {\"Model\": name, \"Task\": \"Cross_1\", \"Loss\": loss_1, \"Accuracy\": accuracy_1}\n",
    "    )\n",
    "\n",
    "    # Evaluate on the second test folder\n",
    "    loss_2, accuracy_2 = model.evaluate(\n",
    "        keras_data_generator(\n",
    "            CROSS_TEST_2_FOLDER,\n",
    "            batch_size=2,\n",
    "            preprocessing_pipeline=cross_preprocessing_pipeline,\n",
    "        ),\n",
    "        steps=8,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    results.append(\n",
    "        {\"Model\": name, \"Task\": \"Cross_2\", \"Loss\": loss_2, \"Accuracy\": accuracy_2}\n",
    "    )\n",
    "\n",
    "    # Evaluate on the second test folder\n",
    "    loss_3, accuracy_3 = model.evaluate(\n",
    "        keras_data_generator(\n",
    "            CROSS_TEST_3_FOLDER,\n",
    "            batch_size=2,\n",
    "            preprocessing_pipeline=cross_preprocessing_pipeline,\n",
    "        ),\n",
    "        steps=8,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    results.append(\n",
    "        {\"Model\": name, \"Task\": \"Cross_3\", \"Loss\": loss_3, \"Accuracy\": accuracy_3}\n",
    "    )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fafc7",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3cb2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53eb465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Task</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Intra</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bidirectional LSTM</td>\n",
       "      <td>Intra</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Intra</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN + self attention</td>\n",
       "      <td>Intra</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN + multihead attention</td>\n",
       "      <td>Intra</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Cross_1</td>\n",
       "      <td>1.193352</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Cross_2</td>\n",
       "      <td>2.933346</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Cross_3</td>\n",
       "      <td>1.237907</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bidirectional LSTM</td>\n",
       "      <td>Cross_1</td>\n",
       "      <td>0.718399</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bidirectional LSTM</td>\n",
       "      <td>Cross_2</td>\n",
       "      <td>2.995252</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bidirectional LSTM</td>\n",
       "      <td>Cross_3</td>\n",
       "      <td>1.652375</td>\n",
       "      <td>0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Cross_1</td>\n",
       "      <td>0.389409</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Cross_2</td>\n",
       "      <td>3.140262</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Cross_3</td>\n",
       "      <td>0.804541</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CNN + self attention</td>\n",
       "      <td>Cross_1</td>\n",
       "      <td>1.427333</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNN + self attention</td>\n",
       "      <td>Cross_2</td>\n",
       "      <td>4.316936</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CNN + self attention</td>\n",
       "      <td>Cross_3</td>\n",
       "      <td>2.118443</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CNN + multihead attention</td>\n",
       "      <td>Cross_1</td>\n",
       "      <td>1.963620</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CNN + multihead attention</td>\n",
       "      <td>Cross_2</td>\n",
       "      <td>4.522859</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CNN + multihead attention</td>\n",
       "      <td>Cross_3</td>\n",
       "      <td>3.163492</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model     Task      Loss  Accuracy\n",
       "0                        LSTM    Intra  0.000349    1.0000\n",
       "1          Bidirectional LSTM    Intra  0.000012    1.0000\n",
       "2                         CNN    Intra  0.001463    1.0000\n",
       "3        CNN + self attention    Intra  0.001732    1.0000\n",
       "4   CNN + multihead attention    Intra  0.000837    1.0000\n",
       "5                        LSTM  Cross_1  1.193352    0.3125\n",
       "6                        LSTM  Cross_2  2.933346    0.2500\n",
       "7                        LSTM  Cross_3  1.237907    0.4375\n",
       "8          Bidirectional LSTM  Cross_1  0.718399    0.8125\n",
       "9          Bidirectional LSTM  Cross_2  2.995252    0.2500\n",
       "10         Bidirectional LSTM  Cross_3  1.652375    0.7500\n",
       "11                        CNN  Cross_1  0.389409    0.8750\n",
       "12                        CNN  Cross_2  3.140262    0.5000\n",
       "13                        CNN  Cross_3  0.804541    0.6250\n",
       "14       CNN + self attention  Cross_1  1.427333    0.6875\n",
       "15       CNN + self attention  Cross_2  4.316936    0.2500\n",
       "16       CNN + self attention  Cross_3  2.118443    0.3750\n",
       "17  CNN + multihead attention  Cross_1  1.963620    0.5000\n",
       "18  CNN + multihead attention  Cross_2  4.522859    0.2500\n",
       "19  CNN + multihead attention  Cross_3  3.163492    0.5000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adcc03b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Task=Intra<br>Model=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "Intra",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Intra",
         "offsetgroup": "Intra",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "LSTM",
          "Bidirectional LSTM",
          "CNN",
          "CNN + self attention",
          "CNN + multihead attention"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAOLbNj8AAABA8Q/pPgAAAAC491c/AAAAwH1gXD8AAABAkWpLPw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Task=Cross_1<br>Model=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "Cross_1",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Cross_1",
         "offsetgroup": "Cross_1",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "LSTM",
          "Bidirectional LSTM",
          "CNN",
          "CNN + self attention",
          "CNN + multihead attention"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAgPgX8z8AAABgH/3mPwAAAIAU7Ng/AAAAIFvW9j8AAACA/Gr/Pw==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Task=Cross_2<br>Model=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "Cross_2",
         "marker": {
          "color": "#00cc96",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Cross_2",
         "offsetgroup": "Cross_2",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "LSTM",
          "Bidirectional LSTM",
          "CNN",
          "CNN + self attention",
          "CNN + multihead attention"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAQH53B0AAAACARvYHQAAAAKBBHwlAAAAA4IpEEUAAAABAaBcSQA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Task=Cross_3<br>Model=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "Cross_3",
         "marker": {
          "color": "#ab63fa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Cross_3",
         "offsetgroup": "Cross_3",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "LSTM",
          "Bidirectional LSTM",
          "CNN",
          "CNN + self attention",
          "CNN + multihead attention"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAoHfO8z8AAADgIHD6PwAAAODMvuk/AAAAQJLyAEAAAAAA1U4JQA==",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "legend": {
         "title": {
          "text": "Task"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Test Loss"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig = px.bar(\n",
    "    results_df,\n",
    "    x=\"Model\",\n",
    "    y=\"Loss\",\n",
    "    color=\"Task\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Test Loss\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed743dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
