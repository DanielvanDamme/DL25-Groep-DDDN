{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c9752d",
   "metadata": {},
   "source": [
    "The data extraction was based on the provided code, with slight adjustments for where we decided to store the downloaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba0f61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/train/\"))\n",
    "\n",
    "def get_dataset_name(filename_with_dir):\n",
    "    filename_without_dir = os.path.basename(filename_with_dir)\n",
    "    temp = filename_without_dir.split('.')[:-1]\n",
    "    dataset_name = ''.join(temp)\n",
    "    temp = dataset_name.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "def extract_data_from_folder_by_file(folder_path, shuffle=False):\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "    for file_name in files:\n",
    "        \n",
    "        filename_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            matrix = f.get(dataset_name)[()]\n",
    "            yield dataset_name, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d7e5",
   "metadata": {},
   "source": [
    "We first have to scale the data across different files in the same way, so we have to scan the files and find min max to perform the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5bf681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_minmax_from_all_files(folder_path: str) -> tuple:\n",
    "    # Placeholders\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "\n",
    "    for (name, data) in extract_data_from_folder_by_file(folder_path):\n",
    "        data = data.T\n",
    "        if min_val is None:\n",
    "            min_val = np.min(data, axis=0)\n",
    "            max_val = np.max(data, axis=0)\n",
    "        else:\n",
    "            # Update min_val and max_val\n",
    "            min_val = np.minimum(min_val, np.min(data, axis=0))\n",
    "            max_val = np.maximum(max_val, np.max(data, axis=0))\n",
    "        \n",
    "    return min_val, max_val\n",
    "\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    # Scale the data to the range [0, 1]\n",
    "    return (data - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5558f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values: (248,), Max values: (248,)\n"
     ]
    }
   ],
   "source": [
    "min_val, max_val = learn_minmax_from_all_files(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Min values: {min_val.shape}, Max values: {max_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c45eb",
   "metadata": {},
   "source": [
    "Because of independent sampling, we can just sample each file independently and the same dropout should occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28617963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data: np.array, factor: float) -> np.array:\n",
    "    # Downsample with uniform chance of emission\n",
    "    \n",
    "    # Calculate the number of samples to keep\n",
    "    num_samples = int(len(data) * factor)\n",
    "    # Generate random indices to select samples\n",
    "    indices = np.random.choice(len(data), num_samples, replace=False)\n",
    "    # Select the samples using the random indices\n",
    "    downsampled_data = data[indices]\n",
    "    # Sort the indices to maintain the original order\n",
    "    downsampled_data.sort()\n",
    "    return downsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cc899",
   "metadata": {},
   "source": [
    "Here we can set the downsample factor for all sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c75352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_FACTOR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edf396",
   "metadata": {},
   "source": [
    "Here, we define the preprocessing steps that we apply to all data after reading it from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3aa81762",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val), \n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f79afc",
   "metadata": {},
   "source": [
    "We should also create labels based on the file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0ae138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(file_name:str) -> np.ndarray:\n",
    "    # Return a one-hot encoded label based on the file name, there are4 classes\n",
    "    # 0: rest\n",
    "    if \"rest_\" in file_name:\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    # 1: task_motor\n",
    "    elif \"task_motor_\" in file_name:\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    # 2: task_story_math\n",
    "    elif \"task_story_math_\" in file_name:\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    # 3: task_working_memory\n",
    "    elif \"task_working_memory_\" in file_name:\n",
    "        return np.array([0, 0, 0, 1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file name: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2de89b",
   "metadata": {},
   "source": [
    "To create batches by number of files, we can use a generator like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f7f9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(number_of_files_per_batch: int, preprocessing_pipeline: list = None, shuffle_files=False) -> Iterator[tuple]:\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for n, (name, data) in enumerate(extract_data_from_folder_by_file(INTRA_TRAIN_FOLDER, shuffle=shuffle_files)):\n",
    "        data = data.T\n",
    "        if preprocessing_pipeline:\n",
    "            for preprocessing_step in preprocessing_pipeline:\n",
    "                data = preprocessing_step(data)\n",
    "        # Add the preprocessed data to the batch\n",
    "        batch_data.append(data)\n",
    "\n",
    "        # Generate the label matrix of the length of the data for the current file\n",
    "        label_vector = np.tile(generate_label(name), (data.shape[0], 1))\n",
    "        batch_labels.append(label_vector)\n",
    "\n",
    "\n",
    "        # Check if we have reached the desired batch size\n",
    "        if (n + 1) % number_of_files_per_batch == 0:\n",
    "            # Stack along the first axis (like a batch dimension)\n",
    "            yield (np.concatenate(batch_data, axis=0), np.concatenate(batch_labels, axis=0))\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "\n",
    "    # Optional: yield the remainder if not divisible\n",
    "    if batch_data:\n",
    "        yield (np.concatenate(batch_data, axis=0), np.concatenate(batch_labels, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c33eecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 248) (24, 4)\n",
      "[[0.12451467 0.136548   0.13877756 ... 0.77114324 0.77218858 0.98219193]\n",
      " [0.08579206 0.09775649 0.10059086 ... 0.77538048 0.77798728 0.98635244]\n",
      " [0.13041652 0.13491226 0.1398753  ... 0.77973122 0.78462014 0.97988468]\n",
      " ...\n",
      " [0.05455497 0.05735303 0.05835616 ... 0.94892235 0.94942445 0.95048357]\n",
      " [0.01126965 0.01480185 0.01596172 ... 0.97989852 0.98088233 0.98591771]\n",
      " [0.04490415 0.04547977 0.04612234 ... 0.96460717 0.96749138 0.97164311]] [[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n",
      "(24, 248) (24, 4)\n",
      "[[0.05804587 0.06670499 0.06859928 ... 0.92570719 0.92955797 0.94059463]\n",
      " [0.04866248 0.05297468 0.05474345 ... 0.92477624 0.93052868 0.94982356]\n",
      " [0.05711845 0.06220372 0.06521095 ... 0.90516952 0.9056015  0.91499122]\n",
      " ...\n",
      " [0.04411501 0.04934978 0.05424091 ... 0.94650942 0.96169588 0.9666948 ]\n",
      " [0.04993109 0.05547085 0.05635893 ... 0.95260112 0.95351977 0.97026221]\n",
      " [0.04310312 0.04767478 0.04871727 ... 0.93326019 0.93766962 0.95734949]] [[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]]\n",
      "(24, 248) (24, 4)\n",
      "[[0.035574   0.06643115 0.06740408 ... 0.91457806 0.91830934 0.91913907]\n",
      " [0.04569542 0.07039153 0.0716686  ... 0.902145   0.91310074 0.92217937]\n",
      " [0.06783147 0.07199099 0.07929983 ... 0.92400315 0.92418886 0.93277824]\n",
      " ...\n",
      " [0.01842831 0.03121677 0.03488348 ... 0.92649991 0.93323939 0.9333429 ]\n",
      " [0.0305196  0.03858307 0.05832078 ... 0.92925281 0.9321207  0.93555999]\n",
      " [0.02884782 0.05461417 0.05998356 ... 0.93192213 0.93710673 0.94380017]] [[0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]]\n",
      "(24, 248) (24, 4)\n",
      "[[0.0710728  0.07723958 0.0773418  ... 0.89466848 0.90347781 0.90782455]\n",
      " [0.06384527 0.08248324 0.08852847 ... 0.84623654 0.85649282 0.89101399]\n",
      " [0.06667092 0.0676649  0.06923091 ... 0.89293586 0.92260832 0.92404947]\n",
      " ...\n",
      " [0.07525201 0.08046773 0.1021177  ... 0.85276767 0.85509011 0.86004567]\n",
      " [0.07608212 0.08960045 0.09292027 ... 0.85493625 0.85797603 0.87307929]\n",
      " [0.08300561 0.08380529 0.10736933 ... 0.84818792 0.85750718 0.8709718 ]] [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for data, labels in create_batches(number_of_files_per_batch=8, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=False):\n",
    "    print(data.shape, labels.shape)\n",
    "    print(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac02c17",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fe9f6",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7825ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = []\n",
    "# TODO: which hyperparameters are we going to grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de527f",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87683b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e283a28",
   "metadata": {},
   "source": [
    "### Trainingloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4a3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0934f51",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8a778",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
