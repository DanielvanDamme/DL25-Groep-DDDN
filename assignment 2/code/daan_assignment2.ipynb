{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Intra/train\")\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, \"Intra/test\")\n",
    "DOWNSAMPLE_FACTOR = 0.7\n",
    "\n",
    "# --- File and Data Handling ---\n",
    "def get_dataset_name(filepath: str) -> str:\n",
    "    filename = os.path.basename(filepath)\n",
    "    return \"_\".join(filename.split('.')[:-1][0].split('_')[:-1])\n",
    "\n",
    "def extract_data_from_folder(folder_path: str, shuffle: bool = False) -> Iterator[tuple[str, np.ndarray]]:\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "    for file_name in files:\n",
    "        with h5py.File(os.path.join(folder_path, file_name), 'r') as f:\n",
    "            dataset_name = get_dataset_name(file_name)\n",
    "            yield dataset_name, f[dataset_name][()].T  # transpose once here\n",
    "\n",
    "def learn_minmax(folder_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    min_val, max_val = None, None\n",
    "    for _, data in extract_data_from_folder(folder_path):\n",
    "        min_data, max_data = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        min_val = min_data if min_val is None else np.minimum(min_val, min_data)\n",
    "        max_val = max_data if max_val is None else np.maximum(max_val, max_data)\n",
    "    return min_val, max_val\n",
    "\n",
    "def generate_label(name: str) -> np.ndarray:\n",
    "    classes = [\"rest\", \"task_motor\", \"task_story_math\", \"task_working_memory\"]\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls + \"_\" in name:\n",
    "            label = np.zeros(len(classes))\n",
    "            label[i] = 1\n",
    "            return label\n",
    "    raise ValueError(f\"Unknown file name: {name}\")\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def downsample(data: np.ndarray, factor: float) -> np.ndarray:\n",
    "    num_samples = int(len(data) * factor)\n",
    "    indices = np.floor(np.arange(num_samples) * (len(data) / num_samples)).astype(int)\n",
    "    return data[indices]\n",
    "\n",
    "def add_gaussian_noise(data: np.ndarray, stddev: float = 0.01) -> np.ndarray:\n",
    "    noise = np.random.normal(0, stddev, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def time_shift(data: np.ndarray, shift_max: int = 10) -> np.ndarray:\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift, axis=0)\n",
    "\n",
    "def channel_dropout(data: np.ndarray, dropout_rate: float = 0.1) -> np.ndarray:\n",
    "    num_channels = data.shape[1]\n",
    "    mask = np.random.rand(num_channels) > dropout_rate\n",
    "    return data * mask[np.newaxis, :]\n",
    "\n",
    "def random_scaling(data: np.ndarray, scale_range=(0.9, 1.1)) -> np.ndarray:\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    return data * scale\n",
    "\n",
    "def bandpass_filter(data: np.ndarray, lowcut=0.5, highcut=30.0, fs=250.0, order=3) -> np.ndarray:\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "def zscore_per_channel(data: np.ndarray) -> np.ndarray:\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    std[std == 0] = 1  # Prevent division by zero\n",
    "    return (data - mean) / std\n",
    "\n",
    "def baseline_correction(data: np.ndarray, baseline_duration=500) -> np.ndarray:\n",
    "    if data.shape[0] < baseline_duration:\n",
    "        return data\n",
    "    baseline_mean = np.mean(data[:baseline_duration], axis=0)\n",
    "    return data - baseline_mean\n",
    "\n",
    "# --- Batching ---\n",
    "def create_batches(folder: str, batch_size: int, preprocessing: list = None, shuffle: bool = False) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n",
    "    batch_data, batch_labels = [], []\n",
    "    for i, (name, data) in enumerate(extract_data_from_folder(folder, shuffle)):\n",
    "        if preprocessing:\n",
    "            for fn in preprocessing:\n",
    "                data = fn(data)\n",
    "        batch_data.append(data)\n",
    "        batch_labels.append(generate_label(name))\n",
    "\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            yield np.array(batch_data), np.array(batch_labels)\n",
    "            batch_data, batch_labels = [], []\n",
    "    if batch_data:\n",
    "        yield np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "# --- Model ---\n",
    "def create_model(input_shape=(3562, 248)) -> Sequential:\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=False, input_shape=input_shape),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_folder: str, epochs: int = 10, batch_size: int = 8, preprocessing: list = None):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_X, batch_y in create_batches(train_folder, batch_size, preprocessing, shuffle=False): # TODO was True\n",
    "            indices = np.random.permutation(len(batch_X))\n",
    "            model.fit(batch_X[indices], batch_y[indices], verbose=0)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, folder: str, batch_size: int, preprocessing: list) -> tuple[float, float]:\n",
    "    losses, accuracies = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        loss, acc = model.evaluate(batch_X, batch_y, verbose=0)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "def detailed_evaluation(model, folder, batch_size, preprocessing):\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        preds = model.predict(batch_X)\n",
    "        y_true.extend(np.argmax(batch_y, axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Training and Evaluation ---\n",
    "min_val, max_val = learn_minmax(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Learned min/max shapes: {min_val.shape}, {max_val.shape}\")\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    # baseline_correction,\n",
    "    # bandpass_filter,\n",
    "    lambda x: scale_data(x, min_val, max_val),\n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR),\n",
    "    add_gaussian_noise, # These are all added for more general performance ()\n",
    "    # time_shift,\n",
    "    channel_dropout\n",
    "    # random_scaling\n",
    "]\n",
    "\n",
    "model = create_model()\n",
    "trained_model = train_model(model, INTRA_TRAIN_FOLDER, epochs=10, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "\n",
    "train_loss, train_acc = evaluate_model(trained_model, INTRA_TRAIN_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_model(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_evaluation(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross configuration\n",
    "CROSS_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Cross/train\")\n",
    "CROSS_TEST1_FOLDER = os.path.join(DATA_PATH, \"Cross/test1\")\n",
    "CROSS_TEST2_FOLDER = os.path.join(DATA_PATH, \"Cross/test2\")\n",
    "CROSS_TEST3_FOLDER = os.path.join(DATA_PATH, \"Cross/test3\")\n",
    "DOWNSAMPLE_FACTOR_CROSS = 1.0\n",
    "\n",
    "min_val_cross, max_val_cross = learn_minmax(CROSS_TRAIN_FOLDER)\n",
    "\n",
    "preprocessing_pipeline_cross = [\n",
    "    lambda x: scale_data(x, min_val_cross, max_val_cross),\n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR_CROSS),\n",
    "    add_gaussian_noise,\n",
    "    channel_dropout\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual training\n",
    "model_cross = create_model() # TODO change this to a more 'general' model\n",
    "trained_model_cross = train_model(model_cross, CROSS_TRAIN_FOLDER, epochs = 10, batch_size = 8, \n",
    "                                  preprocessing = preprocessing_pipeline_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1 folder\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      0.25      0.40         4\n",
      "           2       0.44      1.00      0.62         4\n",
      "           3       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.86      0.69      0.67        16\n",
      "weighted avg       0.86      0.69      0.67        16\n",
      "\n",
      "Test2 folder\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         4\n",
      "           1       0.75      0.75      0.75         4\n",
      "           2       1.00      0.75      0.86         4\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.56      0.62      0.57        16\n",
      "weighted avg       0.56      0.62      0.57        16\n",
      "\n",
      "Test3 folder\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       0.60      0.75      0.67         4\n",
      "           3       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.80      0.75      0.75        16\n",
      "weighted avg       0.80      0.75      0.75        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test1 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST1_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)\n",
    "print('Test2 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST2_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)\n",
    "print('Test3 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST3_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
