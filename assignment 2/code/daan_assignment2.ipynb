{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 10:14:05.499823: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 10:14:05.613347: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 10:14:05.699469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749716045.788594    3953 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749716045.813253    3953 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749716045.993528    3953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749716045.993547    3953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749716045.993548    3953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749716045.993549    3953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 10:14:06.014923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Intra/train\")\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, \"Intra/test\")\n",
    "DOWNSAMPLE_FACTOR = 0.7\n",
    "\n",
    "# --- File and Data Handling ---\n",
    "def get_dataset_name(filepath: str) -> str:\n",
    "    filename = os.path.basename(filepath)\n",
    "    return \"_\".join(filename.split('.')[:-1][0].split('_')[:-1])\n",
    "\n",
    "def extract_data_from_folder(folder_path: str, shuffle: bool = False) -> Iterator[tuple[str, np.ndarray]]:\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "    for file_name in files:\n",
    "        with h5py.File(os.path.join(folder_path, file_name), 'r') as f:\n",
    "            dataset_name = get_dataset_name(file_name)\n",
    "            yield dataset_name, f[dataset_name][()].T  # transpose once here\n",
    "\n",
    "def learn_minmax(folder_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    min_val, max_val = None, None\n",
    "    for _, data in extract_data_from_folder(folder_path):\n",
    "        min_data, max_data = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        min_val = min_data if min_val is None else np.minimum(min_val, min_data)\n",
    "        max_val = max_data if max_val is None else np.maximum(max_val, max_data)\n",
    "    return min_val, max_val\n",
    "\n",
    "def generate_label(name: str) -> np.ndarray:\n",
    "    classes = [\"rest\", \"task_motor\", \"task_story_math\", \"task_working_memory\"]\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls + \"_\" in name:\n",
    "            label = np.zeros(len(classes))\n",
    "            label[i] = 1\n",
    "            return label\n",
    "    raise ValueError(f\"Unknown file name: {name}\")\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def downsample(data: np.ndarray, factor: float) -> np.ndarray:\n",
    "    num_samples = int(len(data) * factor)\n",
    "    indices = np.floor(np.arange(num_samples) * (len(data) / num_samples)).astype(int)\n",
    "    return data[indices]\n",
    "\n",
    "def add_gaussian_noise(data: np.ndarray, stddev: float = 0.01) -> np.ndarray:\n",
    "    noise = np.random.normal(0, stddev, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def time_shift(data: np.ndarray, shift_max: int = 10) -> np.ndarray:\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift, axis=0)\n",
    "\n",
    "def channel_dropout(data: np.ndarray, dropout_rate: float = 0.1) -> np.ndarray:\n",
    "    num_channels = data.shape[1]\n",
    "    mask = np.random.rand(num_channels) > dropout_rate\n",
    "    return data * mask[np.newaxis, :]\n",
    "\n",
    "def random_scaling(data: np.ndarray, scale_range=(0.9, 1.1)) -> np.ndarray:\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    return data * scale\n",
    "\n",
    "def bandpass_filter(data: np.ndarray, lowcut=0.5, highcut=30.0, fs=250.0, order=3) -> np.ndarray:\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "def zscore_per_channel(data: np.ndarray) -> np.ndarray:\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    std[std == 0] = 1  # Prevent division by zero\n",
    "    return (data - mean) / std\n",
    "\n",
    "def baseline_correction(data: np.ndarray, baseline_duration=500) -> np.ndarray:\n",
    "    if data.shape[0] < baseline_duration:\n",
    "        return data\n",
    "    baseline_mean = np.mean(data[:baseline_duration], axis=0)\n",
    "    return data - baseline_mean\n",
    "\n",
    "# --- Batching ---\n",
    "def create_batches(folder: str, batch_size: int, preprocessing: list = None, shuffle: bool = False) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n",
    "    batch_data, batch_labels = [], []\n",
    "    for i, (name, data) in enumerate(extract_data_from_folder(folder, shuffle)):\n",
    "        if preprocessing:\n",
    "            for fn in preprocessing:\n",
    "                data = fn(data)\n",
    "        batch_data.append(data)\n",
    "        batch_labels.append(generate_label(name))\n",
    "\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            yield np.array(batch_data), np.array(batch_labels)\n",
    "            batch_data, batch_labels = [], []\n",
    "    if batch_data:\n",
    "        yield np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "# --- Model ---\n",
    "def create_model(input_shape) -> Sequential:\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=False, input_shape=input_shape),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_folder: str, epochs: int = 10, batch_size: int = 8, preprocessing: list = None):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_X, batch_y in create_batches(train_folder, batch_size, preprocessing, shuffle=False): # TODO was True\n",
    "            indices = np.random.permutation(len(batch_X))\n",
    "            model.fit(batch_X[indices], batch_y[indices], verbose=0)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, folder: str, batch_size: int, preprocessing: list) -> tuple[float, float]:\n",
    "    losses, accuracies = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        loss, acc = model.evaluate(batch_X, batch_y, verbose=0)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "def detailed_evaluation(model, folder, batch_size, preprocessing):\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        preds = model.predict(batch_X)\n",
    "        y_true.extend(np.argmax(batch_y, axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned min/max shapes: (248,), (248,)\n",
      "Determined input shape: (24936, 248)\n"
     ]
    }
   ],
   "source": [
    "# --- Run Training and Evaluation ---\n",
    "min_val, max_val = learn_minmax(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Learned min/max shapes: {min_val.shape}, {max_val.shape}\")\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    # baseline_correction,\n",
    "    # bandpass_filter,\n",
    "    lambda x: scale_data(x, min_val, max_val),\n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR),\n",
    "    add_gaussian_noise, # These are all added for more general performance ()\n",
    "    # time_shift,\n",
    "    channel_dropout\n",
    "    # random_scaling\n",
    "]\n",
    "\n",
    "###########################\n",
    "def load_sample(data_folder):\n",
    "    # Look for files with the .h5 extension.\n",
    "    sample_files = [f for f in os.listdir(data_folder) if f.endswith('.h5')]\n",
    "    if not sample_files:\n",
    "        raise ValueError(\"No .h5 files found in the folder!\")\n",
    "        \n",
    "    sample_path = os.path.join(data_folder, sample_files[0])\n",
    "    \n",
    "    # Open the file and load data from the first available dataset.\n",
    "    with h5py.File(sample_path, 'r') as f:\n",
    "        dataset_keys = list(f.keys())\n",
    "        if not dataset_keys:\n",
    "            raise ValueError(\"No dataset found in the file!\")\n",
    "        # Adjust the key ('data', for example) if needed.\n",
    "        sample = f[dataset_keys[0]][:]\n",
    "    return sample\n",
    "\n",
    "def determine_input_shape(data_folder, preprocessing_pipeline, min_val):\n",
    "    sample = load_sample(data_folder)\n",
    "    \n",
    "    # If the sample has a batch dimension, use the first element.\n",
    "    if sample.ndim == 3:\n",
    "        sample = sample[0]\n",
    "    \n",
    "    # Check if the sample's first dimension equals the feature dimension as given by min_val.\n",
    "    # We expect the data to have shape (timesteps, features). If its shape is (features, timesteps)\n",
    "    # then the first dimension (248) will match min_val's shape (248,). In that case, transpose.\n",
    "    if sample.shape[0] == min_val.shape[0]:\n",
    "        sample = sample.T\n",
    "\n",
    "    # Apply each preprocessing step sequentially.\n",
    "    for process in preprocessing_pipeline:\n",
    "        sample = process(sample)\n",
    "    \n",
    "    # Return the final shape that will be used as input_shape for the model.\n",
    "    return sample.shape\n",
    "############################\n",
    "\n",
    "input_shape = determine_input_shape(INTRA_TRAIN_FOLDER, preprocessing_pipeline, min_val)\n",
    "print(f\"Determined input shape: {input_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=input_shape)\n",
    "trained_model = train_model(model, INTRA_TRAIN_FOLDER, epochs=10, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "\n",
    "train_loss, train_acc = evaluate_model(trained_model, INTRA_TRAIN_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_model(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_evaluation(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross testing\n",
    "\n",
    "De inputs in de eerse lagen van het model worden gefilterd naar deelfde soort features waarop je op  kan trainen. Lokale patronen vangen. Verolgens LSTM of attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "CROSS_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Cross/train\")\n",
    "CROSS1_TEST_FOLDER = os.path.join(DATA_PATH, \"Cross/test1\")\n",
    "CROSS2_TEST_FOLDER = os.path.join(DATA_PATH, \"Cross/test2\")\n",
    "CROSS3_TEST_FOLDER = os.path.join(DATA_PATH, \"Cross/test3\")\n",
    "DOWNSAMPLE_FACTOR = 0.5\n",
    "\n",
    "preprocessing_pipeline_cross = [\n",
    "    lambda x: scale_data(x, min_val, max_val),\n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR),\n",
    "    add_gaussian_noise,\n",
    "    channel_dropout\n",
    "]\n",
    "\n",
    "def create_cross_model(input_shape, num_classes):\n",
    "    # Een input voor de geconcateneerde data\n",
    "    input_layer = Input(shape = input_shape, name = 'concatenated_input')\n",
    "    x = input_layer\n",
    "    \n",
    "    # Lokale features pakken\n",
    "    x = Conv1D(filters = 64, kernel_size = 5, activation = 'relu', padding = 'same')(x)\n",
    "    x = Conv1D(filters = 64, kernel_size = 3, activation = 'relu', padding = 'same')(x)\n",
    "    x = Conv1D(filters = 128, kernel_size = 3, activation = 'relu', padding = 'same')(x)\n",
    "    \n",
    "    # Attention\n",
    "    attention_output = MultiHeadAttention(num_heads = 4, key_dim = 64)(x, x)\n",
    "    x = LayerNormalization()(x + attention_output)\n",
    "    \n",
    "    # Classificatie\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation = 'softmax')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = outputs)\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy']        \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def detailed_evaluation(model, folder, preprocessing, shuffle, batch_size):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Go over each concatenated file, extract the actual label and predict the label\n",
    "    for batch_data, batch_label in create_batches(CROSS_TRAIN_FOLDER, batch_size = batch_size, \n",
    "                                                  preprocessing = preprocessing, \n",
    "                                                  shuffle = shuffle):\n",
    "        y_true.append(batch_label)\n",
    "        y_pred.append(model.predict(batch_data))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "def train_cross_model(model, train_folder: str, epochs: int = 10, batch_size: int = 8, preprocessing: list = None, callbacks=None):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_X, batch_y in create_batches(train_folder, batch_size, preprocessing, shuffle=False):\n",
    "            indices = np.random.permutation(len(batch_X))\n",
    "            model.fit(batch_X[indices], batch_y[indices], verbose = 0, callbacks = callbacks)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 10:15:14.019372: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 10:15:17.418236: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 141356032 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 4), output.shape=(None, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m minval, maxval = learn_minmax(CROSS_TRAIN_FOLDER)\n\u001b[32m      4\u001b[39m cross_model = create_cross_model(input_shape=input_size, num_classes=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cross_trained_model = \u001b[43mtrain_cross_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCROSS_TRAIN_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocessing_pipeline_cross\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mtrain_cross_model\u001b[39m\u001b[34m(model, train_folder, epochs, batch_size, preprocessing, callbacks)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m create_batches(train_folder, batch_size, preprocessing, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     69\u001b[39m         indices = np.random.permutation(\u001b[38;5;28mlen\u001b[39m(batch_X))\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tf_venv/tf_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tf_venv/tf_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:734\u001b[39m, in \u001b[36msparse_categorical_crossentropy\u001b[39m\u001b[34m(target, output, from_logits, axis)\u001b[39m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    729\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `output` must be at least rank 1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    730\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    731\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    732\u001b[39m     )\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target.shape) != \u001b[38;5;28mlen\u001b[39m(output.shape[:-\u001b[32m1\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    735\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    736\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    737\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    738\u001b[39m     )\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target.shape, output.shape[:-\u001b[32m1\u001b[39m]):\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 != e2:\n",
      "\u001b[31mValueError\u001b[39m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 4), output.shape=(None, 4)"
     ]
    }
   ],
   "source": [
    "input_size = determine_input_shape(CROSS_TRAIN_FOLDER, preprocessing_pipeline_cross, min_val)\n",
    "minval, maxval = learn_minmax(CROSS_TRAIN_FOLDER)\n",
    "# Early stopping\n",
    "callback = EarlyStopping(monitor = 'loss', patience = 3)\n",
    "\n",
    "cross_model = create_cross_model(input_shape=input_size, num_classes=4)\n",
    "cross_trained_model = train_cross_model(cross_model, CROSS_TRAIN_FOLDER, epochs=10, batch_size=8, preprocessing=preprocessing_pipeline_cross,\n",
    "                                        callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual training\n",
    "model_cross = create_model(input_shape=input_shape) \n",
    "trained_model_cross = train_model(model_cross, CROSS_TRAIN_FOLDER, epochs = 10, batch_size = 8, \n",
    "                                  preprocessing = preprocessing_pipeline_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test1 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST1_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)\n",
    "print('Test2 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST2_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)\n",
    "print('Test3 folder')\n",
    "detailed_evaluation(trained_model_cross, CROSS_TEST3_FOLDER, batch_size = 8, \n",
    "                    preprocessing = preprocessing_pipeline_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from Mehrkanoon on cross subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne  # MNE-Python toolbox\n",
    "\n",
    "def preprocess_meg(raw_data):\n",
    "    # Minimal preprocessing:\n",
    "    # 1. Normalize the data (order of magnitude is very small)\n",
    "    data = raw_data * 1e5  # Scaling factor empirically found best\n",
    "    \n",
    "    # For AA-CascadeNet and AA-MultiviewNet:\n",
    "    # Create mesh representation (top-down view of human scalp)\n",
    "    # 248 MEG channels mapped to spatial positions\n",
    "    mesh = np.zeros((N, L))  # N rows, L columns based on sensor layout\n",
    "    # Fill mesh with sensor values, others remain 0\n",
    "    \n",
    "    return data, mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, AveragePooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "def create_aa_eegnet(input_shape, num_classes):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1: Temporal convolution with self-attention\n",
    "    x = AttentionAugmentedConv2D(filters=16, kernel_size=(1, 128), \n",
    "                               num_heads=2)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Block 2: Spatial convolution\n",
    "    x = DepthwiseConv2D(kernel_size=(248, 1), depth_multiplier=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AveragePooling2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 3: Separable convolution\n",
    "    x = SeparableConv2D(filters=32, kernel_size=(1, 16))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AveragePooling2D(pool_size=(1, 8))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Global attention before classification\n",
    "    x = Flatten()(x)\n",
    "    x = GlobalAttentionBlock()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    from tensorflow.keras.layers import LSTM, Concatenate\n",
    "\n",
    "def create_aa_cascadenet(input_shape, num_classes):\n",
    "    # Multiple input tensors (W streams)\n",
    "    input_layers = [Input(shape=input_shape) for _ in range(W)]\n",
    "    \n",
    "    processed_streams = []\n",
    "    for inp in input_layers:\n",
    "        # Spatial feature extraction with self-attention\n",
    "        x = AttentionAugmentedConv2D(filters=1, kernel_size=(7,7))(inp)\n",
    "        x = Conv2D(filters=2, kernel_size=(7,7))(x)\n",
    "        x = Conv2D(filters=4, kernel_size=(7,7))(x)\n",
    "        \n",
    "        # Temporal feature extraction with LSTM + global attention\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(125)(x)\n",
    "        x = LSTM(10, return_sequences=True)(x)\n",
    "        x = GlobalAttention()(x)  # Applied between LSTM layers\n",
    "        x = LSTM(10)(x)\n",
    "        \n",
    "        processed_streams.append(x)\n",
    "    \n",
    "    # Combine streams\n",
    "    x = Concatenate()(processed_streams)\n",
    "    x = Dense(125, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=input_layers, outputs=outputs)\n",
    "\n",
    "def create_aa_multiviewnet(spatial_input_shape, temporal_input_shape, num_classes):\n",
    "    # Spatial stream (same as AA-CascadeNet but separate)\n",
    "    spatial_input = Input(shape=spatial_input_shape)\n",
    "    x_spatial = AttentionAugmentedConv2D(filters=1, kernel_size=(7,7))(spatial_input)\n",
    "    x_spatial = Conv2D(filters=2, kernel_size=(7,7))(x_spatial)\n",
    "    x_spatial = Conv2D(filters=4, kernel_size=(7,7))(x_spatial)\n",
    "    x_spatial = Flatten()(x_spatial)\n",
    "    \n",
    "    # Temporal stream\n",
    "    temporal_input = Input(shape=temporal_input_shape)\n",
    "    x_temp = Dense(125)(temporal_input)\n",
    "    x_temp = LSTM(10, return_sequences=True)(x_temp)\n",
    "    x_temp = GlobalAttention()(x_temp)  # Applied to LSTM input sequence\n",
    "    x_temp = LSTM(10)(x_temp)\n",
    "    \n",
    "    # Combine streams\n",
    "    x = Concatenate()([x_spatial, x_temp])\n",
    "    x = Dense(125, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=[spatial_input, temporal_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanisms\n",
    "# Multi-head Self-attention\n",
    "from tensorflow.keras.layers import Layer, Softmax\n",
    "import tensorflow as tf\n",
    "\n",
    "class AttentionAugmentedConv2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, num_heads=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Regular conv path\n",
    "        self.conv = Conv2D(self.filters, self.kernel_size, padding='same')\n",
    "        \n",
    "        # Self-attention path\n",
    "        _, H, W, C = input_shape\n",
    "        self.d_k = C // self.num_heads\n",
    "        self.W_q = Dense(self.d_k)\n",
    "        self.W_k = Dense(self.d_k)\n",
    "        self.W_v = Dense(self.d_k)\n",
    "        self.W_o = Dense(C)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Convolutional path\n",
    "        conv_out = self.conv(inputs)\n",
    "        \n",
    "        # Self-attention path\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        H, W, C = inputs.shape[1:]\n",
    "        \n",
    "        # Reshape for attention\n",
    "        x_flat = tf.reshape(inputs, [batch_size, H*W, C])\n",
    "        \n",
    "        # Compute Q, K, V for each head\n",
    "        heads = []\n",
    "        for _ in range(self.num_heads):\n",
    "            Q = self.W_q(x_flat)\n",
    "            K = self.W_k(x_flat)\n",
    "            V = self.W_v(x_flat)\n",
    "            \n",
    "            # Scaled dot-product attention\n",
    "            attn = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(self.d_k, tf.float32))\n",
    "            attn = Softmax(axis=-1)(attn)\n",
    "            head = tf.matmul(attn, V)\n",
    "            heads.append(head)\n",
    "        \n",
    "        # Combine heads\n",
    "        multi_head = tf.concat(heads, axis=-1)\n",
    "        attn_out = self.W_o(multi_head)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, H, W, C])\n",
    "        \n",
    "        # Combine conv and attention\n",
    "        return tf.concat([conv_out, attn_out], axis=-1)\n",
    "\n",
    "# Global attention (Luong-style)\n",
    "class GlobalAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W_a = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.W_c = self.add_weight(shape=(2*input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: [batch, seq_len, features]\n",
    "        h_t = inputs[:, -1, :]  # Last hidden state\n",
    "        h_s = inputs  # All hidden states\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = tf.matmul(h_t[:, None, :] @ self.W_a, h_s, transpose_b=True)\n",
    "        a_t = Softmax(axis=-1)(scores)\n",
    "        \n",
    "        # Compute context vector\n",
    "        c_t = tf.reduce_sum(a_t * h_s, axis=1)\n",
    "        \n",
    "        # Compute attentional hidden state\n",
    "        h_attn = tf.tanh(tf.matmul(tf.concat([c_t, h_t], axis=-1), self.W_c))\n",
    "        \n",
    "        return h_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def train_model():\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train = load_data(subjects=[1,2,3,4,5,6,7,8,9,10,11,12])  # 12 subjects\n",
    "    X_test, y_test = load_data(subjects=[13,14,15,16,17,18])  # 6 subjects\n",
    "    \n",
    "    # Create model\n",
    "    model = create_aa_eegnet(input_shape=(248, 1425, 1), num_classes=4)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                loss=CategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(X_train, y_train,\n",
    "                      batch_size=16,  # 64 for Cascade/Multiview\n",
    "                      epochs=100,\n",
    "                      validation_split=0.2)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_results[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approach from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Iterator\n",
    "\n",
    "#########################################\n",
    "# Helper Functions (Please customize!)\n",
    "#########################################\n",
    "\n",
    "def extract_data_from_folder(folder: str, shuffle: bool = False) -> Iterator[tuple[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Yields (filename, data) pairs. Assumes that each file is a .npy file.\n",
    "    You might need to change this function to match your file format.\n",
    "    \"\"\"\n",
    "    file_list = os.listdir(folder)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(file_list)\n",
    "    for name in file_list:\n",
    "        path = os.path.join(folder, name)\n",
    "        # Load data – here we assume each file is saved as a .npy file.\n",
    "        data = np.load(path)\n",
    "        yield name, data\n",
    "\n",
    "def generate_label(name: str) -> int:\n",
    "    \"\"\"\n",
    "    Generates a label based on the filename.\n",
    "    For example, if the filename contains the string 'task1' or 'task2'.\n",
    "    Modify this logic for your task.\n",
    "    \"\"\"\n",
    "    if \"task1\" in name:\n",
    "        return 0\n",
    "    elif \"task2\" in name:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def downsample(x: np.ndarray, factor: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Downsamples the data along the time axis.\n",
    "    If factor==1.0, returns original array.\n",
    "    \"\"\"\n",
    "    if factor == 1.0:\n",
    "        return x\n",
    "    new_length = int(x.shape[0] * factor)\n",
    "    # Downsample by simple slicing (you could also use more advanced methods)\n",
    "    return x[::int(1/factor), :]\n",
    "\n",
    "def learn_minmax(folder: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Finds the global minimum and maximum across files in a folder.\n",
    "    \"\"\"\n",
    "    mins, maxs = [], []\n",
    "    for name, data in extract_data_from_folder(folder, shuffle=False):\n",
    "        mins.append(data.min())\n",
    "        maxs.append(data.max())\n",
    "    return min(mins), max(maxs)\n",
    "\n",
    "def determine_input_shape(folder: str, preprocessing: list, min_val: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Determines the input shape for your neural network from one file.\n",
    "    \"\"\"\n",
    "    for name, data in extract_data_from_folder(folder, shuffle=False):\n",
    "        for fn in preprocessing:\n",
    "            data = fn(data)\n",
    "        return data.shape\n",
    "\n",
    "#########################################\n",
    "# Batch Generator Function (given by you)\n",
    "#########################################\n",
    "\n",
    "def create_batches(folder: str, batch_size: int, preprocessing: list = None, shuffle: bool = False) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n",
    "    batch_data, batch_labels = [], []\n",
    "    for i, (name, data) in enumerate(extract_data_from_folder(folder, shuffle)):\n",
    "        if preprocessing:\n",
    "            for fn in preprocessing:\n",
    "                data = fn(data)\n",
    "        batch_data.append(data)\n",
    "        batch_labels.append(generate_label(name))\n",
    "\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            yield np.array(batch_data), np.array(batch_labels)\n",
    "            batch_data, batch_labels = [], []\n",
    "    if batch_data:\n",
    "        yield np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "#########################################\n",
    "# PyTorch Dataset Wrapper\n",
    "#########################################\n",
    "\n",
    "class MEGDataset(Dataset):\n",
    "    def __init__(self, folder: str, preprocessing: list = None, shuffle: bool = False):\n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        for name, data in extract_data_from_folder(folder, shuffle):\n",
    "            if preprocessing:\n",
    "                for fn in preprocessing:\n",
    "                    data = fn(data)\n",
    "            self.data_list.append(data)\n",
    "            self.label_list.append(generate_label(name))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure data is float32 and shape is (time, channels)\n",
    "        data = self.data_list[idx].astype(np.float32)\n",
    "        label = self.label_list[idx]\n",
    "        return data, label\n",
    "\n",
    "#########################################\n",
    "# Model Definition: Shared Backbone + Adapter\n",
    "#########################################\n",
    "\n",
    "class MEGClassifier(nn.Module):\n",
    "    def __init__(self, input_shape: tuple, num_classes: int, adapter_dim: int = 64):\n",
    "        \"\"\"\n",
    "        input_shape: (time, channels)\n",
    "        \"\"\"\n",
    "        super(MEGClassifier, self).__init__()\n",
    "        # The model expects input of shape (batch, time, channels) and we transpose it to (batch, channels, time)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Feature dim is fixed by the last conv output: here it is 64.\n",
    "        feature_dim = 64\n",
    "        # Adapter to allow slight subject-specific adaptations.\n",
    "        self.adapter = nn.Linear(feature_dim, adapter_dim)\n",
    "        self.classifier = nn.Linear(adapter_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch, time, channels) --> convert to (batch, channels, time)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        adapted = self.adapter(features)\n",
    "        output = self.classifier(adapted)\n",
    "        return output\n",
    "\n",
    "#########################################\n",
    "# Training and Evaluation Functions\n",
    "#########################################\n",
    "\n",
    "def train_model(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, device: torch.device) -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        # Move data to device – note that batch_data is a numpy array so we convert it to tensor\n",
    "        batch_data = torch.tensor(batch_data, device=device)\n",
    "        batch_labels = torch.tensor(batch_labels, device=device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_data.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            batch_data = torch.tensor(batch_data, device=device)\n",
    "            batch_labels = torch.tensor(batch_labels, device=device, dtype=torch.long)\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            running_loss += loss.item() * batch_data.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return running_loss / len(dataloader.dataset), accuracy\n",
    "\n",
    "#########################################\n",
    "# Main Routine\n",
    "#########################################\n",
    "\n",
    "def main():\n",
    "    # Configurations and paths:\n",
    "    DATA_PATH = \"./data\"\n",
    "    CROSS_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Cross/train\")\n",
    "    CROSS_TEST1_FOLDER = os.path.join(DATA_PATH, \"Cross/test1\")\n",
    "    # Additional test folders (if needed)\n",
    "    # CROSS_TEST2_FOLDER = os.path.join(DATA_PATH, \"Cross/test2\")\n",
    "    # CROSS_TEST3_FOLDER = os.path.join(DATA_PATH, \"Cross/test3\")\n",
    "    \n",
    "    DOWNSAMPLE_FACTOR_CROSS = 1.0\n",
    "    \n",
    "    # Compute min and max over training data (if needed for normalization)\n",
    "    min_val_cross, max_val_cross = learn_minmax(CROSS_TRAIN_FOLDER)\n",
    "    \n",
    "    preprocessing_pipeline_cross = [\n",
    "        lambda x: downsample(x, DOWNSAMPLE_FACTOR_CROSS)\n",
    "        # Add other preprocessing functions here (e.g., normalization)\n",
    "    ]\n",
    "    \n",
    "    input_shape = determine_input_shape(CROSS_TRAIN_FOLDER, preprocessing_pipeline_cross, min_val_cross)\n",
    "    print(f\"Determined input shape: {input_shape}\")\n",
    "    \n",
    "    # Set hyperparameters:\n",
    "    num_classes = 2       # Adjust based on your classification task\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Prepare device and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MEGClassifier(input_shape=input_shape, num_classes=num_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create PyTorch datasets and dataloaders:\n",
    "    train_dataset = MEGDataset(CROSS_TRAIN_FOLDER, preprocessing_pipeline_cross, shuffle=True)\n",
    "    test_dataset = MEGDataset(CROSS_TEST1_FOLDER, preprocessing_pipeline_cross, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Define optimizer and loss criterion:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop:\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the trained model:\n",
    "    torch.save(model.state_dict(), \"meg_classifier.pth\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
