{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths and Configuration\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/train/\"))\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/test/\"))\n",
    "DOWNSAMPLE_FACTOR = 1.0\n",
    "\n",
    "# Function definitions\n",
    "def get_dataset_name(filename_with_dir):\n",
    "    \"\"\"Given a a full file path returns the name of the dataset\"\"\"\n",
    "    filename_without_dir = os.path.basename(filename_with_dir)\n",
    "    temp = filename_without_dir.split('.')[:-1]\n",
    "    dataset_name = ''.join(temp)\n",
    "    temp = dataset_name.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "def extract_data_from_folder_by_file(folder_path, shuffle=False):\n",
    "    \"\"\"Given a folder path containing h5 files, this function returns the matrices and \n",
    "    optionally shuffles the files.\"\"\"\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "    for file_name in files:\n",
    "        \n",
    "        filename_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            matrix = f.get(dataset_name)[()]\n",
    "            yield dataset_name, matrix\n",
    "\n",
    "# Given a folder \n",
    "def learn_minmax_from_all_files(folder_path: str) -> tuple:\n",
    "    \"\"\"Given a folder path containing h5 files, this functions returns the minimum and \n",
    "    maximum values across all files in the folder.\"\"\"\n",
    "    # Placeholders\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "\n",
    "    for (name, data) in extract_data_from_folder_by_file(folder_path):\n",
    "        data = data.T\n",
    "        if min_val is None:\n",
    "            min_val = np.min(data, axis=0)\n",
    "            max_val = np.max(data, axis=0)\n",
    "        else:\n",
    "            # Update min_val and max_val\n",
    "            min_val = np.minimum(min_val, np.min(data, axis=0))\n",
    "            max_val = np.maximum(max_val, np.max(data, axis=0))\n",
    "        \n",
    "    return min_val, max_val\n",
    "\n",
    "def generate_label(file_name:str) -> np.ndarray:\n",
    "    \"\"\"Returns a vector corresponding to the one-hot encoded label for task type.\"\"\"\n",
    "    # Return a one-hot encoded label based on the file name, there are4 classes\n",
    "    # 0: rest\n",
    "    if \"rest_\" in file_name:\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    # 1: task_motor\n",
    "    elif \"task_motor_\" in file_name:\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    # 2: task_story_math\n",
    "    elif \"task_story_math_\" in file_name:\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    # 3: task_working_memory\n",
    "    elif \"task_working_memory_\" in file_name:\n",
    "        return np.array([0, 0, 0, 1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file name: {file_name}\")\n",
    "\n",
    "def create_batches(folder, number_of_files_per_batch: int, preprocessing_pipeline: list = None, shuffle_files=False) -> Iterator[tuple]:\n",
    "    \"\"\"Given a folder containg '.h5' files and the number of \"\"\"\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for n, (name, data) in enumerate(extract_data_from_folder_by_file(folder, shuffle=shuffle_files)):\n",
    "        data = data.T\n",
    "        if preprocessing_pipeline:\n",
    "            for preprocessing_step in preprocessing_pipeline:\n",
    "                data = preprocessing_step(data)\n",
    "        # Add the preprocessed data to the batch\n",
    "        batch_data.append(data)\n",
    "\n",
    "        # Generate the label matrix of the length of the data for the current file\n",
    "        label_vector = generate_label(name)\n",
    "        batch_labels.append(label_vector)\n",
    "\n",
    "\n",
    "        # Check if we have reached the desired batch size\n",
    "        if (n + 1) % number_of_files_per_batch == 0:\n",
    "            # Stack along the first axis (like a batch dimension)\n",
    "            yield (batch_data, batch_labels)\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "\n",
    "    # Optional: yield the remainder if not divisible\n",
    "    if batch_data:\n",
    "        yield (batch_data, batch_labels)\n",
    "\n",
    "def evaluate_scores(model, folder_to_evaluate):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch_X_list, batch_y_list in create_batches(folder=folder_to_evaluate, number_of_files_per_batch=8, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=False):\n",
    "        # Convert the list of arrays to a 3D numpy array\n",
    "        data = np.array(batch_X_list)\n",
    "        labels = np.array(batch_y_list)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(data, labels)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "# Helper functions\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given a minimal value and maximal value normalises the provided matrix.\"\"\"\n",
    "    # Scale the data to the range [0, 1]\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def downsample(data: np.array, factor: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Downsample time series data by uniformly selecting samples at fixed intervals\n",
    "    to keep the temporal order intact.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Input time series data (1D or 2D with time dimension as first axis)\n",
    "        factor (float): Downsampling factor (e.g., 0.5 means keep half the samples)\n",
    "\n",
    "    Returns:\n",
    "        np.array: Downsampled data with timesteps reduced by the factor\n",
    "    \"\"\"\n",
    "    num_samples = int(len(data) * factor)\n",
    "    # Calculate the stride to evenly pick samples\n",
    "    stride = len(data) / num_samples\n",
    "    # Use np.floor to avoid going out of bounds and convert to int indices\n",
    "    indices = (np.floor(np.arange(num_samples) * stride)).astype(int)\n",
    "    downsampled_data = data[indices]\n",
    "    return downsampled_data\n",
    "\n",
    "min_val, max_val = learn_minmax_from_all_files(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Min values: {min_val.shape}, Max values: {max_val.shape}\")\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val), \n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR)\n",
    "]\n",
    "\n",
    "# # Example usage\n",
    "# for data_batch, labels_batch in create_batches(folder=INTRA_TRAIN_FOLDER, number_of_files_per_batch=8, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=False):\n",
    "    \n",
    "#     for data, label in zip(data_batch, labels_batch):\n",
    "#         print(f\"Data shape: {data.shape}, Label: {label}\")\n",
    "\n",
    "# Actual code\n",
    "def create_model() -> Sequential:\n",
    "    lstm_classifier = Sequential([\n",
    "        LSTM(64, return_sequences=False, input_shape=(3562, 248)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    lstm_classifier.compile(\n",
    "        loss=CategoricalCrossentropy(),  # works directly with one-hot encoded labels\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "\n",
    "def train_model(model, epochs=10, batch_size=8, verbose=1):\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "        for batch, (batch_X_list, batch_y_list) in enumerate(create_batches(folder=INTRA_TRAIN_FOLDER, number_of_files_per_batch=batch_size, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=True)):\n",
    "            # Convert the list of arrays to a 3D numpy array\n",
    "            data = np.array(batch_X_list)\n",
    "            labels = np.array(batch_y_list)\n",
    "\n",
    "            # Shuffle the data and labels together\n",
    "            indices = np.arange(data.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            data = data[indices]\n",
    "            labels = labels[indices]\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(data, labels)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(data, labels)\n",
    "            if verbose:\n",
    "                print(f\"Batch: {batch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_classifier = create_model()\n",
    "trained_lstm_classifier = train_model(lstm_classifier, epochs=10, batch_size=8, verbose=1)\n",
    "\n",
    "loss, accuracy = evaluate_scores(trained_lstm_classifier, INTRA_TRAIN_FOLDER)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "loss, accuracy = evaluate_scores(trained_lstm_classifier, INTRA_TEST_FOLDER)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned min/max shapes: (248,), (248,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daan/venv/tf_venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Training Loss: 0.0899, Accuracy: 1.0000\n",
      "Test Loss: 0.0797, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, \"Intra/train\")\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, \"Intra/test\")\n",
    "DOWNSAMPLE_FACTOR = 1.0\n",
    "\n",
    "# --- File and Data Handling ---\n",
    "def get_dataset_name(filepath: str) -> str:\n",
    "    filename = os.path.basename(filepath)\n",
    "    return \"_\".join(filename.split('.')[:-1][0].split('_')[:-1])\n",
    "\n",
    "def extract_data_from_folder(folder_path: str, shuffle: bool = False) -> Iterator[tuple[str, np.ndarray]]:\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "    for file_name in files:\n",
    "        with h5py.File(os.path.join(folder_path, file_name), 'r') as f:\n",
    "            dataset_name = get_dataset_name(file_name)\n",
    "            yield dataset_name, f[dataset_name][()].T  # transpose once here\n",
    "\n",
    "def learn_minmax(folder_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    min_val, max_val = None, None\n",
    "    for _, data in extract_data_from_folder(folder_path):\n",
    "        min_data, max_data = np.min(data, axis=0), np.max(data, axis=0)\n",
    "        min_val = min_data if min_val is None else np.minimum(min_val, min_data)\n",
    "        max_val = max_data if max_val is None else np.maximum(max_val, max_data)\n",
    "    return min_val, max_val\n",
    "\n",
    "def generate_label(name: str) -> np.ndarray:\n",
    "    classes = [\"rest\", \"task_motor\", \"task_story_math\", \"task_working_memory\"]\n",
    "    for i, cls in enumerate(classes):\n",
    "        if cls + \"_\" in name:\n",
    "            label = np.zeros(len(classes))\n",
    "            label[i] = 1\n",
    "            return label\n",
    "    raise ValueError(f\"Unknown file name: {name}\")\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def downsample(data: np.ndarray, factor: float) -> np.ndarray:\n",
    "    num_samples = int(len(data) * factor)\n",
    "    indices = np.floor(np.arange(num_samples) * (len(data) / num_samples)).astype(int)\n",
    "    return data[indices]\n",
    "\n",
    "def add_gaussian_noise(data: np.ndarray, stddev: float = 0.01) -> np.ndarray:\n",
    "    noise = np.random.normal(0, stddev, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def time_shift(data: np.ndarray, shift_max: int = 10) -> np.ndarray:\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    return np.roll(data, shift, axis=0)\n",
    "\n",
    "def channel_dropout(data: np.ndarray, dropout_rate: float = 0.1) -> np.ndarray:\n",
    "    num_channels = data.shape[1]\n",
    "    mask = np.random.rand(num_channels) > dropout_rate\n",
    "    return data * mask[np.newaxis, :]\n",
    "\n",
    "def random_scaling(data: np.ndarray, scale_range=(0.9, 1.1)) -> np.ndarray:\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    return data * scale\n",
    "\n",
    "def bandpass_filter(data: np.ndarray, lowcut=1.0, highcut=40.0, fs=2034.0, order=5) -> np.ndarray:\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "def zscore_per_channel(data: np.ndarray) -> np.ndarray:\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    std[std == 0] = 1  # Prevent division by zero\n",
    "    return (data - mean) / std\n",
    "\n",
    "def baseline_correction(data: np.ndarray, baseline_duration=100) -> np.ndarray:\n",
    "    if data.shape[0] < baseline_duration:\n",
    "        return data\n",
    "    baseline_mean = np.mean(data[:baseline_duration], axis=0)\n",
    "    return data - baseline_mean\n",
    "\n",
    "# --- Batching ---\n",
    "def create_batches(folder: str, batch_size: int, preprocessing: list = None, shuffle: bool = False) -> Iterator[tuple[np.ndarray, np.ndarray]]:\n",
    "    batch_data, batch_labels = [], []\n",
    "    for i, (name, data) in enumerate(extract_data_from_folder(folder, shuffle)):\n",
    "        if preprocessing:\n",
    "            for fn in preprocessing:\n",
    "                data = fn(data)\n",
    "        batch_data.append(data)\n",
    "        batch_labels.append(generate_label(name))\n",
    "\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            yield np.array(batch_data), np.array(batch_labels)\n",
    "            batch_data, batch_labels = [], []\n",
    "    if batch_data:\n",
    "        yield np.array(batch_data), np.array(batch_labels)\n",
    "\n",
    "# --- Model ---\n",
    "def create_model(input_shape=(3562, 248)) -> Sequential:\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=False, input_shape=input_shape),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_folder: str, epochs: int = 10, batch_size: int = 8, preprocessing: list = None):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_X, batch_y in create_batches(train_folder, batch_size, preprocessing, shuffle=True):\n",
    "            indices = np.random.permutation(len(batch_X))\n",
    "            model.fit(batch_X[indices], batch_y[indices], verbose=0)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, folder: str, batch_size: int, preprocessing: list) -> tuple[float, float]:\n",
    "    losses, accuracies = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        loss, acc = model.evaluate(batch_X, batch_y, verbose=0)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "# --- Run Training and Evaluation ---\n",
    "min_val, max_val = learn_minmax(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Learned min/max shapes: {min_val.shape}, {max_val.shape}\")\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val),\n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR),\n",
    "    add_gaussian_noise, # These are all added for more general performance ()\n",
    "    # time_shift,\n",
    "    channel_dropout\n",
    "    # random_scaling\n",
    "]\n",
    "\n",
    "model = create_model()\n",
    "trained_model = train_model(model, INTRA_TRAIN_FOLDER, epochs=10, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "\n",
    "train_loss, train_acc = evaluate_model(trained_model, INTRA_TRAIN_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_model(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def detailed_evaluation(model, folder, batch_size, preprocessing):\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_X, batch_y in create_batches(folder, batch_size, preprocessing, shuffle=False):\n",
    "        preds = model.predict(batch_X)\n",
    "        y_true.extend(np.argmax(batch_y, axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "detailed_evaluation(trained_model, INTRA_TEST_FOLDER, batch_size=8, preprocessing=preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory efficient but very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers, mixed_precision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from meg_preprocessing import preprocess_meg\n",
    "from downsample import downsample\n",
    "from read_data import get_dataset_name_train, load_split_files, VALID_TASK_TYPES\n",
    "\n",
    "# Enable mixed precision training\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "class MEGDataGenerator:\n",
    "    def __init__(self, file_paths, task_types, label_encoder, batch_size=16, downsample_factor=0.5):\n",
    "        self.file_paths = file_paths\n",
    "        self.task_types = task_types\n",
    "        self.label_encoder = label_encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.target_length = self._determine_target_length()\n",
    "        \n",
    "    def _determine_target_length(self):\n",
    "        \"\"\"Sample a few files to determine max sequence length\"\"\"\n",
    "        max_length = 0\n",
    "        sample_files = min(10, len(self.file_paths))\n",
    "        \n",
    "        for i in range(sample_files):\n",
    "            data_dict = load_split_files(self.file_paths[i])\n",
    "            for data in data_dict.values():\n",
    "                processed = preprocess_meg(data)\n",
    "                if self.downsample_factor < 1.0:\n",
    "                    processed = downsample(processed, self.downsample_factor)\n",
    "                if processed.shape[1] > max_length:\n",
    "                    max_length = processed.shape[1]\n",
    "        return max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for i in range(0, len(self.file_paths), self.batch_size):\n",
    "                batch_paths = self.file_paths[i:i+self.batch_size]\n",
    "                batch_types = self.task_types[i:i+self.batch_size]\n",
    "                batch_data = []\n",
    "                batch_labels = []\n",
    "                \n",
    "                for path, task_type in zip(batch_paths, batch_types):\n",
    "                    data_dict = load_split_files(path)\n",
    "                    for data in data_dict.values():\n",
    "                        processed = preprocess_meg(data)\n",
    "                        if self.downsample_factor < 1.0:\n",
    "                            processed = downsample(processed, self.downsample_factor)\n",
    "                        batch_data.append(processed)\n",
    "                        batch_labels.append(task_type)\n",
    "                \n",
    "                # Efficient padding\n",
    "                X_batch = np.zeros((len(batch_data), batch_data[0].shape[0], self.target_length))\n",
    "                for j, d in enumerate(batch_data):\n",
    "                    length = min(d.shape[1], self.target_length)\n",
    "                    X_batch[j, :, :length] = d[:, :length]\n",
    "                \n",
    "                X_batch = np.expand_dims(X_batch, axis=-1).astype('float32')\n",
    "                y_batch = self.label_encoder.transform(batch_labels)\n",
    "                \n",
    "                yield X_batch, y_batch\n",
    "\n",
    "def create_efficient_model(input_shape, num_classes):\n",
    "    \"\"\"Memory-optimized CNN-LSTM model with mixed precision\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First conv block with reduced filters\n",
    "        layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=4),  # Increased pooling\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second conv block with reduced filters\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Single LSTM layer with reduced units\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Smaller dense layers\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_optimized(typeData='Cross', batch_size=16, downsample_factor=0.5):\n",
    "    # Load file paths and labels without loading actual data\n",
    "    print(\"Collecting training file paths...\")\n",
    "    train_files = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for task_type in VALID_TASK_TYPES:\n",
    "        files = get_dataset_name_train(file_name=\"all\", taskType=task_type, typeData=typeData)\n",
    "        if files:\n",
    "            train_files.extend(files)\n",
    "            train_labels.extend([task_type] * len(files))\n",
    "    \n",
    "    print(\"Collecting test file paths...\")\n",
    "    test_files = find_test_files(typeData)\n",
    "    test_labels = []\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        for valid_type in VALID_TASK_TYPES:\n",
    "            if file_name.startswith(valid_type):\n",
    "                test_labels.append(valid_type)\n",
    "                break\n",
    "    \n",
    "    # Initialize label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_labels + test_labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen = MEGDataGenerator(train_files, train_labels, label_encoder, \n",
    "                               batch_size=batch_size, downsample_factor=downsample_factor)\n",
    "    test_gen = MEGDataGenerator(test_files, test_labels, label_encoder,\n",
    "                               batch_size=batch_size, downsample_factor=downsample_factor)\n",
    "    \n",
    "    # Determine input shape from a sample batch\n",
    "    sample_X, _ = next(train_gen.__iter__())\n",
    "    input_shape = sample_X.shape[1:]\n",
    "    \n",
    "    print(\"Creating optimized model...\")\n",
    "    model = create_efficient_model(input_shape, num_classes)\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(train_gen.__iter__(),\n",
    "                       steps_per_epoch=len(train_gen),\n",
    "                       epochs=20,\n",
    "                       validation_data=test_gen.__iter__(),\n",
    "                       validation_steps=len(test_gen),\n",
    "                       verbose=1)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating model...\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for _ in range(len(test_gen)):\n",
    "        X_batch, y_batch = next(test_gen.__iter__())\n",
    "        y_true.extend(y_batch)\n",
    "        y_pred.extend(np.argmax(model.predict(X_batch, verbose=0), axis=1))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                              target_names=label_encoder.classes_))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Helper functions (keep these from your original code)\n",
    "def find_test_files(typeData):\n",
    "    \"\"\"Find all test files in the test directories\"\"\"\n",
    "    test_files = []\n",
    "    base_dir = os.path.join(\"..\", \"extracted_zip_in_here\", \"Final Project data\", typeData)\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if 'test' in dir_name.lower():\n",
    "            test_dir = os.path.join(base_dir, dir_name)\n",
    "            if os.path.isdir(test_dir):\n",
    "                for file_name in os.listdir(test_dir):\n",
    "                    if file_name.endswith('.h5'):\n",
    "                        test_files.append(os.path.join(test_dir, file_name))\n",
    "    \n",
    "    return test_files    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_and_evaluate_optimized(\n",
    "        typeData='Cross',\n",
    "        batch_size=16,\n",
    "        downsample_factor=0.5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
