{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 13:11:21.472541: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-04 13:11:21.539884: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-04 13:11:21.591527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749035481.642529   57031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749035481.660780   57031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749035481.756885   57031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749035481.756911   57031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749035481.756914   57031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749035481.756917   57031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-04 13:11:21.773025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths and Configuration\n",
    "DATA_PATH = os.path.abspath(\"../extracted_zip_in_here/Final Project data/\")\n",
    "INTRA_TRAIN_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/train/\"))\n",
    "INTRA_TEST_FOLDER = os.path.join(DATA_PATH, os.path.relpath(\"./Intra/test/\"))\n",
    "DOWNSAMPLE_FACTOR = 0.5\n",
    "# FEATURES = 248\n",
    "# TIMESTEPS = 3562\n",
    "# CLASSES = 4\n",
    "\n",
    "# Function definitions\n",
    "def get_dataset_name(filename_with_dir):\n",
    "    \"\"\"Given a a full file path returns the name of the dataset\"\"\"\n",
    "    filename_without_dir = os.path.basename(filename_with_dir)\n",
    "    temp = filename_without_dir.split('.')[:-1]\n",
    "    dataset_name = ''.join(temp)\n",
    "    temp = dataset_name.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "def extract_data_from_folder_by_file(folder_path, shuffle=False):\n",
    "    \"\"\"Given a folder path containing h5 files, this function returns the matrices and \n",
    "    optionally shuffles the files.\"\"\"\n",
    "    files = os.listdir(folder_path)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "    for file_name in files:\n",
    "        \n",
    "        filename_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with h5py.File(filename_path, 'r') as f:\n",
    "            dataset_name = get_dataset_name(filename_path)\n",
    "            matrix = f.get(dataset_name)[()]\n",
    "            yield dataset_name, matrix\n",
    "\n",
    "# Given a folder \n",
    "def learn_minmax_from_all_files(folder_path: str) -> tuple:\n",
    "    \"\"\"Given a folder path containing h5 files, this functions returns the minimum and \n",
    "    maximum values across all files in the folder.\"\"\"\n",
    "    # Placeholders\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "\n",
    "    for (name, data) in extract_data_from_folder_by_file(folder_path):\n",
    "        data = data.T\n",
    "        if min_val is None:\n",
    "            min_val = np.min(data, axis=0)\n",
    "            max_val = np.max(data, axis=0)\n",
    "        else:\n",
    "            # Update min_val and max_val\n",
    "            min_val = np.minimum(min_val, np.min(data, axis=0))\n",
    "            max_val = np.maximum(max_val, np.max(data, axis=0))\n",
    "        \n",
    "    return min_val, max_val\n",
    "\n",
    "def generate_label(file_name:str) -> np.ndarray:\n",
    "    # Return a one-hot encoded label based on the file name, there are4 classes\n",
    "    # 0: rest\n",
    "    if \"rest_\" in file_name:\n",
    "        return np.array([1, 0, 0, 0])\n",
    "    # 1: task_motor\n",
    "    elif \"task_motor_\" in file_name:\n",
    "        return np.array([0, 1, 0, 0])\n",
    "    # 2: task_story_math\n",
    "    elif \"task_story_math_\" in file_name:\n",
    "        return np.array([0, 0, 1, 0])\n",
    "    # 3: task_working_memory\n",
    "    elif \"task_working_memory_\" in file_name:\n",
    "        return np.array([0, 0, 0, 1])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file name: {file_name}\")\n",
    "\n",
    "def create_batches(folder, number_of_files_per_batch: int, preprocessing_pipeline: list = None, shuffle_files=False) -> Iterator[tuple]:\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for n, (name, data) in enumerate(extract_data_from_folder_by_file(folder, shuffle=shuffle_files)):\n",
    "        data = data.T\n",
    "        if preprocessing_pipeline:\n",
    "            for preprocessing_step in preprocessing_pipeline:\n",
    "                data = preprocessing_step(data)\n",
    "        # Add the preprocessed data to the batch\n",
    "        batch_data.append(data)\n",
    "\n",
    "        # Generate the label matrix of the length of the data for the current file\n",
    "        label_vector = generate_label(name)\n",
    "        batch_labels.append(label_vector)\n",
    "\n",
    "\n",
    "        # Check if we have reached the desired batch size\n",
    "        if (n + 1) % number_of_files_per_batch == 0:\n",
    "            # Stack along the first axis (like a batch dimension)\n",
    "            yield (batch_data, batch_labels)\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "\n",
    "    # Optional: yield the remainder if not divisible\n",
    "    if batch_data:\n",
    "        yield (batch_data, batch_labels)\n",
    "\n",
    "def evaluate_scores(model, folder_to_evaluate):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch_X_list, batch_y_list in create_batches(folder=folder_to_evaluate, number_of_files_per_batch=8, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=False):\n",
    "        # Convert the list of arrays to a 3D numpy array\n",
    "        data = np.array(batch_X_list)\n",
    "        labels = np.array(batch_y_list)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(data, labels)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "# Helper functions\n",
    "def scale_data(data: np.ndarray, min_val: np.ndarray, max_val: np.ndarray) -> np.ndarray:\n",
    "    # Scale the data to the range [0, 1]\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "def downsample(data: np.array, factor: float) -> np.array:\n",
    "    \"\"\"\n",
    "    Downsample time series data by uniformly selecting samples at fixed intervals\n",
    "    to keep the temporal order intact.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Input time series data (1D or 2D with time dimension as first axis)\n",
    "        factor (float): Downsampling factor (e.g., 0.5 means keep half the samples)\n",
    "\n",
    "    Returns:\n",
    "        np.array: Downsampled data with timesteps reduced by the factor\n",
    "    \"\"\"\n",
    "    num_samples = int(len(data) * factor)\n",
    "    # Calculate the stride to evenly pick samples\n",
    "    stride = len(data) / num_samples\n",
    "    # Use np.floor to avoid going out of bounds and convert to int indices\n",
    "    indices = (np.floor(np.arange(num_samples) * stride)).astype(int)\n",
    "    downsampled_data = data[indices]\n",
    "    return downsampled_data\n",
    "\n",
    "min_val, max_val = learn_minmax_from_all_files(INTRA_TRAIN_FOLDER)\n",
    "print(f\"Min values: {min_val.shape}, Max values: {max_val.shape}\")\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    lambda x: scale_data(x, min_val, max_val), \n",
    "    lambda x: downsample(x, DOWNSAMPLE_FACTOR)\n",
    "]\n",
    "\n",
    "# # Example usage\n",
    "# for data_batch, labels_batch in create_batches(folder=INTRA_TRAIN_FOLDER, number_of_files_per_batch=8, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=False):\n",
    "    \n",
    "#     for data, label in zip(data_batch, labels_batch):\n",
    "#         print(f\"Data shape: {data.shape}, Label: {label}\")\n",
    "\n",
    "# Actual code\n",
    "def create_model() -> Sequential:\n",
    "    lstm_classifier = Sequential([\n",
    "        LSTM(64, return_sequences=False, input_shape=(3562, 248)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    lstm_classifier.compile(\n",
    "        loss=CategoricalCrossentropy(),  # works directly with one-hot encoded labels\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "\n",
    "def train_model(model, epochs=10, batch_size=8, verbose=1):\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "        for batch, (batch_X_list, batch_y_list) in enumerate(create_batches(folder=INTRA_TRAIN_FOLDER, number_of_files_per_batch=batch_size, preprocessing_pipeline=preprocessing_pipeline, shuffle_files=True)):\n",
    "            # Convert the list of arrays to a 3D numpy array\n",
    "            data = np.array(batch_X_list)\n",
    "            labels = np.array(batch_y_list)\n",
    "\n",
    "            # Shuffle the data and labels together\n",
    "            indices = np.arange(data.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            data = data[indices]\n",
    "            labels = labels[indices]\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(data, labels, callbacks = [PlotLossesKeras()])\n",
    "            \n",
    "            # Evaluate the model\n",
    "            loss, accuracy = model.evaluate(data, labels)\n",
    "            if verbose:\n",
    "                print(f\"Batch: {batch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_classifier = create_model()\n",
    "trained_lstm_classifier = train_model(lstm_classifier, epochs=10, batch_size=8, verbose=1)\n",
    "\n",
    "loss, accuracy = evaluate_scores(trained_lstm_classifier, INTRA_TRAIN_FOLDER)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "loss, accuracy = evaluate_scores(trained_lstm_classifier, INTRA_TEST_FOLDER)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory efficient but very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers, mixed_precision\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from meg_preprocessing import preprocess_meg\n",
    "from downsample import downsample\n",
    "from read_data import get_dataset_name_train, load_split_files, VALID_TASK_TYPES\n",
    "\n",
    "# Enable mixed precision training\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "class MEGDataGenerator:\n",
    "    def __init__(self, file_paths, task_types, label_encoder, batch_size=16, downsample_factor=0.5):\n",
    "        self.file_paths = file_paths\n",
    "        self.task_types = task_types\n",
    "        self.label_encoder = label_encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.target_length = self._determine_target_length()\n",
    "        \n",
    "    def _determine_target_length(self):\n",
    "        \"\"\"Sample a few files to determine max sequence length\"\"\"\n",
    "        max_length = 0\n",
    "        sample_files = min(10, len(self.file_paths))\n",
    "        \n",
    "        for i in range(sample_files):\n",
    "            data_dict = load_split_files(self.file_paths[i])\n",
    "            for data in data_dict.values():\n",
    "                processed = preprocess_meg(data)\n",
    "                if self.downsample_factor < 1.0:\n",
    "                    processed = downsample(processed, self.downsample_factor)\n",
    "                if processed.shape[1] > max_length:\n",
    "                    max_length = processed.shape[1]\n",
    "        return max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for i in range(0, len(self.file_paths), self.batch_size):\n",
    "                batch_paths = self.file_paths[i:i+self.batch_size]\n",
    "                batch_types = self.task_types[i:i+self.batch_size]\n",
    "                batch_data = []\n",
    "                batch_labels = []\n",
    "                \n",
    "                for path, task_type in zip(batch_paths, batch_types):\n",
    "                    data_dict = load_split_files(path)\n",
    "                    for data in data_dict.values():\n",
    "                        processed = preprocess_meg(data)\n",
    "                        if self.downsample_factor < 1.0:\n",
    "                            processed = downsample(processed, self.downsample_factor)\n",
    "                        batch_data.append(processed)\n",
    "                        batch_labels.append(task_type)\n",
    "                \n",
    "                # Efficient padding\n",
    "                X_batch = np.zeros((len(batch_data), batch_data[0].shape[0], self.target_length))\n",
    "                for j, d in enumerate(batch_data):\n",
    "                    length = min(d.shape[1], self.target_length)\n",
    "                    X_batch[j, :, :length] = d[:, :length]\n",
    "                \n",
    "                X_batch = np.expand_dims(X_batch, axis=-1).astype('float32')\n",
    "                y_batch = self.label_encoder.transform(batch_labels)\n",
    "                \n",
    "                yield X_batch, y_batch\n",
    "\n",
    "def create_efficient_model(input_shape, num_classes):\n",
    "    \"\"\"Memory-optimized CNN-LSTM model with mixed precision\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First conv block with reduced filters\n",
    "        layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=4),  # Increased pooling\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second conv block with reduced filters\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Single LSTM layer with reduced units\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Smaller dense layers\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_optimized(typeData='Cross', batch_size=16, downsample_factor=0.5):\n",
    "    # Load file paths and labels without loading actual data\n",
    "    print(\"Collecting training file paths...\")\n",
    "    train_files = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for task_type in VALID_TASK_TYPES:\n",
    "        files = get_dataset_name_train(file_name=\"all\", taskType=task_type, typeData=typeData)\n",
    "        if files:\n",
    "            train_files.extend(files)\n",
    "            train_labels.extend([task_type] * len(files))\n",
    "    \n",
    "    print(\"Collecting test file paths...\")\n",
    "    test_files = find_test_files(typeData)\n",
    "    test_labels = []\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        for valid_type in VALID_TASK_TYPES:\n",
    "            if file_name.startswith(valid_type):\n",
    "                test_labels.append(valid_type)\n",
    "                break\n",
    "    \n",
    "    # Initialize label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_labels + test_labels)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Create data generators\n",
    "    train_gen = MEGDataGenerator(train_files, train_labels, label_encoder, \n",
    "                               batch_size=batch_size, downsample_factor=downsample_factor)\n",
    "    test_gen = MEGDataGenerator(test_files, test_labels, label_encoder,\n",
    "                               batch_size=batch_size, downsample_factor=downsample_factor)\n",
    "    \n",
    "    # Determine input shape from a sample batch\n",
    "    sample_X, _ = next(train_gen.__iter__())\n",
    "    input_shape = sample_X.shape[1:]\n",
    "    \n",
    "    print(\"Creating optimized model...\")\n",
    "    model = create_efficient_model(input_shape, num_classes)\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(train_gen.__iter__(),\n",
    "                       steps_per_epoch=len(train_gen),\n",
    "                       epochs=20,\n",
    "                       validation_data=test_gen.__iter__(),\n",
    "                       validation_steps=len(test_gen),\n",
    "                       verbose=1)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating model...\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for _ in range(len(test_gen)):\n",
    "        X_batch, y_batch = next(test_gen.__iter__())\n",
    "        y_true.extend(y_batch)\n",
    "        y_pred.extend(np.argmax(model.predict(X_batch, verbose=0), axis=1))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                              target_names=label_encoder.classes_))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Helper functions (keep these from your original code)\n",
    "def find_test_files(typeData):\n",
    "    \"\"\"Find all test files in the test directories\"\"\"\n",
    "    test_files = []\n",
    "    base_dir = os.path.join(\"..\", \"extracted_zip_in_here\", \"Final Project data\", typeData)\n",
    "    \n",
    "    for dir_name in os.listdir(base_dir):\n",
    "        if 'test' in dir_name.lower():\n",
    "            test_dir = os.path.join(base_dir, dir_name)\n",
    "            if os.path.isdir(test_dir):\n",
    "                for file_name in os.listdir(test_dir):\n",
    "                    if file_name.endswith('.h5'):\n",
    "                        test_files.append(os.path.join(test_dir, file_name))\n",
    "    \n",
    "    return test_files    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_and_evaluate_optimized(\n",
    "        typeData='Cross',\n",
    "        batch_size=16,\n",
    "        downsample_factor=0.5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
