\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{float}
\usepackage{placeins}
\usepackage{svg}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em\textsc{i}\kern-.025em b\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TODO}

\author{\IEEEauthorblockN{Dani\"el Jochems}
\IEEEauthorblockN{David Huizinga}
\IEEEauthorblockN{Niek Grimbergen}
\IEEEauthorblockN{Daan van Dam}}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}


\section{Methods}

\subsection{Model Selection}

Next, we determine what model architecture is best suited for the MEG classification task.
 In making this choice, we have two main considerations.


 First of all, MEG data is created by recording the magnetic fields generated by neural electrical electricity in the brain.
 This is achieved using a number of sensors attached to different parts of the scalp. Each individual sensor measures activity
 in a specific area of the brain. Therefore, we should expect the MEG data to have a strong spatial structure. We want to select
 a model that can exploit this structure. Because of this, Convolutional Neural Networks (CNNs) are well-suited, as these are 
 specifically designed to capture spatial patterns. 


 Secondly, the data has a sequential nature: each training example consists of a sequence that develops over time. We need to choose a model 
 that has the ability to capture temporal dependencies. To this end, we can use Long Short-Term Memory networks (LSTMs), or we could choose a 
 transformer model. However, since we are dealing with a small dataset, transformers are not suitable as they are too data hungry.
 

 In summary, we experiment using two different architectures: a CNN, which we selected to capture spatial structure, and an LSTM, which can model 
 temporal dependencies.

\subsection{Data Engineering}

\subsubsection{Normalization}
To normalize the data, we applied min-max scaling, bringing each feature to the $[0,1]$ range based on the global minimum and maximum values computed from the entire training set.
 This approach preserves the relative structure of the data while ensuring all features operate on a comparable scale, especially with sensor-based time-series inputs.
The normalization is implemented in the scale data function and applied uniformly to both training and test sets, using the training statistics only. We do this to ensure consistency across the entire pipeline.

\subsubsection{Downsampling}
We applied uniform downsampling to the data to reduce temporal resolution and computational load. 
This is done by selecting a fixed proportion of timesteps in evenly spaced intervals accross the full sequence, in our case we chose this to be 10 percent \texttt{[motivatie voor 10 procent]}.
 With this implementation we compute the stride based on the downsampeling factor and select indices accordingly, ensuring consistent coverage of the original signal. 

\subsection{Model Description}


\subsection{Hyperparameter Tuning}

\section{Results}

\section{Discussion}
An attempt was made to do some specific MEG preprocessing like namely, a fixed bandpass filter, prepare
channel z-scoring, and baseline correction. The inspiration for these came from the 
\href{https://mne.tools/stable/index.html}{MNE tools} package and the corresponding paper by Gramfort et al 
\cite{gramfort2013meg}. This applied preprocessing before model training severaly impacted results.
Together they destroyed the signal for effective classification, rendering the model useless at 
classifying. per channel z-scoring alone was effective.

\section*{AI Statement}

\bibliographystyle{IEEEtran}
\bibliography{references}

\section*{Supplementary Materials}
[Placeholder for supplementary figures or results]

\clearpage
\onecolumn

\end{document}