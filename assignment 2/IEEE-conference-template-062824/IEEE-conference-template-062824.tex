\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{float}
\usepackage{placeins}
\usepackage{svg}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em\textsc{i}\kern-.025em b\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TODO}

\author{\IEEEauthorblockN{Dani\"el Jochems}
\IEEEauthorblockN{David Huizinga}
\IEEEauthorblockN{Niek Grimbergen}
\IEEEauthorblockN{Daan van Dam}}

\maketitle

\begin{abstract}
Decoding brain activity with high accuracy is essential for advancing understanding of human cognition and 
neural processes. In this paper, we perform classification over Magnetoencephalography (MEG) data to infer what
activity subjects are engaging in. We experiment with both convolutional and recurrent neural network architectures, and
investigate whether attentional techniques can enhance performance. We found that the convolutional neural network outperformed
 the other models,




\end{abstract}

\section{Introduction}
Magnetoencephalography (MEG) is a non-invasive neuroimaging technique that measures the magnetic fields produced by electrical 
activity in the brain. Accurate classification of MEG data allows for use cases in brain-computer interfaces, medical diagnoses and
supports neuroscientific research \cite{belhadi2025eeg}. In this paper, we study whether it is possible to infer the cognitive task a subject is performing
based  on MEG recordings. To this end, we utilize deep learning techniques. Deep learning has proven effective in capturing complex, non-linear
 patterns in high-dimensional data \cite{lecun2015deep}, making it potentially well suited for the analysis of neuroimaging data. 

\section{Related Work}
While limited literature on MEG classification exists, EEG signal classification has been studied more intensively. Because of the
high degree of similarity between the two data sources, we expect previous research on EEG classification to be highly relevant.
Early work mainly utilized shallow methods like Support Vector Machines (SVMs) and other linear methods \cite{besserve2007classification}.
However, recent work \cite{lawhern2018eegnet} \cite{zhang2018cascade} \cite{abdellaoui2020deepbrainstateclassification}  has leveraged deep learning
techniques. EEGnet \cite{lawhern2018eegnet} is a compact Convolutional Neural Network (CNN) architecture  that has been designed for EEG signal classification. 
 Later, a combination of convolutional and recurrent techniques were utilized for the same task \cite{zhang2018cascade}. in \cite{abdellaoui2020deepbrainstateclassification} a similar approach is used,
but attention techniques popularized by transformer models \cite{vaswani2017attention} are adopted. 

\section{Methods}

\subsection{Model Selection}
To determine the most suited model architecture for the classification task, we considered both the nature of the data and the strength of various deep learning techniques.
 The selection of our model was guided by the two characteristics of the MEG data we had which were the spatial structure of the various channels and the temporal dynamics.

\subsubsection{Spatial Considerations}
First of all, MEG data is created by recording the magnetic fields generated by neural electrical electricity in the brain.
 This is achieved using a number of sensors attached to different parts of the scalp. Each individual sensor measures activity 
 in a specific area of the brain. This is the spatial structure. We want to select a model that can exploit this structure.
  Because of this, CNNs are well-suited, as these are specifically designed to capture spatial patterns.
CNNs can learn spatially localized patterns of activation that are characteristic of the different tasks to classify. 
CNN offers a degree of translation invariance, which means that it can produce the same output regardless of the input's position or location which should help with inter-subject variability. 

% TODO add this paper for classification of different genres that uses spatial data amplitudes and benchmarks RNNs with transformer and the CNN outperforms the transformer and RNN and RNN outperforms the transformer: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5175090
\subsubsection{Temporal Considerations}
Secondly, the data has a sequential nature: each training example consists of a sequence that develops over time, thus a time series.
 This introduces temporal dependencies that could be critical for accurate classification. 
 Certain patterns may only be apparent through timing or duration. 
 We need to choose a model that has the ability to capture these temporal dependencies. 
 To this end, we can use Long Short-Term Memory networks \cite{yu2019review} (LSTMs). 
 LSTMs are a type of Recurrent Neural Network designed to model long-range temporal dependencies and mitigate vanishing gradient problems. We could also choose a transformer model, which is able to outperform LSTM in a similar temporal dependent classificaiton task (\textbf{SOURCE TODO}), however, since we are dealing with a small dataset and transformers are data-intensive it would likely underperform due to insufficient training data. 

\subsubsection{Attention Mechanisms}
We also explore some models that use attention mechanism, which can dynamically weigh the importance of different time steps during classification. Attention provides the model the flexibility to focus on the most relevant segments of a MEG sequence for each prediction. This is in contrast to processing all time steps equally. This approach could be particularly beneficial for MEG data, where a task-relevant activity may occur at a specific latencies post-stimulus. We apply this on top of CNN or LSTM outputs to capture these time-varying relevances to complement.

In summary, we experiment using two different architectures: a CNN, which we selected to capture spatial structure, and an LSTM, which can model temporal dependencies. 

\subsection{Data Engineering}

\subsubsection{Normalization}
To normalize the data, we applied min-max scaling, bringing each feature to the $[0,1]$ range based on the global minimum and maximum values computed from the entire training set.
 This approach preserves the relative structure of the data while ensuring all features operate on a comparable scale, especially with sensor-based time-series inputs.
The normalization is implemented in the scale data function and applied uniformly to both training and test sets, using the training statistics only. We do this to ensure consistency across the entire pipeline.

\subsubsection{Downsampling}
We applied uniform downsampling to the data to reduce temporal resolution and computational load. 
This is done by selecting a fixed proportion of timesteps in evenly spaced intervals accross the full sequence, in our case we chose this to be 10 percent \texttt{[motivatie voor 10 procent]}.
 With this implementation we compute the stride based on the downsampeling factor and select indices accordingly, ensuring consistent coverage of the original signal. 

\subsection{Model Description}


\subsection{Hyperparameter Tuning}

\section{Results}

\subsection{Accuracy differences Intra vs Cross}
In intra-subject classification, both training and testing data come from the same subject. This means that signal characteristics (sensor placements, movements, physiology) are consistent between readings. The distribution of the test data is also very similar to the training data. This allows for really fine-grained patterns specific to the person, explaining the high accuracy. 

Cross-subject classification, however, performs worse due to the signal characteristics being variable. The trouble then comes from overfitting to the train subjects, which may or may not share physiological and behavioural similarities, and/or sensor placements with the test subjects. The model then learns patterns not represented in some testing cases. One could generalise, which can increase accuracy for lacking test sets, but lowers the score of test sets that were highly similar to the training set. Clearly from testing both test set 1 and 3 fall within the feature distribution, whereas test set 2 does not, this can be beacuse of again physiological differences, but could also be because it is noisy or low-quality data, a different baseline or signal range. This is commonly reffered to as a domain shift.

To improve cross-subject generalisation one could normalise data per subject to reduce individual differences, use transfer learning or domain-invariant representations, use meta learning to train models that can then quickly adapt to a new subject it sees during training, feature engineering to construct features that are invariant across people, or of course more data.

% TODO add figures that show the distribution shift as well as the centroid distance table.
For example, we normalise based only on the training set. If test set 2 contains a different baseline then scaling test set 2 using those values from an unrelated distribution may distort the data. Performing t-SNE to show label-wise distribution (task states) shows that the classes are fairly-well separated in the training and all three test sets, especially rest and motor. This suggests that a classifier should be able to separate these labels provided the test data lies in the same distribution. However, showing source-wise distribution we can tell that test set 1 and 3 overlap significantly with the train distribution. Test 2 however is slightly shifted. Computing the centroid distance in t-SNE space between each test set and the train set shows that test 2 is clearly most shifted. This aligns with classifier results. 

\section{Discussion}
An attempt was made to do some specific MEG preprocessing like namely, a fixed bandpass filter, prepare channel z-scoring, and baseline correction. The inspiration for these came from the \href{https://mne.tools/stable/index.html}{MNE tools} package and the corresponding paper by Gramfort et al \cite{gramfort2013meg}. This applied preprocessing before model training severaly impacted results.Together they destroyed the signal for effective classification, rendering the model useless at classifying. Per channel z-scoring alone was effective however. 


Some experimentation was done to leave a smaller memory footprint when training for the cross-subject classification model because of the size of the matrices. The approach was to extract features from the training data so there was no need for excessively downsampling on the time points. This was done by capturing signal amplitude statistics from the data which are the mean signal per channel, signal variability, peak and minimum amplitude, and the dynamic range of the channels. Some domain knowledge and assumptions are made here where high-amplitudes signify movement tasks, whereas low-amplitudes signify rest signals. Subsequently a discrete fourier transformation was done over the channels, to determine average frequency content and variability in frequencies. The reason is to capture distinct frequency signatures of the different tasks. Lastly cross-channel correlation was performed and only the upper triangle of the correlation matrix was kept, to not involve redundant information. These 'features' were flattened into a single vector for use in training. This creates a high-dimensional vector per sample. These features for the training data and the test data were scaled using the 'StandardScaler' from sklearn. Afterwards PCA was performed to reduce the dimensions of the data and keep $95\%$ of the variance and these were used in training the model. The accuracy for getting the correct task in the test sets $1$, $2$, and $3$ were $0.88$, $0.25$, and $0.70$ respectively. To explain this phenomenon t-SNE was performed on the training and test data to assess distribution overlap of Train vs Test1/2/3. While Test 1 and 3 overlap significantly with the train distribution, test 2 is shifted signifying a distribution shift. This cannot be because of a class imbalance, because the task label distributions are equal. One could leverage a domain adaptation model, augment the train data to generalise better, or use a different model that is more robust to distribution shifts.


\section*{AI Statement}
We consulted ChatGPT for our model choice, suggestions on normalization and downsampling techniques, and general questions during debugging.  We used it to transform panda dataframes
into graphs. It was used to spell check and detect general errors in the writing.
\bibliographystyle{IEEEtran}
\bibliography{references}

\section*{Supplementary Materials}
[Placeholder for supplementary figures or results]

\clearpage
\onecolumn

\end{document}
