\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TODO}

\author{\IEEEauthorblockN{Daniël Jochems}
\IEEEauthorblockA{\textit{TODO (student number)}}
\and
\IEEEauthorblockN{David}
\IEEEauthorblockA{TODO}
\and
\IEEEauthorblockN{Niek Grimbergen}
\IEEEauthorblockA{TODO}
\and
\IEEEauthorblockN{Daan van Dam}
\IEEEauthorblockA{6434657}
}

\maketitle

\begin{abstract}
TODO: Most important paragraph. Context (our question, the problem statement), content (here we go 
over what we did and what we found), conclusion (why/how it matters). Max 150 words.
\end{abstract}

\begin{IEEEkeywords}
TODO
\end{IEEEkeywords}

\section{Introduction}
TODO: Why does the paper matter? Set up gap in science/knowledge focusing down from a bigger 
problem to our smaller problem. Last paragraph should summarize our results to fill this gap. Here 
we can give context and background if we want than the abstract. We cannot give a conclusion, at 
most preview/tease the conclusion/results.

% Daan: I write this section with 'n' folds, if someone does some hyperparameter optimization with
% with the number of folds and finds empirically what is best then please replace the 'n' here with
% that number or put at the end we selected 'n = 5' or something.

\section{Methods}
\subsection{Training data procedures}
First we prepare the timeseries data to accomodate lag values (past values of a variable at a
previous time step). We do this by creating a new dataframe with the lagged values of the time series 
and dropping the rows with missing values. We then split the time series data in $n$-folds. These folds 
are further divided in train and validation folds with a ratio of $80/20$ respectively. Then we standardize 
the data to zero mean and unit variance.

Because the objective of this model is to predict future values of the time series, we need to
make sure that the model is trained on past values and validated on future values. Every fold the 
train fold grows by including the entire previous fold worth of data and adding the next fold's train fold.  
The validation fold remains the same size. The way the validation fold is used is we take the first $x$
input points (where $x$ is the number of lags) and use those to predict the next value $\hat{y}$. We then 
exclude the first input from this input point window and add $\hat{y}$ to the end of the input point window.
Then the next value is predicted. This is repeated until the end of the validation fold is reached.




% Daan: I used mean absolute error as a metric
Every iteration we validate on the validation fold and save the metrics and an increasing weight. 
The metrics over the folds are averaged taking into account these weights using the following
formula:

$$avg = \frac{\sum_{i=1}^{n}(\text{MEA}_{fold_n} * weight_n)}{\sum_{i=1}^n weight_n}$$

for efficiency purposes we save time during the grid search for finding epochs 

\subsection{LSTM}
We used a Long Short-Term Memory (LSTM) neural network to forecast our time series data, building on 
the architecture and tuning strategy described by Vien et al. [1]. Since we were working with a univariate 
time series, the model was set up to predict future values based solely on past observations of the same 
variable.

Before training, we standardized the data using z-scores to ensure consistent scaling. We then performed 
a three-dimensional grid search to tune the model, exploring different combinations of the number of 
training epochs, the number of hidden units in the LSTM layer, and the number of lagged time steps used as 
input. This helped us find the setup that worked best for our dataset.

The LSTM architecture itself was straightforward: it included a sequence input layer, one LSTM layer with 
a tunable number of units, a fully connected layer, and an output layer. To evaluate the model, we used 
expanding window cross-validation (also known as forward-chaining), which ensured that the model was 
always tested on future data it hadn’t seen during training — an important detail for time series tasks.

We used mean absolute error (MAE) to measure performance across folds. The final model was chosen based 
on the average error, weighted by the size of the training data in each fold.

\section{Results}
TODO: This should be a sequence of statements each supported with some evidence and a conclusion.
Each statement is a subsection, the first paragraph of a statement summarizes the overall approach 
to solving the problem stated in the introduction, explain essential features of existing methods
we use and necessary background information. If we made a method ourselvers point it out clearly. 
Figures must be self contained (the legend and caption should have all the info for the figure).

\section{Discussion}
TODO: First we discuss how the gap from the introduction was filled by the report, the limitations
of the approach, some future directions and perhaps an open problem we find. Afterwards Basically 
everything we put some time into but did not write out to a result can be put here and anything we 
wanted to do but did not can also be put here.

\section*{AI statement}
TODO: If we used machine learning generative AI to do anything we put it here.

\section*{References}

\end{document}
