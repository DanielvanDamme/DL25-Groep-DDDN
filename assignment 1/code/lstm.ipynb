{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "matlab = scipy.io.loadmat('Xtrain.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = matlab['Xtrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9037af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d881557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a list\n",
    "X_train_list = X_train.flatten().tolist()\n",
    "print(X_train_list)\n",
    "\n",
    "# Plot a line of the values in the training set\n",
    "fig = px.line(y=X_train_list, x=range(len(X_train_list)), title='Line Plot of X_train Values')\n",
    "fig.update_layout(xaxis_title='Index', yaxis_title='Value') \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c473b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(timeseries_array, n_lags):\n",
    "    df = pd.DataFrame(timeseries_array, columns=['label'])\n",
    "    for i in range(1, n_lags + 1):\n",
    "        df[f'lag {i}'] = df['label'].shift(i)\n",
    "    df = df.dropna()\n",
    "\n",
    "    X = df.drop(columns=['label']).to_numpy()\n",
    "    y = df['label'].to_numpy()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = create_features(X_train_list, 10)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c94c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_cv_sets(X, y, folds: int, validation_split_ratio: float = 0.2):\n",
    "    \n",
    "    n = len(X)\n",
    "    fold_size = int(n / folds)\n",
    "\n",
    "    for fold in range(folds):\n",
    "        start_fold = fold * fold_size\n",
    "        end_fold = start_fold + fold_size\n",
    "\n",
    "        training_start = 0\n",
    "        training_end = int(end_fold - fold_size * validation_split_ratio)\n",
    "\n",
    "        validation_start = training_end\n",
    "\n",
    "        training_set = X[training_start:training_end], y[training_start:training_end]\n",
    "        validation_set = X[validation_start:end_fold], y[validation_start:end_fold]\n",
    "        yield training_set, validation_set\n",
    "\n",
    "# Example usage\n",
    "folds = 5\n",
    "for train_set, val_set in expanding_window_cv_sets(X, y, folds):\n",
    "    train_X, train_y = train_set\n",
    "    val_X, val_y = val_set\n",
    "\n",
    "    print(\"Training Set:\")\n",
    "    print(train_X.shape, train_y.shape)\n",
    "    print(\"Validation Set:\")\n",
    "    print(val_X.shape, val_y.shape)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two tables to store the results of CV grid search\n",
    "epoch_grid_search_results = pd.DataFrame(columns=['epoch', 'fold', 'MSE', 'MAE'])\n",
    "lag_grid_search_results = pd.DataFrame(columns=['lag', 'fold', 'MSE', 'MAE'])\n",
    "hidden_units_grid_search_results = pd.DataFrame(columns=['hidden_units', 'fold', 'MSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "folds = 5\n",
    "for epoch in [10, 20, 50, 100, 200, 500, 1000, 2000]:\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for train_set, val_set in expanding_window_cv_sets(X_scaled, y_scaled, folds):\n",
    "        train_X, train_y = train_set\n",
    "        val_X, val_y = val_set\n",
    "        \n",
    "        # Reshape the data to be 3D for LSTM input\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "        val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "\n",
    "        # First, we are gonna fix the LSTM to 10 hidden units and grid search the epochs\n",
    "\n",
    "        # Define the lSTM model with 10 hidden units\n",
    "        model = Sequential([\n",
    "            LSTM(10, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(train_X, train_y, epochs=epoch, batch_size=32, verbose=0)\n",
    "\n",
    "        # Recursively predict the validation set, so start with the first example of the validation set and use the model to predict the next value, then use that value to predict the next one, and so on.\n",
    "        current_input = val_X[0].reshape((1, 1, val_X.shape[2]))\n",
    "        y_pred = []\n",
    "\n",
    "        for i in range(len(val_y)):\n",
    "            new_y = model.predict(current_input)\n",
    "            y_pred.append(new_y[0][0])\n",
    "            # Remove the first value of the current input and append the new value to the end of the input sequence\n",
    "            current_input = np.append(current_input[:, :, 1:], new_y.reshape((1, 1, 1)), axis=2)\n",
    "        \n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "        print(\"Epochs: \", epoch)\n",
    "        print(\"Standardized MSE: \", mean_squared_error(val_y, y_pred))\n",
    "        print(\"Standardized MAE: \", np.mean(np.abs(val_y - y_pred)))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Append the results to the epoch grid search results table\n",
    "        epoch_grid_search_results = pd.concat([epoch_grid_search_results, pd.DataFrame({'epoch': [epoch], 'MSE': [mean_squared_error(val_y, y_pred)], 'MAE': [np.mean(np.abs(val_y - y_pred))]})], ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ac21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Your data\n",
    "data = np.array(X_train_list)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Sequence builder\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 10\n",
    "X_all, y_all = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "# Reshape for LSTM\n",
    "X_all = X_all.reshape((X_all.shape[0], X_all.shape[1], 1))\n",
    "\n",
    "# TimeSeries Cross-Validation\n",
    "n_splits = 4\n",
    "split_size = len(X_all) // n_splits\n",
    "mse_scores = []\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(seq_length, 1), dropout=0.2, recurrent_dropout=0.2),\n",
    "    LSTM(16, dropout=0.4),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    split_point = split_size * (fold + 1)\n",
    "\n",
    "    X_train, y_train = X_all[:split_point], y_all[:split_point]\n",
    "    X_val, y_val = X_all[split_point:split_point+split_size], y_all[split_point:split_point+split_size]\n",
    "\n",
    "    # Skip final folds if not enough validation data\n",
    "    if len(X_val) < 10:\n",
    "        break\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    print(f\"Fold {fold+1} MSE: {mse:.5f}\")\n",
    "\n",
    "# Report\n",
    "print(\"Average MSE across folds:\", np.mean(mse_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14f273",
   "metadata": {},
   "source": [
    "# Expanding Window Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ed23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_cv(data, model, n_splits=, verbose=False):\n",
    "    split_size = len(data) // n_splits\n",
    "    mse_scores = []\n",
    "\n",
    "    \n",
    "\n",
    "    return np.mean(mse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125f253",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_all))\n",
    "\n",
    "model.fit(X_all, y_all, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Create space for predictions (same length as the original scaled data)\n",
    "predicted = np.full((len(data_scaled), 1), np.nan)\n",
    "\n",
    "# Loop over all sequences\n",
    "for i in range(len(X_all)):\n",
    "    input_seq = X_all[i].reshape(1, seq_length, 1)  # Single input sequence\n",
    "    prediction = model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Map prediction to the correct future index\n",
    "    predicted[i + seq_length] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeaa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=data_scaled.flatten(), mode='lines', name='True'))\n",
    "fig.add_trace(go.Scatter(y=predicted.flatten(), mode='lines', name='Predicted'))\n",
    "fig.update_layout(title='True vs Predicted Values', xaxis_title='Index', yaxis_title='Value')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef3808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
